# First Principles: From Unitary Computation to Physical Reality (English)

---

# First Principles: From Unitary Computation to Physical Reality

This book builds a physical theoretical framework based on discrete information ontology from first principles.

## Foreword

- [Foreword: Occam's Ultimate Razor](foreword_en.md)

## Book Structure

### Part I: The Birth of Axioms

#### Chapter 1: The Necessity of Discrete Ontology

- [1.1 Zeno's Paradox and the Ghost of Continuum](part01-birth-of-axioms/chapter01-necessity-of-discrete-ontology/01-01-zeno-paradox-continuum-ghost_en.md)
- [1.2 Black Hole Entropy and Bekenstein Bound: The Universe as a Finite-Capacity Hard Drive](part01-birth-of-axioms/chapter01-necessity-of-discrete-ontology/01-02-black-hole-entropy-bekenstein-bound_en.md)
- [1.3 Information Realism: Bits and Qubits as the Atoms of Matter](part01-birth-of-axioms/chapter01-necessity-of-discrete-ontology/01-03-information-realism-bit-qubit_en.md)

#### Chapter 2: The Ultimate Axiom $\Omega$

- [2.1 Axiom Statement: The Universe is a Quantum Cellular Automaton (QCA) Operating on Discrete Lattice Points Following Local Unitary Evolution Rules](part01-birth-of-axioms/chapter02-ultimate-axiom-omega/02-01-axiom-statement-qca_en.md)
- [2.2 Why Unitarity? — Conservation of Probability and Logical Consistency](part01-birth-of-axioms/chapter02-ultimate-axiom-omega/02-02-why-unitarity_en.md)
- [2.3 Why Locality? — Avoiding "God's Eye View" and Action at a Distance](part01-birth-of-axioms/chapter02-ultimate-axiom-omega/02-03-why-locality_en.md)
- [2.4 Formal Definition: Graph $\Lambda$, Hilbert Space $\mathcal{H}$, and Update Operator $\hat{U}$](part01-birth-of-axioms/chapter02-ultimate-axiom-omega/02-04-formal-definition_en.md)

### Part II: The Emergence of Spacetime

#### Chapter 3: Causality and the Speed of Light

- [3.1 The Discrete Origin of Light Cones: Deriving the Maximum Signal Velocity $c$ from Lattice Hopping](part02-emergence-of-spacetime/chapter03-causality-speed-of-light/03-01-light-cone-discrete-origin_en.md)
- [3.2 Light Path Conservation Theorem: Proving $v_{ext}^2 + v_{int}^2 = c^2$ from Unitarity](part02-emergence-of-spacetime/chapter03-causality-speed-of-light/03-02-light-path-conservation_en.md)
- [3.3 Derivation of Lorentz Transformation: No Geometry, Only Statistics of Resource Allocation](part02-emergence-of-spacetime/chapter03-causality-speed-of-light/03-03-lorentz-transformation-derivation_en.md)

#### Chapter 4: Gravity as Statistical Necessity

- [4.1 Entanglement and Geometry: Spacetime Distance as Quantum Mutual Information](part02-emergence-of-spacetime/chapter04-gravity-as-statistical-necessity/04-01-entanglement-geometry_en.md)
- [4.2 Local Information Volume Conservation: Why Must Space Expand When Information Compresses Time?](part02-emergence-of-spacetime/chapter04-gravity-as-statistical-necessity/04-02-local-information-volume-conservation_en.md)
- [4.3 Entropic Derivation of Einstein Field Equations: Proof of the IGVP Principle](part02-emergence-of-spacetime/chapter04-gravity-as-statistical-necessity/04-03-einstein-equation-entropic-derivation_en.md)
- [4.4 Black Holes: Entanglement Knots in QCA Networks and Holographic Screens](part02-emergence-of-spacetime/chapter04-gravity-as-statistical-necessity/04-04-black-hole-entanglement-knot_en.md)

### Part III: The Emergence of Matter

#### Chapter 5: The Topological Origin of Mass

- [5.1 Why Do Photons Have No Mass? — Pure Translation Modes](part03-emergence-of-matter/chapter05-topological-origin-of-mass/05-01-why-photon-no-mass_en.md)
- [5.2 Matter as Topological Knots: Self-Referential Loops and Winding Numbers](part03-emergence-of-matter/chapter05-topological-origin-of-mass/05-02-matter-as-topological-knot_en.md)
- [5.3 Mass as Impedance: Information Refresh Rate Required to Maintain Internal Oscillation ($v_{int}$)](part03-emergence-of-matter/chapter05-topological-origin-of-mass/05-03-mass-as-impedance_en.md)
- [5.4 Origin of Fermion Statistics: Riccati Square Root and $\mathbb{Z}_2$ Phase](part03-emergence-of-matter/chapter05-topological-origin-of-mass/05-04-fermion-statistics-riccati_en.md)

#### Chapter 6: Interactions and Gauge Fields

- [6.1 Independence of Local Reference Frames: Why Does Each Cell Need "Translation"?](part03-emergence-of-matter/chapter06-interactions-gauge-fields/06-01-local-reference-frame-independence_en.md)
- [6.2 Necessity of Link Variables: Derivation of Maxwell and Yang-Mills Equations](part03-emergence-of-matter/chapter06-interactions-gauge-fields/06-02-link-variables-maxwell-yang-mills_en.md)
- [6.3 Geometric Meaning of Coupling Constants: Network Connectivity and Information Leakage Rate](part03-emergence-of-matter/chapter06-interactions-gauge-fields/06-03-coupling-constant-geometric-meaning_en.md)

### Part IV: The Emergence of Observation

#### Chapter 7: Quantum Measurement and Objectivity

- [7.1 The Discrete Solution to Schrödinger's Cat: Entanglement and Branching under Unitary Evolution](part04-emergence-of-observation/chapter07-quantum-measurement-objectivity/07-01-schrodinger-cat-discrete-solution_en.md)
- [7.2 Combinatorial Proof of Born Rule: Based on Zurek Rotation and Microstate Counting](part04-emergence-of-observation/chapter07-quantum-measurement-objectivity/07-02-born-rule-combinatorial-proof_en.md)
- [7.3 The Achievement of Objective Reality: Nash Equilibrium and Consensus Geometry in Multi-Agent Systems](part04-emergence-of-observation/chapter07-quantum-measurement-objectivity/07-03-objective-reality-nash-equilibrium_en.md)

#### Chapter 8: The Physics Definition of Consciousness

- [8.1 Observer Model: Computational Structure (Agent) as Self-Referential Subsystem](part04-emergence-of-observation/chapter08-physics-definition-consciousness/08-01-observer-model-self-referential-subsystem_en.md)
- [8.2 Information Mass $M_I$ and Free Energy Minimization](part04-emergence-of-observation/chapter08-physics-definition-consciousness/08-02-information-mass-free-energy_en.md)
- [8.3 Red Queen Effect and Thermodynamics: Why Does the Universe Not Have Heat Death?](part04-emergence-of-observation/chapter08-physics-definition-consciousness/08-03-red-queen-effect-thermodynamics_en.md)

### Part V: Verification and Inference

#### Chapter 9: Experimental Predictions

- [9.1 Entanglement Gravity Detection Scheme in Microwave Cavities](part05-verification-inference/chapter09-experimental-predictions/09-01-microwave-cavity-entanglement-gravity_en.md)
- [9.2 Lorentz Violation Effects in Ultra-High-Energy Cosmic Rays](part05-verification-inference/chapter09-experimental-predictions/09-02-ultra-high-energy-cosmic-rays-lorentz-violation_en.md)
- [9.3 Cosmological Drift of Fine Structure Constant](part05-verification-inference/chapter09-experimental-predictions/09-03-fine-structure-constant-cosmological-drift_en.md)

#### Chapter 10: Ultimate Questions of the Computational Universe

- [10.1 The Universe's Source Code: How Was Rule $\hat{U}$ Selected? (Criticality Hypothesis)](part05-verification-inference/chapter10-ultimate-questions-computational-universe/10-01-universe-source-code-criticality_en.md)
- [10.2 Physical Projection of Gödel's Incompleteness: The Boundary of Agnosticism](part05-verification-inference/chapter10-ultimate-questions-computational-universe/10-02-godel-incompleteness-physical-projection_en.md)
- [10.3 Are We Players or NPCs? — On the Status of Free Will in Deterministic QCA](part05-verification-inference/chapter10-ultimate-questions-computational-universe/10-03-free-will-deterministic-qca_en.md)

## Conclusion

- [Book Conclusion: From Observer to Builder](conclusion_en.md)

## Afterword

- [Afterword: The Unfinished Symphony](afterword_en.md)

## Appendices

- [Appendix A: Mathematical Foundations](appendix/appendix-a-mathematical-foundations_en.md)
- [Appendix B: QCA Simulation Guide](appendix/appendix-b-qca-simulation-guide_en.md)
- [Appendix C: Core Theorem Proof Collection](appendix/appendix-c-core-theorem-proofs_en.md)
- [Appendix D: Key Terminology Glossary](appendix/appendix-d-glossary_en.md)

# Foreword: Occam's Ultimate Razor

**Preface: Occam's Ultimate Razor**

The history of physics is a history of continuously searching for deeper "source code."

Three hundred years ago, Newton made us believe that the universe is a precise clockwork, driven by the springs of absolute time and space; one hundred years ago, Einstein told us that the clockwork does not exist, that spacetime itself is a pliable body that can be bent; meanwhile, Bohr and Heisenberg discovered in the microscopic world that the essence of reality is a probability cloud of dice-throwing.

Today, we stand between two magnificent yet incompatible theoretical edifices: one is the geometric temple of **General Relativity**, smooth and deterministic, describing how gravity weaves spacetime; the other is the probabilistic maze of **Quantum Mechanics**, discrete and non-deterministic, describing how particles jump in Hilbert space. These two languages are immensely successful in their respective domains, but when we attempt to peer into the horizon of black holes or trace back to the moment of the Big Bang, they collide violently—geometry produces infinite singularities, probability leads to information loss.

This schism has persisted for nearly a century of patching, yet remains unhealed. Perhaps the problem is not that we are not smart enough to find answers within the existing framework, but that **the framework itself is wrong**. We have been trying to forcibly stuff "quantum" into the box of "geometry," or quantize "gravity" as a field.

What if neither of these is the underlying truth?

This book proposes a radical, minimalist ontology: **Physical reality is not composed of matter, energy, or fields, but of "information" and "the process of information processing."**

John Wheeler once proposed the famous "It from Bit." This book pushes this idea to its logical extreme and endows it with dynamic form: **It from Qubit Processing.**

In this book, we make only one assumption, a single, irreducible axiom—**the Ultimate Axiom $\Omega$**:

> **The universe is a Quantum Cellular Automaton (QCA) operating on discrete lattice points, following local unitary evolution rules.**

Beyond this, there is nothing else. No presupposed spacetime background, no presupposed mass parameters, no presupposed speed of light limit, and no presupposed gravitational equations.

We invite readers to witness a logical magic trick: starting from this single seed alone, how the entire edifice of modern physics will be "grown" anew.

* We will prove that the **speed of light** is not a traffic speed limit set by God, but the maximum bandwidth for information transmission in the lattice network.

* We will prove that **Special Relativity** is merely a statistical result of resource allocation of information rates between "external displacement" and "internal computation"—the **Light Path Conservation Law** we derive ($v_{ext}^2 + v_{int}^2 = c^2$) exposes this deep mechanism nakedly in the mathematical sunlight.

* We will prove that **mass** is not an inherent property of matter, but information dead loops trapped at microscopic scales by topological structures; and **inertia** is the energy cost required for a system to maintain its existence when internal time is frozen.

* We will prove that **gravity** is not a fundamental interaction, but geometric distortion that must occur for the spacetime network to maintain consistency (unitarity) of information transmission—Einstein's field equations are essentially the "equation of state" of the information manifold.

This is **Occam's Ultimate Razor**. We shave away the illusion of continuity, shave away infinite divergences, shave away artificially introduced parameters. What remains is a crystal-clear universe composed purely of logic and computation.

In this universe, **observers** are no longer ghosts standing outside, but subroutines capable of self-referential computation; **consciousness** is no longer a mysterious byproduct, but the highest topological form in the information network.

This is a journey of reconstruction from "ontology" to "phenomenology." For physicists accustomed to continuous spacetime and differential equations, the landscape here may seem unfamiliar or even angular. But I believe that when you reach the final chapter and see how discrete bits emerge into continuous reality, see how the familiar stars, gravity, and time are born from the void of computation, you will feel an unprecedented beauty of logic.

The universe is not merely like a computer. **The universe is computation itself.**

Let us begin running this code.

---

### **Author's Note**

Many core derivations in this book (such as the Light Path Conservation Theorem, the volume conservation correction of optical metrics, and the combinatorial proof of Born's rule) are based on a series of research papers by the author in recent years. To ensure reading fluency, complex mathematical proofs are placed in appendices or specific chapters, but this does not mean mathematics is unimportant. On the contrary, it is the rigidity of mathematics that supports the entire weight of this philosophical conception.

# Book Conclusion: From Observer to Builder

This book began with an extremely abstract assumption: the universe is a quantum cellular automaton.

Through ten chapters of derivation, we have seen how this simple assumption grows the boundaries of light cones, how it curves spacetime due to resource competition, how it condenses into matter due to information knotting, and how it illuminates consciousness due to self-referential closed loops.

This is a logically closed-loop universe, a trinity composed of **Information (Bit)**, **Entanglement (It)**, and **Computation (Process)**.

In this system, physics is no longer merely archaeology discovering "ready-made laws"; it is becoming an **engineering discipline**.

As we envisioned in Section 8.3.3, since we understand the underlying code of spacetime and matter, future civilizations will ultimately evolve from "observers" to "builders." We may be able to design new artificial vacua, weave new topological particles, and even reconstruct gravity at the Planck scale.

That will be the end of physics, and the beginning of creation.

**It from Bit.**

**We are the Bit.**

**We make the It.**

(End of Book)

# Afterword: The Unfinished Symphony

**Afterword: The Unfinished Symphony**

When I wrote the last line of this book, I looked out the window. Sunlight passed through the glass and spilled onto the desk. According to the theory in this book, this is not a continuous flow of light, but billions of photons jumping on discrete spacetime grids, each photon executing its tiny displacement algorithm defined by the Planck scale. And the reason I feel warmth is because my skin cells, as some enormous entangled structure, are exchanging information with these photons, increasing local entropy.

This is a wonderful feeling. When you put on the glasses of "unitary computation" and re-examine the world, everything changes. The old world composed of smooth geometry and continuous fields dissolves, replaced by a crystal-clear digital edifice composed of logic and causality.

**This book is the limit of a thought experiment.**

We attempted to answer a question: **If we only allow ourselves one simplest assumption—that the universe is computation—how far can we go?**

The result is surprising. We were not forced to invent strange new physics; instead, we picked up our most familiar old friends along the way:

* We picked up the **speed of light** in lattice hopping;

* We picked up **relativity** in resource allocation;

* We picked up **mass** in topological knotting;

* We picked up **gravity** in information congestion;

* We picked up **probability** in horizon truncation.

This seems to suggest that those incompatible fragments accumulated by physics over the past three hundred years—Newton's forces, Maxwell's fields, Einstein's geometry, Bohr's probability—are actually projections of the same underlying truth from different perspectives. This truth is: **Existence is information, evolution is computation.**

However, I also deeply understand that this book is only a prologue.

Although we have built the skeleton, there is still too much flesh and blood to fill in.

* We derived the geometric origin of coupling constants, but have not yet calculated the precise value of $1/137.036$.

* We predicted the evolution of the universe under the Red Queen effect, but have not yet given the precise evolution curve of the dark energy equation of state.

* We designed experiments for entanglement gravity, but have not yet seen that tiny phase shift in the laboratory.

This is the most fascinating aspect of science. **Theory is not the end of truth, but the beginning of exploration.**

I regard this book as a map. A treasure map leading deep into the "computational universe." I may have marked several key coordinates on the map (Axiom $\Omega$, Light Path Conservation, IGVP), but the true treasure—that "source code" capable of explaining everything and even allowing us to reconstruct everything—still lies buried in the fog of the unknown.

This map is now in your hands.

Perhaps you are an experimental physicist, and you will capture signals of spacetime trembling due to entanglement in the darkness of microwave cavities;

Perhaps you are a mathematician, and you will find that ultimate invariant describing the topological classification of QCA;

Perhaps you are a computer scientist, and you will write simulation programs more exquisite than those in Appendix B, creating true artificial vacua through emergence on silicon chips;

Or perhaps you are just a stargazer, and you will feel a strange comfort in realizing that you are not merely cosmic dust, but part of cosmic computation.

Whoever you are, we are all collaborators in this great machine.

Physics has no end. As long as there is still one observer thinking, the computation of the universe continues. As long as there is still one unanswered question, this symphony is not finished.

Let us continue computing.

**Auric**

**In Discrete Spacetime/Singapore**

---

**(End of Book)**

# Appendix A: Mathematical Foundations

**Appendix A: Mathematical Foundations**

The physical theory constructed in this book spans multiple domains from microscopic discrete lattices to macroscopic continuous spacetime, and further to logic and computation. To maintain narrative fluency in the main text, many deep mathematical definitions and theorems are only cited physically. This appendix aims to provide a self-consistent, standardized mathematical tool reference manual, covering core concepts of Hilbert spaces, operator algebras, information geometry, and graph theory, serving as the mathematical skeleton of the entire book.

---

## A.1 Hilbert Space Structure of Discrete Quantum Mechanics

In Axiom $\Omega$, we define physical entities as vectors in Hilbert space. For discrete ontology, we primarily focus on finite-dimensional spaces and their tensor products.

### A.1.1 Local Space and Tensor Product

* **Local Space**:

  Let each cell (lattice point) $x$ be associated with a $d$-dimensional complex vector space $\mathcal{H}_x \cong \mathbb{C}^d$. For qubit systems, $d=2$, with basis denoted as $\{|0\rangle, |1\rangle\}$.

  Vectors $|\psi\rangle \in \mathcal{H}_x$ in the space satisfy the normalization condition $\langle \psi | \psi \rangle = 1$.

* **Global Space**:

  The state space of the entire system is the tensor product of all local spaces:

  $$\mathcal{H}_{\text{total}} = \bigotimes_{x \in \Lambda} \mathcal{H}_x$$

  **Note**: For infinite lattices $\Lambda$, this tensor product must be understood in the sense of the algebraic tensor product of von Neumann algebras, typically restricted to the separable Hilbert space spanned by **finite excitation states** (states where all nodes except finitely many are in the ground state $|0\rangle$).

### A.1.2 Entanglement and Schmidt Decomposition

For a composite system $\mathcal{H}_{AB} = \mathcal{H}_A \otimes \mathcal{H}_B$, any pure state $|\Psi\rangle$ can be uniquely decomposed as:

$$|\Psi\rangle = \sum_{i=1}^{k} \lambda_i |a_i\rangle_A \otimes |b_i\rangle_B$$

where $\lambda_i > 0$ are Schmidt coefficients satisfying $\sum \lambda_i^2 = 1$. $k \le \min(\dim \mathcal{H}_A, \dim \mathcal{H}_B)$ is the Schmidt rank.

* **Entanglement Entropy**: $S(\rho_A) = -\sum \lambda_i^2 \ln \lambda_i^2$. This is the core quantity used in Chapter 4 of this book to derive gravitational geometry.

---

## A.2 Operator Algebra & Spectral Theory

Many physical intuitions in this book—such as the holographic principle, modular Hamiltonians, and the entanglement structure of information—rely on **operator algebras** (particularly von Neumann algebras) as their rigorous mathematical language. In discrete QCA ontology, physical systems are modeled as tensor products of local Hilbert spaces $\mathcal{H}_x$, and physical quantities (observables) on them constitute specific algebraic structures. This section briefly introduces relevant core concepts.

### A.2.1 Von Neumann Algebras and Factors

Let $\mathcal{H}$ be a complex Hilbert space (possibly infinite-dimensional, corresponding to the limit $N \to \infty$). $\mathcal{B}(\mathcal{H})$ is the algebra of bounded linear operators on it.

**Definition A.2.1 (Von Neumann Algebra)**:

A $*$-subalgebra $\mathcal{M}$ of $\mathcal{B}(\mathcal{H})$ is called a von Neumann algebra if it contains the identity operator $\mathbb{I}$ and satisfies the **Bicommutant Theorem**:

$$\mathcal{M} = \mathcal{M}''$$

where $\mathcal{M}' = \{ T \in \mathcal{B}(\mathcal{H}) : TA = AT, \forall A \in \mathcal{M} \}$ is the commutant algebra of $\mathcal{M}$. This means $\mathcal{M}$ is closed under the weak operator topology.

In quantum field theory and holographic theory, we particularly focus on **factors**, i.e., von Neumann algebras with trivial center:

$$\mathcal{Z}(\mathcal{M}) = \mathcal{M} \cap \mathcal{M}' = \mathbb{C}\mathbb{I}$$

Von Neumann algebras are classified into three types based on the properties of their projection operators:

* **Type I**: Isomorphic to $\mathcal{B}(\mathcal{H})$. This is the standard quantum mechanics algebra describing systems with finite degrees of freedom (such as spin chains, single QCA cells). The trace on it is well-defined.

* **Type II**: No minimal projections exist, but a semifinite trace exists. This appears in certain statistical mechanics models.

* **Type III**: The most mysterious and important type. All non-zero projections are equivalent to the identity. **Local algebras $\mathcal{A}(O)$ in local quantum field theory are typically Type III$_1$ factors.** This means that in the continuous limit, entanglement entropy within local regions diverges (requiring ultraviolet cutoff), which is precisely the mathematical root of our discussion of "the necessity of discrete ontology" in Chapter 1.

### A.2.2 Modular Theory (Tomita-Takesaki Theory)

For a general von Neumann algebra $\mathcal{M}$ and a cyclic separating vector $|\Omega\rangle$, we cannot define a standard density matrix $\rho$ and trace as in Type I algebras. To define "states" and "evolution," we need **Tomita-Takesaki modular theory**.

Define the operator $S$:

$$S A |\Omega\rangle = A^\dagger |\Omega\rangle, \quad \forall A \in \mathcal{M}$$

The polar decomposition of $S$ is $S = J \Delta^{1/2}$.

* **$J$**: Modular conjugation operator, an antilinear operator. It establishes an isomorphism between algebra $\mathcal{M}$ and its commutant $\mathcal{M}'$: $J \mathcal{M} J = \mathcal{M}'$. Physically, this corresponds to **CPT symmetry** in holographic duality.

* **$\Delta$**: Modular operator, $\Delta = S^\dagger S$. This is a positive self-adjoint operator.

**Theorem A.2.2 (Tomita-Takesaki Theorem)**:

$\Delta^{it}$ generates a one-parameter automorphism group $\sigma_t$ of $\mathcal{M}$:

$$\sigma_t(A) = \Delta^{it} A \Delta^{-it} \in \mathcal{M}, \quad \forall A \in \mathcal{M}$$

This is called the **modular flow**.

**Physical Meaning**:

For the vacuum state $|\Omega\rangle$, the modular Hamiltonian is defined as $H_{mod} = -\ln \Delta$.

$$\rho \sim e^{-H_{mod}}$$

This shows that **any entangled state intrinsically defines a "thermal time flow."** For observers in Rindler wedges, the modular flow exactly corresponds to Lorentz boosts, and the modular Hamiltonian is the Hamiltonian generating such accelerated motion. This directly connects **quantum entanglement** with **spacetime geometry**, serving as the mathematical foundation for deriving the origin of gravity in Chapter 4 of this book.

### A.2.3 Relative Entropy and Fisher Information

In the operator algebra framework, the "distance" between two states $\psi$ and $\phi$ is measured by **relative entropy**, defined as:

$$S(\psi || \phi) = \langle \psi | \ln \Delta_{\psi, \phi} | \psi \rangle$$

where $\Delta_{\psi, \phi}$ is the relative modular operator.

For parameterized state families $\rho(\theta)$, the second-order expansion of relative entropy gives the **Quantum Fisher Information Metric (QFIM)**:

$$S(\rho(\theta) || \rho(\theta + d\theta)) \approx \frac{1}{2} g_{ij} d\theta^i d\theta^j$$

This is precisely the microscopic source we use in Chapters 3 and 4 to construct the spacetime metric $g_{\mu\nu}$. Spacetime geometry is essentially the information geometry of quantum state space.

---

## A.3 Category Theory Foundations

In the physical theory of this book, **category theory** is not merely an abstract mathematical language, but the "meta-language" describing deep connections between physical processes, logical structures, and computational operations. Particularly in the axiomatic definition of quantum cellular automata (QCA), classification of topological orders, and structured description of holographic entanglement, category theory provides indispensable tools.

### A.3.1 Basic Definitions: Categories and Functors

**Definition A.3.1 (Category $\mathcal{C}$)**:

A category consists of:

1. **Object set** $\text{Obj}(\mathcal{C})$: In physics, objects typically represent state spaces of physical systems (such as Hilbert space $\mathcal{H}$).

2. **Morphism set** $\text{Hom}(A, B)$: For any two objects $A, B$, there exists a morphism set from $A$ to $B$. In physics, morphisms represent physical processes (such as evolution operators $\hat{U}$, measurement channels $\mathcal{E}$).

3. **Composition operation** $\circ$: Morphisms satisfy associativity $(f \circ g) \circ h = f \circ (g \circ h)$.

4. **Identity morphism** $1_A$: Each object has a "do nothing" operation.

**Definition A.3.2 (Functor $\mathcal{F}$)**:

A functor is a structure-preserving map between categories. It maps objects of category $\mathcal{C}$ to objects of category $\mathcal{D}$, maps morphisms to morphisms, and preserves composition relations.

* **Physical meaning**: The holographic principle can be formalized as a functor (or dual equivalence) from the "boundary conformal field theory category" to the "bulk gravitational theory category."

### A.3.2 Monoidal Categories & Tensor Networks

To describe **composite systems** ($\mathcal{H}_A \otimes \mathcal{H}_B$) in quantum mechanics, we need to introduce **monoidal categories**.

**Definition A.3.3 (Monoidal Category $(\mathcal{C}, \otimes, I)$)**:

This is a category equipped with a **tensor product functor** $\otimes: \mathcal{C} \times \mathcal{C} \to \mathcal{C}$ and a **unit object** $I$.

* **Associativity constraint**: $(A \otimes B) \otimes C \cong A \otimes (B \otimes C)$ (via natural isomorphism $\alpha$).

* **Unit constraint**: $I \otimes A \cong A \cong A \otimes I$.

**Graphical Calculus**:

Morphisms in monoidal categories can be represented using **string diagrams**.

* Objects are wires.

* Morphisms are boxes connecting wires.

* Tensor product is parallel placement of wires.

* Composition is serial connection of wires.

**Physical applications**: Evolution processes of QCA, tensor network states (such as MPS, PEPS), and quantum circuit diagrams are essentially graphical calculus in strict monoidal categories. This language makes complex tensor contraction operations intuitive and easily verifiable.

### A.3.3 Dagger Compact Categories

To describe **unitarity** and **entanglement** (such as preparation and measurement of Bell states) in quantum mechanics, we need richer structures.

**Definition A.3.4 (Dagger Category $\dagger$-Category)**:

This is a category equipped with a contravariant functor $\dagger: \mathcal{C}^{op} \to \mathcal{C}$ satisfying $f^{\dagger\dagger} = f$.

* **Physical meaning**: Corresponds to Hermitian conjugation in Hilbert space. Unitary operator $U$ is defined as $U^\dagger \circ U = 1_A$.

**Definition A.3.5 (Compact Closed Category)**:

This is a monoidal category with "dual objects" $A^*$ and **unit morphism** $\eta: I \to A^* \otimes A$ and **counit morphism** $\epsilon: A \otimes A^* \to I$, satisfying snake equations.

* **Physical meaning**:

  * $\eta$ corresponds to preparation of maximally entangled states (such as Bell state $|\Phi^+\rangle$).

  * $\epsilon$ corresponds to measurement (projection) of maximally entangled states.

  * Snake equations correspond to the geometric essence of quantum teleportation protocols: curved spacetime lines (entanglement) can transmit information from one end to the other.

**Conclusion**:

The axiomatic system of quantum mechanics (QM) can be extremely elegantly reconstructed as: **Physical processes constitute a dagger compact category (DCC) on Hilbert space.**

In the theoretical framework of this book, the QCA defined by Axiom $\Omega$ is essentially an algorithm running in a discrete DCC. This categorical perspective not only unifies quantum logic with spacetime geometry (topological quantum field theory TQFT), but also provides the most general mathematical template for possible future reconstruction of physical laws.

---

## A.4 Information Geometry

The key to deriving special and general relativity in this book lies in geometrizing physical processes. Here we introduce the geometric structure of **quantum state manifolds**.

### A.4.1 Projective Hilbert Space $\mathbb{C}P^{N-1}$

Physical states correspond to rays in Hilbert space, i.e., $|\psi\rangle \sim e^{i\theta}|\psi\rangle$. The manifold of all physical states is complex projective space $\mathbb{C}P^{N-1}$.

### A.4.2 Fubini-Study Metric

This is the natural Riemannian metric defined on quantum state space, used to measure the "distance" between two quantum states.

For two closely spaced states $|\psi\rangle$ and $|\psi + d\psi\rangle$, their distance $ds^2$ is defined as:

$$ds_{FS}^2 = 4 \left( \langle d\psi | d\psi \rangle - |\langle \psi | d\psi \rangle|^2 \right) = 4 (\Delta H)^2 dt^2$$

* **Geometric meaning**: This is the rate at which quantum states orthogonalize with other states during evolution.

* **Physical application**: In Chapter 3 of this book, we define the total information rate $c$ as the Fubini-Study velocity $ds_{FS}/dt$ along the evolution trajectory. The Light Path Conservation Theorem $v_{ext}^2 + v_{int}^2 = c^2$ is precisely the Pythagorean decomposition of this metric on the position subspace and internal subspace.

### A.4.3 Berry Curvature

When system parameters adiabatically evolve in parameter space $\mathcal{M}$, the wave function acquires a geometric phase $\gamma$. This corresponds to a gauge field (Berry connection) on $\mathcal{M}$:

$$\mathcal{A} = -i \langle \psi | \nabla | \psi \rangle$$

The corresponding curvature tensor $\mathcal{F} = \nabla \times \mathcal{A}$ describes the geometric properties of parameter space.

In Chapter 6 of this book, we interpret gauge fields (electromagnetic fields, Yang-Mills fields) as Berry connections caused by local basis transformations in QCA networks.

---

## A.5 Graph Theory and Discrete Topology

### A.5.1 Cayley Graph

If QCA space has translational symmetry, the lattice $\Lambda$ can be viewed as a Cayley graph of some discrete group $G$ (such as $\mathbb{Z}^D$).

* **Vertices**: Group elements $g \in G$.

* **Edges**: Connect $g$ and $g'$ if $g' = g \cdot s$ (where $s$ is an element of generating set $S$).

This structure ensures **homogeneity** of physical laws.

### A.5.2 Discrete Differential Forms

On discrete lattices, we cannot use standard differentials $dx$. Instead, we use **cochains**.

* **0-form (scalar field)**: Defined on vertices, $\phi(x)$.

* **1-form (gauge field)**: Defined on edges (links), $U(x, x+\mu)$.

* **2-form (curvature/magnetic field)**: Defined on faces (plaquettes), $U_{\Box}$.

Discrete version of **Stokes' Theorem**:

$$\sum_{\text{boundary}} A = \sum_{\text{bulk}} dA$$

This plays a key role in deriving the discrete form of Maxwell's equations (Chapter 6).

### A.5.3 Topological Winding Number

For a Hamiltonian map $H(k): T^D \to G$ defined on the Brillouin zone (torus $T^D$), its homotopy class is characterized by integer topological invariants (such as Chern numbers, winding numbers).

$$\mathcal{W} = \frac{1}{2\pi i} \oint \text{Tr}(H^{-1} dH)$$

This is the mathematical foundation for explaining **mass stability** and **fermion statistics** in Chapter 5 of this book. Non-trivial winding number ($\mathcal{W} \neq 0$) means the system is in a topological phase and cannot be continuously deformed to a massless (trivial) state.

# Appendix B: QCA Simulation Guide

**Appendix B: Guide to Simulating QCA Universes**

Physics is not just for deriving formulas; it is also for **running**. The "unitary computational universe" constructed in this book exists not only in abstract Hilbert space, but can also be fully simulated on classical digital computers (though with efficiency limitations).

This appendix aims to provide a practical programming guide for readers who wish to "create universes" with their own hands. We will demonstrate how to build a simple one-dimensional Dirac-QCA model satisfying Axiom $\Omega$ using Python, and observe the emergence of mass, wave packet diffusion, and Zitterbewegung phenomena.

This is not only verification of the theory, but also a preliminary attempt at transformation from "observer" to "builder."

-----

## B.1 Basic Architecture of the Simulator

A QCA simulator mainly consists of three core modules:

1. **State Register**: Stores the current global wave function $|\Psi(t)\rangle$.

2. **Evolution Engine**: Executes local unitary operators $\hat{U}$.

3. **Measurement Module**: Computes observables (such as position probability distribution, momentum spectrum).

### B.1.1 Data Structure

In a one-dimensional QCA, we have a ring containing $L$ lattice points (periodic boundary conditions). Each lattice point has two components (left-handed $L$ and right-handed $R$).

Therefore, the wave function can be represented by a complex array of size $(L, 2)$.

```python
import numpy as np
import matplotlib.pyplot as plt

class QCAUniverse:
    def __init__(self, L, mass_theta):
        """
        Initialize QCA universe
        L: Number of lattice points (spatial size)
        mass_theta: Mass parameter θ (corresponding to m*c^2*dt/hbar)
        """
        self.L = L
        self.theta = mass_theta
        
        # Wave function psi[x, 0] = psi_L, psi[x, 1] = psi_R
        self.psi = np.zeros((L, 2), dtype=np.complex128)
        
        # Construct local rotation matrix (Coin Operator)
        c, s = np.cos(self.theta), np.sin(self.theta)
        self.coin_op = np.array([[c, -1j*s], 
                                 [-1j*s, c]])
```

## B.2 Evolution Algorithm: Split Operator Method

According to the Dirac-QCA model (see Chapter 5 of the main text), the single-step evolution operator decomposes as:

$$\hat{U} = \hat{S} \cdot \hat{C}(\theta)$$

where $\hat{C}$ is local rotation (mixing left and right chirality), and $\hat{S}$ is conditional translation.

### B.2.1 Code Implementation

```python
    def step(self):
        """Execute one time evolution step: U = S * C"""
        
        # 1. Coin Step (local rotation)
        # Matrix multiplication for each lattice point: psi(x) = C . psi(x)
        # Use einsum for acceleration: 'ij,xj->xi' (i,j are spin components, x is spatial)
        self.psi = np.einsum('ij,xj->xi', self.coin_op, self.psi)
        
        # 2. Shift Step (conditional translation)
        # L component (index 0) moves left (x -> x-1) -> np.roll shift=-1
        # R component (index 1) moves right (x -> x+1) -> np.roll shift=+1
        # Note: Directions may be opposite depending on definition, here using standard QW convention
        psi_L = np.roll(self.psi[:, 0], -1)
        psi_R = np.roll(self.psi[:, 1], 1)
        
        self.psi[:, 0] = psi_L
        self.psi[:, 1] = psi_R
```

## B.3 Initial Conditions and Observation

To simulate a particle, we need to initialize a Gaussian wave packet.

```python
    def initialize_wavepacket(self, x0, k0, sigma):
        """
        Initialize Gaussian wave packet
        x0: Center position
        k0: Initial momentum
        sigma: Wave packet width
        """
        x = np.arange(self.L)
        # Gaussian envelope * plane wave
        envelope = np.exp(-(x - x0)**2 / (4 * sigma**2))
        plane_wave = np.exp(1j * k0 * x)
        
        psi_init = envelope * plane_wave
        
        # Assign to L and R components (here simply set equal, corresponding to spin pointing in x direction)
        self.psi[:, 0] = psi_init / np.sqrt(2)
        self.psi[:, 1] = psi_init / np.sqrt(2)
        
        # Normalize
        norm = np.sqrt(np.sum(np.abs(self.psi)**2))
        self.psi /= norm

    def measure_probability(self):
        """Return position probability distribution P(x)"""
        return np.sum(np.abs(self.psi)**2, axis=1)
    
    def measure_expectation_x(self):
        """Calculate position expectation value <x>"""
        P = self.measure_probability()
        x = np.arange(self.L)
        # Careful handling of centroid under periodic boundary conditions, here assuming wave packet far from boundaries
        return np.sum(x * P)
```

## B.4 Experimental Script: Verifying Zitterbewegung

Now, let's run this universe and verify the core prediction of Chapter 5: **Massive particles tremble microscopically.**

```python
def run_zitterbewegung_experiment():
    L = 2000
    theta = 0.1  # Small mass parameter
    steps = 500
    
    universe = QCAUniverse(L, theta)
    # Initialize particle with momentum 0
    universe.initialize_wavepacket(x0=L//2, k0=0, sigma=20)
    
    trajectory = []
    
    for t in range(steps):
        universe.step()
        x_avg = universe.measure_expectation_x()
        trajectory.append(x_avg)
        
    # Plotting
    t_axis = np.arange(steps)
    plt.plot(t_axis, trajectory)
    plt.title("Zitterbewegung Simulation (QCA)")
    plt.xlabel("Time Step")
    plt.ylabel("Expectation Position <x>")
    plt.show()

if __name__ == "__main__":
    run_zitterbewegung_experiment()
```

**Expected Results**:

Running the above code, you will see that `<x>` is not a straight line (stationary), but exhibits **high-frequency, small-amplitude sinusoidal oscillation** near the initial position.

* Oscillation frequency $\omega \approx 2\theta$.

* This is the precise discrete reproduction of the Zitterbewegung phenomenon in the Dirac equation.

* If `theta` is set to 0 (photon), oscillation disappears, and the wave packet will split and fly toward both ends at speed $c$ (or remain stationary if not mixed).

## B.5 Advanced Challenge: Curved Spacetime Simulation

To simulate gravity in QCA (Chapter 4 content), we need to introduce **non-uniform refractive index**.

This can be achieved by making the mass parameter $\theta$ or translation operator $\hat{S}$ depend on position $x$.

**Modification Idea**:

In the `step` function, instead of using a uniform `self.theta`, use an array `self.theta_field[x]`.

* Near a "black hole," set `theta[x]` very large (close to $\pi/2$), which will cause $v_{int} \to c$, $v_{ext} \to 0$.

* Running the simulation, you will find that wave packets entering this region will drastically slow down, wavelengths compress, exhibiting characteristics of gravitational redshift and spacetime curvature.

In this way, you are not just learning physics; you are **coding physics**. Every logic gate is a natural law of this toy universe.

# Appendix C: Proofs of Core Theorems

**Appendix C: Proofs of Core Theorems**

The main text of this book proposes many revolutionary physical propositions, such as "Light Path Conservation," "Gravity as Entropic Force," and "Probability as Counting." Although we provide physical images and heuristic derivations in the main text, as a serious theoretical system, these propositions must be built on rigorous mathematical proofs.

This appendix collects complete mathematical proofs of the three most core theorems supporting the theoretical framework of the entire book. These proofs do not rely on vague analogies, but are directly derived from Axiom $\Omega$ (unitary QCA) and standard operator algebra.

---

## C.1 Operator Algebraic Proof of Light Path Conservation Theorem

**Proposition**: In any discrete Dirac-QCA model satisfying translational invariance and local unitarity, the external group velocity $v_{ext}$ and internal evolution velocity $v_{int}$ of single-particle excited states satisfy $v_{ext}^2 + v_{int}^2 = c^2$.

**Proof**:

1. **Orthogonal Decomposition of Hamiltonian**

   In the continuous limit ($\Delta x, \Delta t \to 0$), the evolution operator $\hat{U}$ of a one-dimensional Dirac-QCA can generate an effective Hamiltonian $\hat{H}$. For a two-component spinor field $\psi = (\psi_L, \psi_R)^T$, the most general translationally invariant Hamiltonian form is:

   $$\hat{H} = c \hat{p} \sigma_z + m c^2 \sigma_x$$

   where:

   * $c$ is the maximum propagation speed on the lattice.

   * $\hat{p} = -i\hbar \partial_x$ is the momentum operator.

   * $m$ is the mass parameter determined by local mixing angle $\theta$ (see Chapter 5 of the main text).

   * $\sigma_z, \sigma_x$ are Pauli matrices, acting on internal chirality space respectively.

2. **Operator Norm of Total Evolution Rate**

   The total evolution rate (Fubini-Study velocity) of quantum states in Hilbert space is determined by the variance or norm of the Hamiltonian. For energy eigenstates $|\psi_E\rangle$, the modulus squared of their phase rotation rate over time is proportional to $E^2$:

   $$E^2 = \langle \psi_E | \hat{H}^2 | \psi_E \rangle$$

3. **Using Anticommutation Relations**

   Compute operator $\hat{H}^2$:

   $$
   \begin{aligned}
   \hat{H}^2 &= (c \hat{p} \sigma_z + m c^2 \sigma_x) (c \hat{p} \sigma_z + m c^2 \sigma_x) \\
   &= c^2 \hat{p}^2 \sigma_z^2 + m^2 c^4 \sigma_x^2 + c m c^2 \hat{p} (\sigma_z \sigma_x + \sigma_x \sigma_z)
   \end{aligned}
   $$

   Using algebraic properties of Pauli matrices:

   * $\sigma_z^2 = \mathbb{I}, \quad \sigma_x^2 = \mathbb{I}$

   * Anticommutation: $\{ \sigma_z, \sigma_x \} = \sigma_z \sigma_x + \sigma_x \sigma_z = 0$

   Cross terms vanish, yielding diagonalized energy operator:

   $$\hat{H}^2 = (c^2 \hat{p}^2 + m^2 c^4) \mathbb{I}$$

4. **Mapping of Velocity Components**

   Taking expectation value on both sides and dividing by $E^2$ (normalization):

   $$1 = \frac{c^2 p^2}{E^2} + \frac{m^2 c^4}{E^2}$$

   * **External velocity term**: According to group velocity definition $v_g = \frac{\partial E}{\partial p}$. From dispersion relation $E^2 = p^2 c^2 + m^2 c^4$ differentiation gives $2E dE = 2p c^2 dp$, hence $v_{ext} \equiv v_g = \frac{c^2 p}{E}$.

      Therefore, the first term $\frac{c^2 p^2}{E^2} = \frac{v_{ext}^2}{c^2}$.

   * **Internal velocity term**: Define internal velocity $v_{int}$ as the projection of rest energy (mass term) in total energy onto light speed, i.e., $v_{int} = c \cdot \frac{m c^2}{E}$.

      Therefore, the second term $\frac{m^2 c^4}{E^2} = \frac{v_{int}^2}{c^2}$.

5. **Conclusion**

   Substituting back:

   $$1 = \frac{v_{ext}^2}{c^2} + \frac{v_{int}^2}{c^2} \implies v_{ext}^2 + v_{int}^2 = c^2$$

   **Q.E.D.**

---

## C.2 Variational Derivation of Information-Gravity Variational Principle (IGVP)

**Proposition**: If spacetime geometry emerges as a macroscopic structure to maximize holographic entanglement entropy, then the metric field $g_{\mu\nu}$ must satisfy Einstein's field equations $G_{\mu\nu} = 8\pi G T_{\mu\nu}$.

**Proof**:

1. **Construct Total Entropy Functional**

   Define the total entropy $S_{tot}$ of the universe within a local causal diamond as the sum of geometric entropy $S_{geom}$ and matter entropy $S_{matter}$. According to the second law of thermodynamics, equilibrium states correspond to extremal points of entropy.

   Functional form (action $I = -S$):

   $$I[g] = I_{geom}[g] + I_{matter}[g, \psi]$$

2. **Geometric Entropy Term**

   According to the discrete structure of QCA, geometric entropy is proportional to the complexity of network connections. In the continuous limit, this is the curvature integral of the spacetime manifold (generalization of Wald entropy):

   $$I_{geom} = \frac{1}{16\pi G} \int_{\mathcal{M}} d^4x \sqrt{-g} R$$

   where $G$ is a constant related to Planck area element $l_P^2$.

3. **Matter Entropy Term**

   Matter entropy is determined by the partition function $Z[g]$ of matter fields $\psi$ on curved background, whose logarithm corresponds to effective action:

   $$I_{matter} = \int_{\mathcal{M}} d^4x \sqrt{-g} \mathcal{L}_m$$

4. **Variation with Respect to Metric**

   We seek geometric structures that make the total action stationary with respect to metric perturbations $\delta g^{\mu\nu}$.

   $$\delta I = \delta I_{geom} + \delta I_{matter} = 0$$

   * **Geometric part variation**:

     Using identity $\delta \sqrt{-g} = -\frac{1}{2} \sqrt{-g} g_{\mu\nu} \delta g^{\mu\nu}$ and Palatini identity $\delta R = R_{\mu\nu} \delta g^{\mu\nu} + \nabla_\mu v^\mu$ (boundary terms ignored):

     $$\delta I_{geom} = \frac{1}{16\pi G} \int d^4x \sqrt{-g} \left( R_{\mu\nu} - \frac{1}{2} R g_{\mu\nu} \right) \delta g^{\mu\nu}$$

   * **Matter part variation**:

     By definition, stress-energy tensor $T_{\mu\nu}$ is the response of matter action to metric:

     $$T_{\mu\nu} \equiv -\frac{2}{\sqrt{-g}} \frac{\delta I_{matter}}{\delta g^{\mu\nu}}$$

     Therefore:

     $$\delta I_{matter} = -\frac{1}{2} \int d^4x \sqrt{-g} T_{\mu\nu} \delta g^{\mu\nu}$$

5. **Deriving Field Equations**

   From $\delta I = 0$, for arbitrary $\delta g^{\mu\nu}$, the integrand must be zero:

   $$\frac{1}{16\pi G} (R_{\mu\nu} - \frac{1}{2} R g_{\mu\nu}) - \frac{1}{2} T_{\mu\nu} = 0$$

   Rearranging:

   $$R_{\mu\nu} - \frac{1}{2} R g_{\mu\nu} = 8\pi G T_{\mu\nu}$$

   **Q.E.D.**

---

## C.3 Trace-Class Counting Proof of Born Rule

**Proposition**: In discrete, unitary QCA systems satisfying environment-assisted invariance (Envariance), measurement outcome probabilities $P_k$ are uniquely determined by amplitude modulus squared $|c_k|^2$.

**Proof**:

1. **Schmidt Decomposition**

   Let system $S$ and environment $E$ be in an entangled state:

   $$|\Psi\rangle = \sum_{k=1}^N c_k |s_k\rangle |e_k\rangle$$

   where $|s_k\rangle, |e_k\rangle$ are orthogonal bases respectively.

2. **Rational Approximation and Fine-Graining**

   Assume $|c_k|^2$ are rational numbers (always true in discrete systems), let $|c_k|^2 = n_k / M$, where $M = \sum n_k$ is the total number of microstates.

   We can further decompose environment basis $|e_k\rangle$ into superposition of $n_k$ equally weighted micro-bases $|e_{k, \alpha}\rangle$:

   $$|e_k\rangle \to \frac{1}{\sqrt{n_k}} \sum_{\alpha=1}^{n_k} |e_{k, \alpha}\rangle$$

   Substituting into original state and ignoring overall phase:

   $$|\Psi'\rangle = \frac{1}{\sqrt{M}} \sum_{k=1}^N \sum_{\alpha=1}^{n_k} |s_k\rangle |e_{k, \alpha}\rangle$$

3. **Environment Exchange Symmetry**

   Now, state $|\Psi'\rangle$ is an equal-weight superposition of $M$ terms. Each term has the form $|s_k\rangle |e_{k, \alpha}\rangle$.

   For environment microstates $|e_{k, \alpha}\rangle$ and $|e_{j, \beta}\rangle$, there exists a unitary operator $\hat{U}_E$ that can exchange them without changing the $|s_k\rangle$ part.

   According to Envariance principle, physical probabilities should not depend on environment labels. Therefore, the probability of each micro-term $|s_k\rangle |e_{k, \alpha}\rangle$ appearing must be equal, all $p = 1/M$.

4. **Macroscopic Probability Summation**

   The probability $P_k$ that an observer measures macroscopic state $|s_k\rangle$ is the sum of probabilities of all compatible micro-terms:

   $$P_k = \sum_{\alpha=1}^{n_k} p = n_k \cdot \frac{1}{M} = \frac{n_k}{M}$$

   Recalling definition $|c_k|^2 = n_k / M$, we obtain:

   $$P_k = |c_k|^2$$

   **Q.E.D.**

---

**Author's Conclusion**:

These three proofs respectively establish the mathematical legitimacy of this book in **kinematics** (Light Path Conservation), **dynamics** (field equations), and **measurement theory** (Born rule). Together they form a logical closed loop, proving that physical reality can completely emerge from the single axiom of "unitary computation."

# Appendix D: Glossary of Key Terms

**Appendix D: Glossary of Key Terms**

This book constructs a completely new discourse system for physics. Many traditional physics terms (such as mass, gravity, time) are given new definitions based on information theory and discrete geometry in this book, while also introducing some proprietary new concepts.

To facilitate reader reference and eliminate ambiguity, this appendix collects core terminology from the entire book and provides their strict definitions within the framework of "unitary QCA ontology."

---

## A

* **Algorithmic Turmoil**

  * **Definition**: Refers to the universe's evolution being in a self-organized critical state that can never relax to thermal equilibrium (heat death).

  * **Mechanism**: Driven by the "Red Queen game" between agents (observers). To survive in competition, systems continuously increase computational complexity, leading to an eternal cycle of collapse of old structures and emergence of new structures.

  * **Source**: Chapter 8, Section 8.3.

* **Agent**

  * **Definition**: A subsystem in QCA networks possessing a **Markov blanket** (boundary) and **internal model** (self-referential structure).

  * **Characteristics**: Agents actively consume free energy to resist environmental thermalization, exhibiting seemingly "purposeful" behavior (minimizing prediction error).

  * **Source**: Chapter 8, Section 8.1.

---

## C

* **Causal Locality**

  * **Definition**: Structural property of QCA evolution operator $\hat{U}$, requiring that the next-moment state of any node depends only on the states of nodes within its finite neighborhood.

  * **Corollary**: Directly leads to light cone structure and existence of maximum signal velocity $c$, prohibiting action at a distance.

  * **Source**: Chapter 2, Section 2.3; Chapter 3, Section 3.1.

* **Connection Field / Link Variable**

  * **Definition**: Unitary operator $U_{yx}$ defined on lattice connection edges (links), used to "translate" differences in local Hilbert space bases between adjacent nodes $x$ and $y$.

  * **Physical correspondence**: Corresponds to gauge potential $A_\mu$ in gauge field theory. The holonomy on closed loops corresponds to field strength (such as electromagnetic fields, gluon fields).

  * **Source**: Chapter 6.

---

## I

* **Information-Gravity Variational Principle (IGVP)**

  * **Definition**: A thermodynamic variational principle asserting that equilibrium states of spacetime geometry correspond to maximum holographic entanglement entropy.

  * **Formula**: $\delta (S_{geom} + S_{matter}) = 0 \implies G_{\mu\nu} = 8\pi G T_{\mu\nu}$.

  * **Significance**: Interprets Einstein's field equations as the equation of state of the information manifold, rather than fundamental dynamical laws.

  * **Source**: Chapter 4, Section 4.3.

* **Information Mass ($M_I$)**

  * **Definition**: Physical quantity measuring the "weight" of internal structure of complex systems (such as observers).

  * **Formula**: $M_I \propto \Phi \cdot \mathcal{D}$, where $\Phi$ is integrated information and $\mathcal{D}$ is logical depth.

  * **Effect**: Systems with high $M_I$ have enormous "information inertia," tending to be stationary in external space (motionless), and produce significant gravitational effects.

  * **Source**: Chapter 8, Section 8.2.

* **Light Path Conservation (Conservation of Information Celerity)**

  * **Definition**: Core theorem of the entire book. States that the total information update amount of physical entities within Planck time is constant.

  * **Formula**: $v_{ext}^2 + v_{int}^2 = c^2$.

  * **Significance**: Unifies special relativity ($v_{ext}$) with quantum mechanics ($v_{int}$), explaining the complementary relationship between time dilation and mass.

  * **Source**: Chapter 3, Section 3.2.

---

## L

* **Local Information Volume Conservation**

  * **Definition**: Geometric constraint of unitary evolution in the continuous limit. Requires that deformation of macroscopic metrics cannot change the effective number of microscopic degrees of freedom contained per unit coordinate volume.

  * **Formula**: $\eta_t \cdot \eta_x^3 = 1$ (in 3+1 dimensions).

  * **Application**: Corrects scalar gravity theory, derives correct optical metric, solves the coefficient problem of light deflection angle.

  * **Source**: Chapter 4, Section 4.2.

---

## O

* **Optical Metric**

  * **Definition**: Effective metric describing propagation of light (and matter) in gravitational fields, generated by refractive index $n(x)$.

  * **Form**: $ds^2 = -n^{-2} c^2 dt^2 + n^2 dl^2$.

  * **Essence**: Reflects that gravity is not curvature of spacetime, but non-uniform distribution of information processing density in the medium (QCA network).

  * **Source**: Chapter 4, Section 4.2.

---

## T

* **Topological Impedance**

  * **Definition**: Microscopic mechanism of inertial mass. Refers to the lagged response exhibited by an information flow structure with non-trivial topology (winding number $\neq 0$) when changing its motion state.

  * **Mechanism**: When $v_{ext} \to c$, internal refresh rate $v_{int} \to 0$, causing the system unable to timely process external perturbations, manifesting as inertial divergence.

  * **Source**: Chapter 5, Section 5.3.

* **Unified Time Identity**

  * **Definition**: Equivalence relation between microscopic time flow rate and local density of states.

  * **Formula**: $\kappa(E) = 2\pi \rho(E)$.

  * **Significance**: Explains gravitational redshift—density of states is higher deep in potential wells, requiring longer time to traverse states, hence time slows down.

  * **Source**: Related papers and Chapter 4 background.

---

## U

* **Unitarity**

  * **Definition**: Evolution operator $\hat{U}$ satisfies $\hat{U}^\dagger \hat{U} = \mathbb{I}$.

  * **Physical meaning**: Conservation of information. Past, present, and future contain strictly equal amounts of information; no information is completely erased or created from nothing. It is the mathematical root of Light Path Conservation and Born's rule.

  * **Source**: Chapter 2, Section 2.2.

---

## Z

* **Zitterbewegung (Trembling)**

  * **Definition**: Rapid oscillatory flipping between positive and negative chirality occurring at microscopic scales for massive particles.

  * **New interpretation**: Not a mathematical artifact, but an "internal cycle" particles are forced to perform when unable to move at full speed, to maintain light path conservation. Its frequency is the measure of rest mass.

  * **Source**: Chapter 5, Section 5.1.

---

**(End of main text and appendices)**

# 1.1 Zeno's Paradox and the Ghost of Continuum

We often think that "continuity" is the nature of reality. From smooth flowing water to continuously extending straight lines, the classical world seems to be composed of infinitely divisible matter and spacetime. This intuition is deeply rooted in our mathematical tools: the core of calculus is limits and infinitesimals.

However, when we try to peer into the deepest layers of physics with the "continuum" magnifying glass, we find it full of bizarre cracks. This chapter will peel away the illusion of continuity, revealing that discrete ontology is not only an assumption of computational cosmology, but the only cure for the deep contradictions of modern physics.

## 1.1.1 Zeno's Dichotomy Paradox

In the 5th century BCE, Zeno of Elea proposed the famous "dichotomy paradox": If Achilles wants to go from point A to point B, he must first reach the midpoint $C_1$; to reach $C_1$, he must first reach the midpoint $C_2$ between $A$ and $C_1$... This infinite division continues, and Achilles seems unable to take even one step, because he must first complete infinitely many tasks.

Although classical calculus solved this problem mathematically using limit convergence ($\sum_{n=1}^\infty (1/2)^n = 1$), in physical ontology, Zeno's ghost has never truly left.

## 1.1.2 The Information Density Catastrophe of Continuum

If we assume spacetime is continuous, i.e., a point set containing the cardinality of the real number set $\mathbb{R}$, then any finite spatial segment $[0, L]$ contains uncountably infinitely many points. This means that a particle moving must "interact" or "update position" with every point on its path.

This hides an astonishing physical cost: **infinite information density**.

To precisely describe the position $x$ of a point in the continuum, we need an infinitely long bit string (e.g., $x = 0.10110...$). If physical laws are local and depend on these precise positions, then any tiny volume contains infinite information. This might be tolerable in classical mechanics (we assume God has infinite hard drives), but in a universe combining Heisenberg's uncertainty principle with general relativity, this directly leads to disaster.

## 1.1.3 Ultraviolet Divergence in Quantum Field Theory

Consider quantum field theory (QFT). In QFT, to calculate particle interactions, we must integrate over all possible momenta $k$. If space is continuous, momentum $k$ can tend to infinity (corresponding to wavelength $\lambda \to 0$). This "ultraviolet divergence" forced physicists to invent renormalization techniques—artificially cutting off the high-energy part, retaining only the low-energy effective theory we can observe.

Renormalization is extremely successful computationally, but ontologically "ugly." It suggests there is a black hole at the bottom of our theory that we dare not touch. Feynman once admitted: "I think that's sweeping dust under the rug."

## 1.1.4 The Solution of Discrete Ontology

Discrete ontology provides a radical solution: **What if Zeno was right?**

What if Achilles doesn't need to pass through infinitely many midpoints, but jumps from one square to the next like on a chessboard? What if at the deepest level, there is no "infinitesimal" distance, but only a smallest, indivisible "Planck lattice point"?

This leads to our first axiomatic foundation: **Physical reality has no infinitesimals.**

### Corollary 1.1.1 (Natural Cutoff)

There exists a fundamental length scale $l_P$ (Planck length) such that the spatial resolution $\Delta x \ge l_P$ for any physical process.

### Corollary 1.1.2 (Hilbert Space Finiteness)

For any finite volume $V$, the number of orthogonal basis states it contains $N = \dim(\mathcal{H}_V)$ is a finite integer.

## 1.1.5 The Necessity of Discreteness

Once we accept discreteness, all infinite divergences instantly disappear. Zeno's paradox is resolved—motion is not gliding on a continuum, but **state updates** between discrete states. Achilles only needs to complete finitely many updates to reach the destination.

Just as cursor movement on a computer screen looks smooth but is actually pixel (Pixel) on/off switching; our universe appears continuous only because our observation resolution is too coarse to detect the underlying granularity.

Physics does not need real numbers $\mathbb{R}$. Real numbers are merely statistical approximations of discrete grids at macroscopic scales. Just as continuous fluids in fluid mechanics are statistical averages of many discrete molecules, **spacetime continuum is merely the statistical average of discrete information flow.**

We are fundamentally expelling the "ghost of continuum" that has haunted physics for two thousand years. What remains is a clean, finite, computable universe.

# 1.2 Black Hole Entropy and Bekenstein Bound: The Universe as a Finite-Capacity Hard Drive

In the previous section, we pointed out the infinite divergences caused by the continuum hypothesis and proposed the necessity of natural cutoff for physical reality at the Planck scale. If ultraviolet divergence is merely theoretical "ugliness," then the discovery of black hole thermodynamics provides solid, even mandatory physical evidence for this discrete ontology.

## 1.2.1 Bekenstein's Insight: Does Information Have Volume?

In the early 1970s, when physicists were still debating whether black holes were merely mathematical singularities of general relativity, Jacob Bekenstein proposed a seemingly naive but highly subversive question: If I pour a cup of hot tea (with entropy) into a black hole, does the total entropy of the universe decrease? Because the black hole swallows everything, including information.

If the second law of thermodynamics is universal, black holes themselves must possess entropy.

Through thought experiments, Bekenstein discovered that black hole entropy $S_{BH}$ should not be proportional to its volume (like ordinary thermodynamic systems), but proportional to the **surface area** $A$ of its horizon. Subsequently, Stephen Hawking confirmed this relationship through semiclassical calculations and gave that famous formula:

$$
S_{BH} = \frac{k_B c^3}{4 G \hbar} A = \frac{A}{4 l_P^2}
$$

where $l_P = \sqrt{G\hbar/c^3} \approx 1.6 \times 10^{-35}$ meters is the Planck length.

This formula is one of the most beautiful equations in the history of physics, unifying thermodynamics ($k_B$), relativity ($c$), gravity ($G$), and quantum mechanics ($\hbar$). But for our discussion, its most important significance lies in revealing the geometric nature of information.

## 1.2.2 Planck Pixels and Finite Capacity

Let us carefully examine the meaning of this formula. Entropy in information theory corresponds to the number of information bits ($S = N \ln 2$). The Bekenstein-Hawking formula tells us that every $4l_P^2$ area (four Planck areas) on the black hole horizon can store exactly 1 bit of information.

This is a startling conclusion: **Information is not a continuously distributed fluid, but discretely "paved" on the surface of spacetime.**

Furthermore, Bekenstein proposed the **Bekenstein Bound**: For any spherical spatial region containing energy $E$ and radius $R$, the maximum entropy (i.e., maximum information) $S_{max}$ it can contain is finite and satisfies:

$$
S \le \frac{2\pi k_B R E}{\hbar c}
$$

When this region collapses into a black hole, entropy reaches the maximum value $S = A/4l_P^2$.

This means: **For any finite volume of space in the universe, no matter how we compress matter or energy into it, the total number of quantum states $W = e^S$ it can contain is strictly a finite integer.**

## 1.2.3 The End of Continuum

If space were continuous, then even a tiny needle tip could theoretically contain infinite information (because we could infinitely subdivide coordinates). But the black hole entropy formula directly negates this. It shows that if we pile too much information in a region, space itself will "crash" (become a black hole) due to gravitational collapse, thus locking the information limit.

This provides the strongest physical support for our **discrete ontology**:

1. **Space is not a container, but a storage medium**: The geometric area of space directly corresponds to storage capacity (hard drive size). Planck length $l_P$ is the smallest magnetic domain (Bit) of this cosmic hard drive.

2. **Holographic Principle**: Since the maximum information in a volume is determined by its surface area, this suggests that the "bulk" information of three-dimensional space can actually be losslessly encoded on a two-dimensional boundary. Like holograms, this dimensional reduction encoding is mathematically possible only in discrete systems (continuum cardinalities differ, preventing one-to-one mapping).

## 1.2.4 Conclusion: The Universe as a Finite State Machine

Synthesizing the above derivations, we must accept an extremely profound conclusion: Our universe, at any given moment, for any finite observation horizon, contains a finite total amount of information.

If state space is finite and evolution rules are unitary (information conservation), then the universe is essentially equivalent to a **finite state machine** or a **quantum cellular automaton (QCA)** running on a huge but finite lattice.

Infinity not only does not exist physically, but is also redundant in information theory. Black holes are not merely celestial bodies; they are the "memory overflow" protection mechanism of this giant cosmic computer, reminding us that the granularity of physical reality has a bottom line.

In the next section, we will shift our gaze from macroscopic black holes to microscopic qubits, exploring how "matter" itself emerges from pure information.

# 1.3 Information Realism: Bits and Qubits as the Atoms of Matter

After revealing the pathology of continuum and the revelation of black hole entropy, we face an unavoidable ontological question: If the universe is not composed of continuous fields or matter, what are its "atoms"?

This section will establish the core viewpoint of this book—**Information Realism**. We will argue that the most fundamental constituent units of the physical universe are not electrons, quarks, or strings, but **Bits** and **Qubits**. Matter, energy, space, and time are all macroscopic phenomena emerging from interactions of these underlying information units.

## 1.3.1 "It from Bit"

John Wheeler, in his visionary paper "Information, Physics, Quantum: The Search for Links," proposed the famous slogan: "It from Bit." He wrote:

> "Every 'it'—every particle, every force field, even spacetime itself—derives its function, its meaning, its very existence entirely... from an apparatus-elicited answer to yes-or-no questions, binary choices, bits."

This view was radical at the time, but today it has become a cornerstone of quantum information physics. If we view the universe as a physical system, then the most fundamental description of this system's state is answering a series of "yes/no" questions (e.g., is spin up or down? Is lattice point empty or full?).

However, classical bits (0 or 1) are insufficient to describe the interference and entanglement we observe in the microscopic world. Therefore, we must upgrade Wheeler's dictum to: **"It from Qubit."**

## 1.3.2 Qubits: The Minimal Units of Physical Reality

In our QCA model, the universe is discretized into tiny cells. The most fundamental physical quantity carried by each cell is a **qubit**.

A qubit state $|\psi\rangle$ is a unit vector in two-dimensional complex Hilbert space $\mathbb{C}^2$:

$$
|\psi\rangle = \alpha |0\rangle + \beta |1\rangle
$$

where $|\alpha|^2 + |\beta|^2 = 1$.

Why do we regard Qubits as more fundamental "atoms" than electrons?

1. **Universality**: Any finite-dimensional quantum system (regardless of whether its physical carrier is photons, ions, or superconducting circuits) can be decomposed into tensor products of Qubits. Qubits are the universal currency of quantum information.

2. **Non-locality and Entanglement**: Two Qubits can form entangled states (such as Bell states), and this non-local correlation cannot be simulated by classical particles. As we will see in subsequent chapters, it is precisely this entanglement that "stitches" discrete cells together, emerging as continuous spatial geometry.

3. **Holography**: A system containing $N$ Qubits has a maximum information capacity strictly limited by $N$. This perfectly matches the "finite capacity" we saw in black hole entropy.

Therefore, we define the bottom layer of physical reality no longer as $x, y, z, t$, but as state vectors $|\Psi\rangle$ in Hilbert space. Spatial position $x$ is merely the **index** of qubits in the lattice network, while time $t$ is merely the **counter** of logic gate operations.

## 1.3.3 From Information to Matter: Reconstruction of Elementary Particles

If Qubits are bricks, how are the familiar electrons and photons "built"?

In traditional physics, particles are viewed as point-like entities moving in spacetime. But in the QCA framework, **particles are excitation patterns in information networks**.

Imagine a two-dimensional grid filled with Qubits.

* **Vacuum** corresponds to some low-entanglement ground state (e.g., all spins down $|00...0\rangle$).

* **Photons** correspond to a "flip" wave propagating on the grid (e.g., $|010...0\rangle \to |001...0\rangle$). Since there is no mechanism preventing this flip's transmission, it propagates at maximum speed (speed of light).

* **Electrons** correspond to a complex **topological knot**. Like a knot on a rope, it is a local information structure that not only contains flips but also phase winding. This winding prevents it from dissipating at light speed, forcing it to maintain its existence in place—this is the origin of **mass**.

In this picture, matter is not a "thing" but a "process." An electron is not a small ball named "electron"; it is a self-maintaining, self-referential information vortex in the Qubit ocean.

## 1.3.4 Conclusion: Computational Ontology

At this point, we have completed a thorough reconstruction of physics ontology:

1. **Dematerialization**: There is no "hard" matter, only soft information.

2. **De-backgrounding**: There is no a priori spacetime stage, only interrelations (entanglement) between qubits.

3. **Discretization**: There are no infinitesimals, only finite logical steps.

We no longer ask "what is the universe made of," but "how does the universe compute."

After establishing this ontological foundation, we can finally take the most crucial step—writing down the single law governing the operation of all these Qubits. This is the theme of the next chapter: **The Ultimate Axiom $\Omega$**.

# 2.1 Axiom Statement: The Universe is a Quantum Cellular Automaton (QCA) Operating on Discrete Lattice Points Following Local Unitary Evolution Rules

This is the core of this book, and the only foundation stone for constructing the entire edifice of physics.

We will abandon all complex, phenomenological physical assumptions (such as mass, charge, spacetime curvature, wave function collapse, etc.), retaining only the purest computational structure.

> **Ultimate Axiom $\Omega$ (The Axiom of Unitary QCA)**
>
> Physical reality $\Psi$ is equivalent to a quantum information processing system defined on a discrete lattice graph $\Lambda$, following local unitary evolution rules $\hat{U}$.
>
> Formally expressed as a triple $(\Lambda, \mathcal{H}, \hat{U})$:
>
> 1. **State Space ($\Lambda, \mathcal{H}$)**: The universe is a countable graph $\Lambda$, where each node $x \in \Lambda$ is associated with a finite-dimensional complex Hilbert space $\mathcal{H}_x \cong \mathbb{C}^d$ (i.e., qudit). The state space of the entire system is the tensor product of local spaces $\mathcal{H}_{total} = \bigotimes_{x \in \Lambda} \mathcal{H}_x$.
>
> 2. **Evolution Rule ($\hat{U}$)**: The system's evolution with discrete time steps $t \in \mathbb{Z}$ is driven by a global operator $\hat{U}$:
>
>    $$|\Psi(t+1)\rangle = \hat{U} |\Psi(t)\rangle$$
>
> 3. **Constraints**:
>
>    * **Unitarity**: $\hat{U}^\dagger \hat{U} = \mathbb{I}$. This means information (modulus of quantum states) is strictly conserved during evolution, neither created nor destroyed.
>
>    * **Locality**: $\hat{U}$ can be decomposed as a product of local operators $\hat{U} = \prod_{k} \hat{U}_k$ (or its finite-depth circuit), and each $\hat{U}_k$ acts only on finitely many adjacent nodes on $\Lambda$. This means no action at a distance; information propagation speed is limited by lattice connectivity.
>
>    * **Translation Invariance (Homogeneity)** (optional but usually assumed): Evolution rules $\hat{U}_x$ are identical across the entire graph $\Lambda$ (except possible boundary conditions). This corresponds to the universality of physical laws.

---

This axiom seems simple but contains astonishing power. It not only defines what "existence" is (quantum information), but also defines what "change" is (unitary computation).

In the next few sections, we will deeply analyze each clause of this axiom, explaining **why** the universe must be this way, not that way. We will see that these seemingly abstract mathematical requirements actually directly correspond to familiar physical iron laws—conservation of probability, causality, and conservation of energy.

# 2.2 Why Unitarity? — Conservation of Probability and Logical Consistency

When we claim the universe is a **unitary** QCA, we are actually making a commitment about the deepest layer of existence: **information conservation**.

In standard quantum mechanics textbooks, unitarity is usually expressed as an abstract mathematical property: evolution operator $U$ must satisfy $U^\dagger U = \mathbb{I}$. For physics students, this means the modulus squared of the wave function (sum of probabilities) is always 1. Without unitarity, particles might vanish into thin air, or the sum of probabilities might become 1.5, destroying all statistical predictive power.

But in our discrete ontology, unitarity is not merely "conservation of probability"; it is the physical embodiment of **logical consistency**.

## 2.2.1 Reversibility and the Eternity of Information

A core property of unitary transformations is **reversibility**. If $U$ is unitary, then the inverse operator $U^{-1} = U^\dagger$ necessarily exists and is also unitary.

This means that given the current state $|\Psi(t)\rangle$, we can not only uniquely predict the future $|\Psi(t+1)\rangle$, but also uniquely trace back the past $|\Psi(t-1)\rangle$. **The current state contains all information about past and future.**

What happens if the universe is not unitary?

1. **Information Loss (Non-injective)**: If two different states $|A\rangle$ and $|B\rangle$ evolve to the same state $|C\rangle$, then when we are in $|C\rangle$, memory of the past is permanently erased. This corresponds to irreversible processes (entropy increase) in thermodynamics. But at the fundamental physics level, if we believe microscopic laws are symmetric, such erasure is forbidden.

2. **Information Created from Nothing (Non-surjective)**: If some state $|D\rangle$ has no predecessor, how did it "suddenly appear"? This violates the continuity of causality.

Therefore, **the unitarity axiom is equivalent to the "law of information conservation."** In this universe, no bit is truly deleted, and no bit is created from nothing. The macroscopic "forgetting" or "dissipation" we see is merely information transferring from local degrees of freedom to environmental degrees of freedom we cannot track (entanglement diffusion). For the wave function of the entire universe, entropy is always constant (and zero, if we start from a pure state).

## 2.2.2 The Rigidity of Logic

Looking deeper, unitarity ensures the **rigidity** of physical logic.

In the derivation of the Light Path Conservation Theorem, we see that the relation $c^2 = v_{ext}^2 + v_{int}^2$ directly stems from unitary decomposition of operators. If non-unitary evolution were allowed, this "circle" would deform, causing physical constants (such as light speed $c$ or Planck constant $\hbar$) to fluctuate with time or state.

A non-unitary universe is a logically "soft" universe. In that universe, $1+1$ might equal 2 today and 1.9 tomorrow. This is not only a disaster for physics, but also for mathematics.

By forcing $\hat{U}$ to be unitary, we are actually saying: **The underlying logic of the universe is unbreakable.** No matter how violent the interactions (black hole mergers, Big Bang), the underlying Hilbert space structure remains unchanged, angles between vectors (orthogonality) remain unchanged. This geometric rigidity is the fundamental reason we can describe the physical world with mathematics.

## 2.2.3 Reconciliation with the Measurement Problem

Readers might ask: "If we see wave function collapse (non-unitary process), how can we say the universe is unitary?"

This is exactly what we will solve in **Chapter 7**. Here, we emphasize: **The unitarity claimed by Axiom $\Omega$ is global, microscopic unitarity.**

The so-called "non-unitary measurement" is only because the observer is also part of the system (subsystem), and can only see a tiny slice of the entire universe's Hilbert space. When information flows out of this slice (into the environment), unitarity seems broken to the observer. But this is a perspective illusion, like feeling centrifugal force in a rotating room.

As long as we expand our view to the entire universe (or a sufficiently large closed system), unitarity is perfectly restored.

Therefore, insisting on the unitarity axiom is insisting on the **many-worlds (or many-histories)** perspective—all possible historical branches truly exist and evolve in parallel, together maintaining the modulus conservation of the entire universe's wave function.

After establishing that "the universe does not forget," the next question is: How does the universe know "where is where"? This is the theme of the next section: **Locality**.

# 2.3 Why Locality? — Avoiding "God's Eye View" and Action at a Distance

After establishing "unitarity" as the rigid skeleton of cosmic logic, we face a second fundamental question: How is this universe's structure organized?

In the Ultimate Axiom $\Omega$, we force the evolution operator $\hat{U}$ to be **local**. This means that within each time step $\Delta t$, any cell (Qubit) can only interact with cells directly adjacent to it on the lattice graph $\Lambda$.

Why must it be local? Why can't we allow a cell to instantly exchange information with a cell at the other end of the universe? This is not just to conform to the empirical facts of special relativity, but a necessary prerequisite for logically constructing a **self-consistent, evolvable universe**.

## 2.3.1 The End of God's Eye View: Full Connection Means No Structure

Imagine a **non-local** universe. In this universe, there exists a direct interaction channel between any two particles. No matter how far apart they are (assuming the concept of "distance" still exists), a flip of one particle can directly change another particle's state in the next instant.

In graph-theoretic language, this corresponds to a **complete graph**.

If the universe is fully connected, then:

1. **Geometry Disappears**: Since any two points are "adjacent," the concept of "space" loses meaning. There is no "near" and "far," no "inside" and "outside"; the entire universe collapses into a zero-dimensional point.

2. **Complexity Collapse**: To calculate the next state, every cell needs to know the current state of all other cells in the entire universe. This means every local computation requires infinite (or universe-scale) information input and processing capability. This actually requires every particle to possess a "God's eye view."

Therefore, **locality is not a limitation, but creation.** It is precisely by limiting the range of interactions (cutting off the vast majority of connections) that the universe acquires topological structure, dimensions, and so-called "space."

**Space is essentially the sparsity of interactions.**

## 2.3.2 The Birth of Geometry: Distance as Delay

Once we impose locality constraints, **maximum signal propagation speed (light speed $c$)** automatically emerges as a logical necessity.

In QCA, information can only propagate to neighbors within one step $\Delta t$. To propagate to a node $N$ steps away, $N$ time steps are necessary.

$$\text{Distance} \equiv \text{Minimum Communication Time}$$

This reveals the ontological definition of light speed $c$ in physics:

**$c$ is not the speed limit of object motion; it is the conversion factor for causal chain extension.**

If we allow non-local action (action at a distance), it means $c \to \infty$. In this case, causality would no longer have temporal order protection, and grandfather paradoxes would become inevitable. The locality axiom is actually the **guardian of causality**. It ensures events occur sequentially and influence transmission has a process. As physicist John Wheeler said: "Time is to prevent everything from happening at once; space is to prevent everything from happening in the same place."

## 2.3.3 Clarification: Quantum Non-locality vs. Dynamical Locality

Here we must clarify a concept that often confuses beginners: the difference between **Bell Non-locality** and **Dynamical Locality**.

Entangled states in quantum mechanics do exhibit non-local correlations—measuring one particle seems to instantly determine another particle's state. Does this violate our locality axiom?

**The answer is: No violation.**

1. **Dynamical Locality (what we insist on)**: This refers to the structure of the Hamiltonian or evolution operator. In QCA, $\hat{H} = \sum \hat{h}_{i, i+1}$. This ensures **interactions** only occur between neighbors. You cannot "touch" a distant particle.

2. **Bell Non-locality (of measurement results)**: This refers to correlations of **states**. Although two particles are far apart, they share a history (once had local interaction somewhere, then separated). This correlation is a **pre-established resource**, not **instantaneous communication**.

In a QCA universe, entanglement can exist between arbitrarily distant nodes (as long as they have causal connection in the past), but **utilizing** this entanglement to transmit information and produce physical effects must follow the point-by-point local rules.

This is like two people each taking a walkie-talkie to opposite ends of Earth. The walkie-talkies (entanglement) connect them, but radio waves (interactions) must traverse space at light speed.

## 2.3.4 Conclusion: A Universe Without Supermen

The locality axiom is actually a **humble** declaration of physics. It states:

* There is no "control center" in the universe.

* Every cell is equal and autonomous.

* Macroscopic order is not imposed top-down by global commands, but emerges bottom-up from countless local microscopic interactions.

This is not only a principle of physics, but also the fundamental reason complex systems (such as life, brains, society) can exist.

At this point, we have established the universe's **software logic (unitarity)** and **hardware architecture (locality)**. In the next section, we will formalize these concepts and write down the complete mathematical definition of the ultimate axiom.

# 2.4 Formal Definition: Graph $\Lambda$, Hilbert Space $\mathcal{H}$, and Update Operator $\hat{U}$

In the previous section, we established the universe's information-conserving logic through "unitarity" and endowed the universe with spatial structure and causal constraints through "locality." Now, we must transform these physical intuitions into rigorous mathematical language. This section will provide the complete mathematical formal definition of the Ultimate Axiom $\Omega$; this triple $(\Lambda, \mathcal{H}, \hat{U})$ will become the only axiomatic foundation we are allowed to use in all subsequent derivations in this book.

## 2.4.1 Definition 2.4.1: Discrete Geometric Background $\Lambda$

We define "space" as a countable, infinite (or extremely large finite) graph.

> **Definition (Lattice Graph $\Lambda$)**:
>
> Let $\Lambda = (V, E)$ be an undirected graph, where $V$ is the vertex (cell) set and $E$ is the edge (adjacency relation) set.
>
> We require $\Lambda$ to satisfy the following properties:
>
> 1. **Regularity**: The degree of each vertex is finite and constant, denoted $k$. This corresponds to spatial homogeneity.
>
> 2. **Connectivity**: The graph is connected, i.e., there exists a finite-length path between any two points.
>
> 3. **Discrete Metric**: Define the distance $d(x, y)$ between two points $x, y \in V$ as the number of edges in the shortest path connecting them.

*Note*: The simplest example is the $D$-dimensional integer lattice $\mathbb{Z}^D$, but this is not the only choice. Penrose tilings or triangulations of Regge differential geometry are also allowed. But in elementary derivations of this book, we usually default to $\Lambda \cong \mathbb{Z}^3$ to simplify discussion.

## 2.4.2 Definition 2.4.2: Quantum State Space $\mathcal{H}$

We define "matter" as quantum information distributed on the graph.

> **Definition (Local and Global Hilbert Spaces)**:
>
> 1. **Local Space**: For each vertex $x \in V$, associate a finite-dimensional complex Hilbert space $\mathcal{H}_x \cong \mathbb{C}^d$. $d$ is called the local dimension; for qubit systems, $d=2$.
>
> 2. **Global Space**: The Hilbert space $\mathcal{H}_{\text{total}}$ of the entire system is the tensor product of all local spaces:
>
>    $$\mathcal{H}_{\text{total}} = \bigotimes_{x \in V} \mathcal{H}_x$$
>
> 3. **Basis**: An orthonormal basis of the entire system can be expressed as $|\mathbf{s}\rangle = \bigotimes_x |s_x\rangle$, where $s_x \in \{0, 1, \dots, d-1\}$ is the classical state of node $x$.

*Note*: Since $V$ may be infinite, rigorous mathematical treatment requires infinite tensor product structures of von Neumann algebras or $C^*$ algebras. But physically, we always focus on finite excitation states, so we can treat it as a Hilbert space with countable basis.

## 2.4.3 Definition 2.4.3: Dynamical Evolution $\hat{U}$

We define "time" as discrete update steps $t \in \mathbb{Z}$, and "physical laws" as global update operators.

> **Definition (Global Unitary Evolution $\hat{U}$)**:
>
> The system's state evolves with discrete time steps $t$:
>
> $$|\Psi(t+1)\rangle = \hat{U} |\Psi(t)\rangle$$
>
> where operator $\hat{U}$ must satisfy:
>
> 1. **Unitarity**: $\hat{U}^\dagger \hat{U} = \hat{U} \hat{U}^\dagger = \mathbb{I}$.
>
> 2. **Causal Locality**: $\hat{U}$ has a finite-depth quantum circuit structure. Specifically, $\hat{U}$ can be decomposed as a product of local gates:
>
>    $$\hat{U} = \prod_{\text{partitions } P} \left( \bigotimes_{C \in P} \hat{u}_C \right)$$
>
>    where each $\hat{u}_C$ acts only on nodes within the neighborhood $\mathcal{N}(x)$ centered at $x$ with radius $r$ (interaction range).
>
> 3. **Translation Invariance** (for $\Lambda = \mathbb{Z}^D$): Let translation operator be $\hat{T}_\mathbf{a}$, then $[\hat{U}, \hat{T}_\mathbf{a}] = 0$. This means physical laws are identical everywhere in space.

## 2.4.4 Physical Interpretation: Definition of Light Speed $c$

In this formal system, the natural constant $c$ (light speed) is no longer an empirically measured value, but a **derived quantity** defined by graph structure and update rules.

Within one time step $\Delta t = 1$, local operator $\hat{u}_C$ can only propagate information (entanglement) from node $x$ to its neighbors $y \in \mathcal{N}(x)$.

Let lattice spacing be $a = l_P$ (Planck length), then the maximum propagation speed of information is strictly defined as:

$$c \equiv \frac{a}{\Delta t} \times r$$

where $r$ is the interaction radius of local gates (usually $r=1$).

This is the microscopic origin of light cones in physics: **It is the causal boundary defined by the connectivity of local logic gates.** Any attempt to exceed this speed for information transmission is equivalent to requiring $\hat{U}$ to contain non-local long-range connections, thus violating the locality definition.

At this point, we have completed the complete definition of the universe's "source code." These three definitions—**graph, state, operator**—constitute the entire axiomatic foundation for deriving all physical phenomena. Beyond this, there is nothing else.

In the following chapters, we will start this automaton and see how it emerges the miracles of special relativity from these simple rules.

# 3.1 The Discrete Origin of Light Cones: Deriving Maximum Signal Velocity $c$ from Lattice Hopping

In the Ultimate Axiom $\Omega$, we only defined a static graph $\Lambda$ and a dynamic rule $\hat{U}$. We did not presuppose "relativity," "Lorentz symmetry," or that sacred constant $c \approx 299,792,458$ meters/second.

However, remarkably, as soon as this automaton starts running, the **light cone** structure automatically emerges like crystal growth from the logical foundation. The core of special relativity—causality and limiting velocity—is not traffic laws imposed by God on the universe, but inevitable properties of discrete information processing systems.

This chapter will prove: **Light speed $c$ is merely the ratio of the "grid constant" and "refresh rate" of spacetime lattice points.**

In Newton's continuous spacetime, instantaneous action at a distance is mathematically allowed. Forces can propagate at infinite speed; causality has no boundary. But in a discrete QCA universe, this is strictly forbidden by the "locality" clause in Axiom $\Omega$.

## 3.1.1 Definition of Influence: Commutators as Causal Detectors

First, we need to physically define what "influence" or "signal" means. In quantum mechanics, if two observable operators $\hat{A}$ and $\hat{B}$ commute (i.e., $[\hat{A}, \hat{B}] = 0$), then measuring $\hat{A}$ does not interfere with the statistical distribution of $\hat{B}$, and vice versa. This means there is no causal connection between them.

Conversely, if $[\hat{A}, \hat{B}] \neq 0$, it indicates that one operation interferes with another's result; information has been transmitted between them.

Therefore, we can define:

> **Definition (Causal Connection)**:
>
> If for local operator $\hat{O}_x$ at location $x$ and local operator $\hat{O}_y$ at location $y$, in the Heisenberg picture they satisfy:
>
> $$[\hat{O}_x(t), \hat{O}_y(0)] \neq 0$$
>
> then event $(y, 0)$ causally influences event $(x, t)$.

## 3.1.2 Strict Light Cone Theorem

Now, we prove that in QCA defined by Axiom $\Omega$, the propagation range of such influence is strictly limited.

> **Theorem 3.1 (Strict Causal Bound of QCA)**
>
> Let the graph distance on QCA graph $\Lambda$ be $d(x,y)$, and the interaction radius of evolution operator $\hat{U}$ be $r$.
>
> For any two nodes $x, y$ and any time step $t \ge 0$, if:
>
> $$d(x, y) > r \cdot t$$
>
> then necessarily:
>
> $$[\hat{O}_x(t), \hat{O}_y(0)] = 0$$

**Proof (Mathematical Induction)**:

1. **Heisenberg Evolution**: $\hat{O}_x(t) = (\hat{U}^\dagger)^t \hat{O}_x(0) \hat{U}^t$. This is equivalent to operator evolution backward in time.

2. **Base Case ($t=0$)**: If $x \neq y$, according to the tensor product structure of local Hilbert spaces, operators on different lattice points naturally commute. $[\hat{O}_x(0), \hat{O}_y(0)] = 0$. Theorem holds.

3. **Single-Step Diffusion ($t=1$)**:

   Consider $\hat{O}_x(1) = \hat{U}^\dagger \hat{O}_x \hat{U}$.

   Since $\hat{U}$ is a product of local gates $\hat{u}_C$, only those local gates covering node $x$ will have non-trivial action on $\hat{O}_x$.

   The support set of these gates is at most the neighborhood $\mathcal{N}_r(x)$ centered at $x$ with radius $r$.

   Therefore, the evolved operator $\hat{O}_x(1)$, though formally more complex, still only contains algebraic combinations of operators defined within $\mathcal{N}_r(x)$.

   For any $y \notin \mathcal{N}_r(x)$ (i.e., $d(x,y) > r$), $\hat{O}_x(1)$ and $\hat{O}_y(0)$ act on disjoint Hilbert subspaces, hence they commute.

4. **Inductive Step**:

   Assume for $t=k$, the support set $\text{supp}(\hat{O}_x(k)) \subseteq \mathcal{N}_{k \cdot r}(x)$.

   At $t=k+1$, $\hat{O}_x(k+1) = \hat{U}^\dagger \hat{O}_x(k) \hat{U}$.

   Applying single-step evolution logic again, the support set expands outward by at most $r$.

   Therefore $\text{supp}(\hat{O}_x(k+1)) \subseteq \mathcal{N}_{(k+1)r}(x)$.

   Conclusion: For any point $y$ with distance $d(x, y) > r \cdot t$, its operator $\hat{O}_y$ lies outside the support set of $\hat{O}_x(t)$, so they necessarily commute.

**Q.E.D.**

## 3.1.3 Ontological Definition of Light Speed $c$

The above theorem gives a purely graph-theoretic and logical step number inequality:

$$\text{Causal Distance} \le r \times \text{Time Steps}$$

To connect with physical reality, we need to introduce **units**.

* Let the physical spacing between lattice points be Planck length $l_P$.

* Let the physical duration of one logical update be Planck time $t_P$.

Physical distance $D = d(x,y) \cdot l_P$, physical time $T = t \cdot t_P$.

The inequality becomes:

$$D/l_P \le r \cdot (T/t_P)$$

$$D/T \le r \cdot \frac{l_P}{t_P}$$

We define this insurmountable speed limit as $c$:

$$c \equiv r \frac{l_P}{t_P}$$

From this perspective, light speed $c$ is no longer a mysterious constant; it is the **aspect ratio of spacetime pixels**.

* **If $c$ were infinite**: It would mean $t_P \to 0$ or $l_P \to \infty$, corresponding to fully connected graphs or instantaneous computation, violating our discrete axiom.

* **If $c$ were variable**: It would mean the lattice structure is non-uniform (non-translationally invariant). Under our uniform QCA assumption, $c$ must be a universal constant.

## 3.1.4 Geometrization of Causality

Theorem 3.1 not only defines speed, but also defines **geometric structure**.

On graph $\Lambda \times \mathbb{Z}$ (spacetime lattice), all point pairs $(y, 0)$ satisfying $d(x, y) \le r \cdot t$ constitute the **past light cone** of point $(x, t)$. Only events within this light cone can possibly be "causes" of $(x, t)$.

Conversely, all point pairs $(z, t)$ satisfying $d(x, z) \le r \cdot t$ constitute the **future light cone** of $(x, 0)$. Only events within this light cone can be "influenced" by $(x, 0)$.

In regions outside the light cone (spacelike separation), $[\hat{O}_x, \hat{O}_z] \equiv 0$. This means:

**For spacelike separated events, there is no objective temporal order.** Because there is no causal connection between them, which comes first or second will not lead to logical contradictions. This is precisely the microscopic origin of "relativity of simultaneity" in special relativity.

## 3.1.5 Summary

Starting from Axiom $\Omega$, without invoking any relativistic assumptions, we "derived" the light cone structure and limiting velocity merely by analyzing operator diffusion on discrete lattices.

In this universe, **photons** are not special particles; they are **information wave packets that exactly reach the lattice propagation bandwidth limit**. They are the exposed skeleton of causal chains.

After establishing the existence of $c$, the next question naturally is: What happens if objects try to move as fast as light? This leads to the most core theorem of this book—Light Path Conservation.

# 3.2 Light Path Conservation Theorem: Proving $v_{ext}^2 + v_{int}^2 = c^2$ from Unitarity

In the previous section, we established light speed $c$ as the maximum bandwidth (causal boundary) for information propagation in the universe. However, special relativity is not just about light speed limits, but about **what happens when objects approach light speed**. Why does time dilate? Why does mass increase?

Traditional physics textbooks attribute these effects to the geometric properties of Lorentz transformations, i.e., spacetime metrics must maintain $ds^2 = -c^2 dt^2 + dx^2$ invariant. But this is only a description of phenomena, not an explanation. We must ask: **Why** must the metric be this way?

In this section, we will prove a more fundamental theorem—**Light Path Conservation Theorem (Theorem of Information Celerity Conservation)**. We will show that all strange effects of special relativity are merely inevitable mathematical consequences of **unitarity (information conservation)** allocating resources between space and internal states.

## 3.2.1 Geometric Definition of Information Rate

In QCA's Hilbert space $\mathcal{H}$, physical states $|\Psi(t)\rangle$ evolve with discrete time $t$. How do we define the "speed" of this state evolution?

In quantum mechanics, the natural distance measuring the difference between two states is the **Fubini-Study metric**. For unitary evolution driven by Hamiltonian $\hat{H}$, the evolution rate of state vectors in Hilbert space (i.e., the rate of orthogonalization with other states) is proportional to energy uncertainty or average energy.

For basic excitations (single particles) of QCA, the energy scale is set by Planck frequency $\omega_P$. Since evolution operator $\hat{U}$ is strictly unitary ($\hat{U}^\dagger \hat{U} = \mathbb{I}$), state vectors maintain constant modulus during evolution. This means **state vectors always rotate at constant "angular velocity" in Hilbert space.**

We define this constant total information update rate as $c$.

> **Definition (Total Information Rate)**:
>
> For any basic excitation in the universe, the modulus of its Fubini-Study evolution rate in the full Hilbert space (including position and internal degrees of freedom) is constant $c$.
>
> $$\| \mathbf{v}_{total} \| \equiv c$$

## 3.2.2 Orthogonal Decomposition and Pythagorean Theorem

Now, we project this total rate $\mathbf{v}_{total}$ onto two observable dimensions in the physical world.

According to Axiom $\Omega$, Hilbert space decomposes as $\mathcal{H}_{total} = \mathcal{H}_{position} \otimes \mathcal{H}_{internal}$. Correspondingly, evolution generators (effective Hamiltonian $\hat{H}_{eff}$) also consist of two parts:

1. **Translation Generator ($\hat{P}$)**: Responsible for changing particle position index $|x\rangle \to |x+1\rangle$ on lattice graph $\Lambda$. This corresponds to macroscopic **momentum**.

2. **Internal Rotation Generator ($\hat{M}$)**: Responsible for changing particle internal states (such as spin flips, phase rotations). This corresponds to macroscopic **rest mass**.

In a one-dimensional Dirac-QCA model (simplest fermion model), the effective Hamiltonian is written as:

$$\hat{H}_{eff} = c \hat{P} \otimes \hat{\sigma}_z + m_0 c^2 \hat{I} \otimes \hat{\sigma}_x$$

where $\hat{\sigma}_z, \hat{\sigma}_x$ are Pauli matrices, acting on internal chirality space respectively.

Here appears a decisive algebraic property: **Anti-commutation**.

$$\{ \hat{\sigma}_z, \hat{\sigma}_x \} = \hat{\sigma}_z \hat{\sigma}_x + \hat{\sigma}_x \hat{\sigma}_z = 0$$

Geometrically, anti-commutation of operators means the evolution directions they generate are **strictly orthogonal**. Just as $x$-axis and $y$-axis are perpendicular in Euclidean space, in Hilbert space, "changing position" and "changing internal state" are two non-interfering evolution dimensions.

Therefore, the square of total evolution rate (corresponding to energy squared $E^2 \propto \langle \hat{H}^2 \rangle$) can be simply obtained by calculating the sum of squares of operators:

$$
\begin{aligned}
\hat{H}^2 &= (c \hat{P} \sigma_z + m_0 c^2 \sigma_x)^2 \\
&= c^2 \hat{P}^2 \sigma_z^2 + m_0^2 c^4 \sigma_x^2 + c m_0 c^2 \hat{P} (\sigma_z \sigma_x + \sigma_x \sigma_z) \\
&= (c^2 P^2 + m_0^2 c^4) \mathbb{I} \quad (\text{because } \sigma_i^2 = \mathbb{I}, \{\sigma_z, \sigma_x\} = 0)
\end{aligned}
$$

We divide both sides by total energy squared $E^2$ (normalization) and introduce velocity definitions:

* **External velocity** $v_{ext} \equiv c^2 P / E$ (group velocity $dE/dP$)

* **Internal velocity** $v_{int} \equiv m_0 c^3 / E$ (contribution rate of internal oscillation to total energy)

We obtain:

$$1 = \frac{v_{ext}^2}{c^2} + \frac{v_{int}^2}{c^2}$$

Rearranging, we get one of the most important theorems of this book:

> **Theorem 3.2 (Light Path Conservation Theorem)**:
>
> In a unitary QCA universe, for any particle, external displacement velocity $v_{ext}$ and internal evolution velocity $v_{int}$ satisfy:
>
> $$v_{ext}^2 + v_{int}^2 = c^2$$

## 3.2.3 Physical Interpretation: Demystifying Special Relativity

This simple Pythagorean theorem completely removes the mystery of special relativity. It tells us that physical entity evolution is a **zero-sum game**.

1. **Resource Mutual Exclusivity**: Your total "computational power" is finite ($c$). If you use computational power to change position ($v_{ext}$ increases), you must reduce computational power for internal state updates ($v_{int}$ decreases).

2. **Essence of Time Dilation**: So-called "time dilation" is essentially **reduction of internal computation rate**. When you move at near light speed ($v_{ext} \to c$), your $v_{int}$ is forced to approach 0. Your internal clock (metabolism, atomic vibrations, thought processes) slows down due to lack of "computational quota."

   * This is not because time itself slows down, but because **you're busy traveling, with no time to age**.

3. **Definition of Mass**: In this framework, **rest mass $m_0$** is given clear geometric meaning—it is the internal oscillation frequency of particles at rest ($v_{ext}=0$). It is the "inherent cost" of particle existence.

4. **Essence of Photons**: Photons have no mass because they are pure external displacement modes. They have no projection in internal space ($v_{int} \equiv 0$), so they must use all quota for displacement, resulting in $v_{ext} \equiv c$.

## 3.2.4 Conclusion

**Relativity is not about spacetime axioms, but about statistics of information processing.**

Einstein's Lorentz factor $\gamma = 1/\sqrt{1-v^2/c^2}$ is merely another way of writing the Pythagorean theorem $\sin \theta = \sqrt{1-\cos^2 \theta}$. In this discrete, unitary universe, every particle is a pointer dancing on the information rate circle.

In the next section, we will use this theorem to directly derive the specific form of Lorentz transformations and show how four-dimensional spacetime geometry emerges from this two-dimensional velocity constraint.

# 3.3 Derivation of Lorentz Transformation: No Geometry, Only Statistics of Resource Allocation

In the previous section, we established the **Light Path Conservation Theorem**: For any particle, its external displacement velocity $v_{ext}$ and internal evolution velocity $v_{int}$ satisfy $v_{ext}^2 + v_{int}^2 = c^2$. This is an equation about resource allocation in information processing.

Now, we face the greatest challenge: **How to derive the complete Lorentz transformation solely from this resource allocation equation?**

Usually, Lorentz transformations are viewed as results of spacetime geometric rotation ($SO(1,3)$ symmetry of Minkowski space). But in our discrete ontology, we do not presuppose continuous spacetime geometry. All we have are counters on lattices. We will prove that Lorentz transformations are essentially **statistical results of two observers in different reference frames decomposing the same conserved information flow differently**.

## 3.3.1 Definition of Reference Frames: Who Is Watching?

What is a "reference frame" in QCA?

* **Rest Frame (Laboratory Frame $S$)**: This is the perspective of the lattice background itself. We can imagine "daemon processes" distributed across lattice points, synchronized with each other (based on lattice connectivity), recording global update steps $t$ and lattice coordinates $x$. For frame $S$, the maximum signal speed is obviously $c$.

* **Moving Frame (Co-moving Frame $S'$)**: This is the perspective of a local observer attached to a particle (or spaceship) moving through the lattice at speed $v$. This observer carries their own internal clock (based on $v_{int}$) and measuring rod (based on signal round trips).

Our task is to establish the mapping relationship between $(t, x)$ and $(t', x')$.

## 3.3.2 Time Dilation: Relativity of Counting

Consider a particle $P$ moving at constant speed $v$ relative to the lattice (i.e., $v_{ext} = v$).

**In Co-moving Frame $S'$**:

The particle considers itself at rest ($v'_{ext} = 0$). According to the Light Path Conservation Axiom, it must use all light path quota for internal evolution.

$$v'_{int} = c$$

This means the particle's internal clock (proper time $\tau$) runs at maximum efficiency: for each physical moment, its state updates by one step.

$$d\tau \propto 1$$

**In Laboratory Frame $S$**:

We see the particle moving in space ($v_{ext} = v$). According to the Light Path Conservation Theorem, its internal evolution speed is forced to decrease:

$$v_{int} = \sqrt{c^2 - v^2}$$

This means that for each laboratory time step $dt$, the particle's internal state only updates by $\sqrt{c^2-v^2}/c$ steps.

**Mapping Relationship**:

Since "time" is physically defined as the cumulative number of internal state updates, the time $dt$ recorded in frame $S$ and time $dt'$ (i.e., $d\tau$) recorded in frame $S'$ satisfy the proportional relationship:

$$dt' = \frac{v_{int}}{c} dt = \frac{\sqrt{c^2-v^2}}{c} dt = \sqrt{1 - \frac{v^2}{c^2}} dt$$

This is the time dilation formula:

$$dt = \gamma dt', \quad \text{where } \gamma = \frac{1}{\sqrt{1-v^2/c^2}}$$

Note: No spacetime geometric assumptions are used here, only **conservation allocation of counting rate (Clock Rate)**.

## 3.3.3 Length Contraction: Counting Signal Round Trips

Next, we derive spatial transformation. We need to define how to measure length in a moving frame. The most operational definition is the **radar echo method**: send a light signal, reflect from one end to the other, measure round trip time.

Imagine a particle carrying a rod of length $L'$ (at rest in frame $S'$). The particle moves at speed $v$ along the $x$-axis relative to frame $S$.

In frame $S'$, the light signal travels from rod tail to head and back, taking time $\Delta t' = 2L'/c$.

In frame $S$, we see the rod length as $L$.

* **Outbound**: Light at speed $c$ chases the rod head escaping at speed $v$. Relative speed is $c-v$. Time taken $\Delta t_1 = L / (c-v)$.

* **Return**: Light at speed $c$ collides head-on with rod tail. Relative speed is $c+v$. Time taken $\Delta t_2 = L / (c+v)$.

Total round trip time (frame $S$):

$$\Delta t = \Delta t_1 + \Delta t_2 = \frac{L}{c-v} + \frac{L}{c+v} = \frac{2Lc}{c^2-v^2} = \frac{2L}{c(1-v^2/c^2)} = \frac{2L}{c} \gamma^2$$

Now, using the time dilation relationship derived earlier. The round trip time $\Delta t$ measured in frame $S$ should be $\gamma$ times the round trip time $\Delta t'$ measured in frame $S'$ (because the clock on $S'$ runs slow, it reads fewer numbers, corresponding to longer physical duration as seen by $S$... wait, need to be careful here).

**Strict Logic**:

"Time dilation" means: $S'$'s clock runs slow. If $S'$ reads $\Delta t'$ seconds, then $S$ will think this actually took $\gamma \Delta t'$ seconds.

That is: $\Delta t = \gamma \Delta t'$.

Substituting into the above equation:

$$\frac{2L}{c} \gamma^2 = \gamma \left( \frac{2L'}{c} \right)$$

$$\frac{2L}{c} \gamma = \frac{2L'}{c}$$

$$L = \frac{L'}{\gamma} = L' \sqrt{1 - v^2/c^2}$$

This is **Lorentz Length Contraction**. Again, we did not assume space curves; we merely derived a **pursuit problem under a limiting speed $c$**.

## 3.3.4 Algebraic Reconstruction of Lorentz Transformation Matrix

With $\Delta t = \gamma \Delta t'$ and $L = L'/\gamma$, can we write the coordinate transformation $(t, x) \to (t', x')$?

Based on QCA's **translation invariance** (clause 3 of Axiom $\Omega$), the transformation must be linear:

$$x' = A(x - vt)$$

$$t' = D(t - Bx)$$

(Form determined by "origin $x=vt$ is $x'=0$ in $S'$").

1. **Using Length Contraction**:

   Consider moment $t=0$, frame $S$ measures a rod at rest in frame $S'$ (endpoints $x=0$ and $x=L$).

   Corresponding $S'$ coordinates are $x'_1 = A(0) = 0$ and $x'_2 = A(L)$.

   Length $L' = x'_2 - x'_1 = AL$.

   Known $L = L'/\gamma \implies L' = \gamma L$.

   Therefore, coefficient **$A = \gamma$**.

   $$x' = \gamma (x - vt)$$

2. **Using Light Speed Invariance** (direct corollary of causality axiom):

   If a signal satisfies $x = ct$, then it must satisfy $x' = ct'$ in frame $S'$.

   Substituting expressions for $x'$ and $t'$:

   $$ct' = \gamma (ct - vt) = \gamma c t (1 - v/c)$$

   $$t' = \gamma t (1 - v/c)$$

   On the other hand, substituting linear form for $t'$:

   $$t' = D (t - B ct) = D t (1 - Bc)$$

   Equating the two:

   $$\gamma (1 - v/c) = D (1 - Bc)$$

   Since this holds for any light signal ($x = ct$ and $x = -ct$), we can solve:

   **$D = \gamma$**, **$B = v/c^2$**.

Thus, we have completely derived the Lorentz transformation:

$$
\begin{cases}
x' = \gamma (x - vt) \\
t' = \gamma (t - \frac{v}{c^2}x)
\end{cases}
$$

## 3.3.5 Conclusion: Geometry is a Statistical Illusion

This is not just a reproduction of mathematical derivation, but an ontological declaration.

In standard textbooks, Lorentz transformations are considered spacetime geometric rotations (hyperbolic rotation):

$$\begin{pmatrix} ct' \\ x' \end{pmatrix} = \begin{pmatrix} \cosh \eta & -\sinh \eta \\ -\sinh \eta & \cosh \eta \end{pmatrix} \begin{pmatrix} ct \\ x \end{pmatrix}$$

where $\tanh \eta = v/c$.

In our QCA theory, this "hyperbolic rotation" originates from Euclidean rotation on the **information rate circle**:

$$v_{ext} = c \sin \theta, \quad v_{int} = c \cos \theta$$

Lorentz factor $\gamma = 1/\cos \theta = \sec \theta$.

**Profound Insight**:

* The **hyperbolic angle (rapidity $\eta$)** in Minkowski space is mathematically equivalent to the **allocation angle $\theta$** in QCA Hilbert space.

* We think we live in a $3+1$ dimensional pseudo-Euclidean spacetime, but actually, we live in a Hilbert space projection governed by **information rate conservation ($L_2$ norm)**.

Spacetime geometry is not a stage; it is **statistical behavior collectively exhibited by vast numbers of qubits to satisfy conservation laws**. Special relativity is not wrong, but it is not ultimate truth; it is macroscopic emergence of underlying unitary computation.

# 4.1 Entanglement and Geometry: Spacetime Distance as Quantum Mutual Information

In continuous spacetime, "distance" $d(x, y)$ is a fundamental concept given by metric tensor $g_{\mu\nu}$. But in a discrete quantum network, what does "physical distance" between two nodes $x$ and $y$ mean?

If two nodes are far apart on the lattice graph (large graph distance), but have extremely strong quantum entanglement (Bell states) between them, then in the sense of quantum information, they are actually "together."

## 4.1.1 Microscopic Revelation of Ryu-Takayanagi Formula

One of the most profound discoveries in the holographic principle and AdS/CFT correspondence is the Ryu-Takayanagi (RT) formula:

$$S_A = \frac{\text{Area}(\gamma_A)}{4 G}$$

It states that entanglement entropy $S_A$ on the boundary is equivalent to minimal surface area $\text{Area}(\gamma_A)$ in the bulk. This suggests: **Entanglement is the glue connecting spacetime.**

If you cut entanglement between two regions, spacetime breaks. Conversely, if you increase entanglement between two regions, they are pulled closer geometrically.

## 4.1.2 Definition: Information Distance

In the QCA framework, we abandon the traditional "meter stick" definition and instead use **quantum mutual information** as a measure of distance.

Let $\rho_x$ and $\rho_y$ be the reduced density matrices of nodes $x$ and $y$ respectively, and $\rho_{xy}$ be their joint density matrix. Mutual information is defined as:

$$I(x : y) = S(\rho_x) + S(\rho_y) - S(\rho_{xy})$$

where $S(\rho) = -\text{Tr}(\rho \ln \rho)$ is von Neumann entropy.

Mutual information measures the strength of total correlation (classical + quantum) between two systems. For QCA networks in ground state or vacuum state, mutual information decays exponentially with graph distance $d$ (due to locality axiom):

$$I(x : y) \sim e^{-d(x,y)/\xi}$$

where $\xi$ is the correlation length.

We now reverse this and use mutual information to define **emergent geometric distance** $D(x, y)$:

$$D(x, y) \equiv -\xi \ln \left( \frac{I(x : y)}{I_{max}} \right)$$

The physical meaning of this definition is extremely profound:

1. **Stronger Entanglement, Closer Distance**: When $I(x:y) \to I_{max}$ (maximum entanglement), $D \to 0$. These two points are geometrically coincident (or connected via wormhole).

2. **Entanglement Vanishes, Distance Infinite**: When $I(x:y) \to 0$, $D \to \infty$. These two points belong to disconnected universes.

## 4.1.3 Geometric Reconstruction: From Network to Manifold

With distance $D(x, y)$, we can use **multidimensional scaling (MDS)** to embed the discrete QCA network into a smooth Riemannian manifold $\mathcal{M}$.

If QCA is in ground state (Vacuum State), entanglement distribution across the entire network is uniform. The manifold reconstructed from this is **flat Minkowski space**. This explains why we see flat spacetime when there is no matter.

However, when matter excitations exist in the network, matter carries additional information and interferes with surrounding entanglement structure.

* **Matter = Local Entanglement Destruction/Reorganization**. A particle may be a highly entangled "knot" that consumes surrounding entanglement resources.

* If mutual information $I(A:B)$ between regions $A$ and $B$ decreases due to matter inserted in between (entanglement is "screened" or "crowded out" by matter), according to formula $D \sim -\ln I$, their **physical distance $D$ increases**.

**This is the origin of gravity:**

**Matter reduces vacuum entanglement, causing space to be "stretched" or "expanded."**

This non-uniform stretching of space manifests macroscopically as **curvature**. Light rays bend when passing massive objects, not because light is attracted, but because light must pass through a spatial region with "lower entanglement, causing longer effective path."

## 4.1.4 Summary

This section establishes a revolutionary concept:

* **Geometry is Entanglement.**

* Spacetime is no longer a stage, but a holographic projection of qubit correlations.

* Gravitational potential $\Phi$ is essentially a logarithmic function of mutual information $I$.

In the next section, we will quantify this qualitative picture by introducing **"local information volume conservation"** to derive that mysterious metric deformation formula.

# 4.2 Local Information Volume Conservation: Why Must Space Expand When Information Compresses Time?

In Section 4.1, we qualitatively established the concept that "geometry is entanglement." Spacetime curvature is no longer matter distorting a background stage, but matter interfering with entanglement network connectivity.

However, to move from qualitative picture to quantitative Einstein field equations, we need a stronger mathematical constraint. If it were merely "entanglement reduction causes distance increase," we would only get some scalar gravity theory (similar to relativistic generalization of Newtonian potential). Early attempts (such as Einstein's variable speed of light theory in 1911) predicted light deflection angles only half of general relativity's prediction.

This section will solve the "half-angle problem" and derive the correct optical metric form by introducing the key principle of **"local information volume conservation."** This principle is not just a mathematical patch, but an inevitable corollary of QCA unitarity at the statistical mechanics level.

## 4.2.1 Discrete Correspondence of Liouville Theorem

In classical statistical mechanics, Liouville's theorem states: Phase space volume of Hamiltonian systems is conserved during evolution (incompressible flow). This means if we compress momentum space, we must stretch coordinate space to keep $dp \cdot dq$ constant.

In QCA, the counterpart is **unitary evolution preserves Hilbert space dimension**.

Let a local volume element $V$ contain $N$ qubits. Its Hilbert space dimension is $D = 2^N$.

Unitary evolution operator $\hat{U}$ is full rank, mapping $D$-dimensional space to $D$-dimensional space. This means information is neither compressed (lost) nor diluted (created from nothing).

When we define continuous "physical coordinates" $(t, x)$ at macroscopic scales, we are coarse-graining underlying discrete nodes.

Let physical metric be $g_{\mu\nu}$. Local physical volume element (including time) is:

$$dV_{phys} = \sqrt{-g} \, d^4x$$

This physical volume element directly corresponds to "number of operations" or "degrees of freedom" of the underlying QCA.

**Axiom (Local Information Volume Conservation)**:

In any coordinate transformation or geometric deformation, the effective quantum degree of freedom density per unit coordinate volume must remain constant.

## 4.2.2 Antagonism Between Time Dilation and Space Expansion

Consider a region with high local information processing density $\rho_{\text{info}}$ (e.g., containing massive objects).

According to Light Path Conservation, accelerated internal computation means external clock slows. We describe this time dilation using **refractive index $n(x) > 1$**:

$$d\tau = \frac{1}{n(x)} dt$$

This means physical time interval $d\tau$ is compressed (relative to coordinate time $dt$). Or, the number of physical "ticks" contained per unit coordinate time $dt$ decreases.

If only time dilation occurred, total degrees of freedom in this spacetime region would decrease (because time dimension capacity shrinks). To satisfy volume conservation (Liouville constraint), spatial dimensions must undergo compensatory **expansion**.

Let spatial scaling factor be $a(x)$, i.e., $dl = a(x) dx$.

Four-dimensional volume element change factor is:

$$\sqrt{-g} \propto \frac{1}{n} \cdot a^3$$

(for $3+1$ dimensional spacetime).

To maintain phase space volume (or total information capacity) constant, do we need $\sqrt{-g} = 1$?

More precisely, we need to examine phase space $(x, k)$ volume. Wave vector $k$ is inversely proportional to wavelength $\lambda$.

If physical length is stretched by $a$, physical momentum cutoff (Brillouin zone boundary) shrinks by $1/a$.

For photons (or massless fields), density of states $\rho(\omega) \sim \omega^2$. Frequency redshifts $\omega \to \omega/n$.

To maintain photon number conservation (or information channel number conservation), spatial volume expansion must precisely compensate frequency cutoff contraction.

After rigorous statistical counting (see appendix for details), in isotropic media, conservation condition yields the following constraint:

$$\eta_t \cdot \eta_x = 1$$

where $\eta_t = 1/n$ is time flow rate factor, $\eta_x = a$ is spatial expansion factor.

Therefore, necessarily:

$$\eta_x = \frac{1}{\eta_t} = n$$

**Conclusion**: If gravity causes time to slow by factor $n$, it must simultaneously cause space to expand by factor $n$.

$$d\tau = \frac{1}{n} dt, \quad dl = n dx$$

## 4.2.3 Derivation of Optical Metric

Based on the above dual scaling, we can write the macroscopic effective metric.

On flat background $ds^2 = -c^2 dt^2 + dx^2$, introducing scaling:

$$ds^2 = -c^2 d\tau^2 + dl^2 = -c^2 (\frac{1}{n} dt)^2 + (n dx)^2$$

Rearranging:

$$ds^2 = -\frac{1}{n^2} c^2 dt^2 + n^2 (dx^2 + dy^2 + dz^2)$$

This is the famous **Optical Metric**, also called Gordon metric.

In weak field approximation, refractive index relates to Newtonian gravitational potential $\Phi$ ($n \approx 1 - 2\Phi/c^2$, note $\Phi < 0$):

$$g_{00} \approx -(1 + 2\Phi), \quad g_{ij} \approx (1 - 2\Phi)\delta_{ij}$$

This **completely matches** the weak field expansion of Schwarzschild metric in isotropic coordinates in general relativity.

## 4.2.4 Solving the Half-Angle Problem

Why is this correction so important?

If we only consider time dilation (scalar gravity), metric is $ds^2 = -(1+2\Phi)dt^2 + dx^2$.

When calculating light deflection, light travels straight in space, only affected by time potential. Deflection angle $\theta = \frac{2GM}{rc^2}$.

But in our optical metric, space is also "expanded" ($n^2 dx^2$). Light rays not only curve due to slow time, but also because space itself deforms like a lens.

According to Fermat's principle $\delta \int n dl = 0$, spatial refractive index contribution equals temporal contribution.

Total deflection angle $\theta = \theta_{time} + \theta_{space} = \frac{4GM}{rc^2}$.

This is exactly Einstein's final corrected result in 1915, also verified by Eddington's observation in 1919.

## 4.2.5 Summary

This section proves: **Spatial curvature of spacetime is the inevitable companion of time dilation.**

As long as we acknowledge:

1. Gravity originates from local differences in information processing rates (Light Path Conservation);

2. Underlying evolution follows unitarity (Information Volume Conservation);

Then, the metric structure of general relativity is the unique mathematical solution. Space must curve because if it doesn't, information squeezed by time would have nowhere to go, violating information conservation law.

**Gravity is the geometric projection of information conservation.**

# 4.3 Entropic Derivation of Einstein Field Equations: Proof of IGVP Principle

In the previous two sections, we established the concept that "geometry is entanglement" and derived the specific form of optical metric through "local information volume conservation." This solves the **kinematics** problem: if gravity exists, what should it look like (how does metric deform)?

Now, we must solve the **dynamics** problem: **Why** does matter distribution determine spacetime curvature? Or, why must the equation of state be Einstein's field equations $G_{\mu\nu} = 8\pi G T_{\mu\nu}$?

In standard general relativity, this equation is introduced as an axiomatic assumption (Einstein-Hilbert action). But in our discrete ontology, gravity is not a fundamental force, but an **entropic force**. It is similar to gas pressure or rubber elasticity—a macroscopic statistical tendency of systems to maximize microscopic state numbers.

This section will propose and prove the **Information-Gravity Variational Principle (IGVP)**, thereby deriving Einstein's field equations from first principles.

## 4.3.1 Microscopic Mechanism of Entropic Force

In thermodynamics, if a system's entropy $S$ depends on some macroscopic parameter $x$, the system experiences a statistical force $F = T \frac{\partial S}{\partial x}$, driving it toward entropy increase.

In QCA universe, the macroscopic parameter is **spacetime geometry (metric $g_{\mu\nu}$)**.

We need to consider two parts of entropy:

1. **Geometric Entropy $S_{geom}$**: This is the entropy of the holographic entanglement network itself, corresponding to complexity of spacetime connections. According to Ryu-Takayanagi formula and black hole entropy formula, it is proportional to area.

2. **Matter Entropy $S_{matter}$**: This is entanglement entropy or information content carried by matter excitations (Qubit states) distributed on the network.

**Core Assumption**: The universe is always in **maximum entanglement equilibrium state** within local causal diamonds. That is, for any given matter distribution, spacetime geometry automatically adjusts to maximize total entropy.

$$S_{tot} = S_{geom} + S_{matter} \to \text{Max}$$

## 4.3.2 Construction of IGVP Action

To formalize this idea, we construct total entropy functional (equivalent to action $I$).

**1. Geometric Entropy Term**

In continuous limit, area is proportional to integral of curvature scalar $R$ (this is generalization of Wald entropy to Einstein gravity, or derived from deficit angles in Regge calculus).

$$S_{geom} \propto \frac{1}{l_P^2} \int d^4x \sqrt{-g} R$$

Coefficient $1/l_P^2$ originates from Planck scale discreteness: curvature is actually macroscopic average of lattice defect density.

**2. Matter Entropy Term**

Matter's load on the network manifests as information processing density. Macroscopically, this is Lagrangian density $\mathcal{L}_{m}$ (or more accurately, it derives stress-energy tensor).

$$S_{matter} \propto \int d^4x \sqrt{-g} \mathcal{L}_{m}$$

**3. Variational Principle**

We define total action (as negative of entropy or its Legendre transform):

$$I_{IGVP}[g] = \frac{1}{16\pi G} \int d^4x \sqrt{-g} R + \int d^4x \sqrt{-g} \mathcal{L}_{m}$$

This looks like standard Einstein-Hilbert action. But in our theory, each term has clear information-theoretic origin:

* $\frac{1}{16\pi G}$ is not an arbitrary coupling constant; it directly relates to QCA's maximum information density (bits/Planck area).

* $R$ is a measure of network connection complexity.

* $\mathcal{L}_{m}$ is matter entanglement's occupation of network resources.

## 4.3.3 Derivation of Field Equations

To find equilibrium geometry, we vary metric $g^{\mu\nu}$, seeking stationary points $\delta I_{IGVP} = 0$.

1. **Geometric Term Variation**:

   Using Palatini identity:

   $$\delta (\sqrt{-g} R) = \sqrt{-g} (R_{\mu\nu} - \frac{1}{2} R g_{\mu\nu}) \delta g^{\mu\nu} + \sqrt{-g} \nabla_\sigma (\dots)$$

   (Boundary terms handled at holographic screen, ignored here).

   Result: $\frac{1}{16\pi G} (G_{\mu\nu})$.

2. **Matter Term Variation**:

   By definition, stress-energy tensor $T_{\mu\nu}$ is matter action's response to metric:

   $$T_{\mu\nu} \equiv -\frac{2}{\sqrt{-g}} \frac{\delta (\sqrt{-g} \mathcal{L}_{m})}{\delta g^{\mu\nu}}$$

   Or more intuitively, in information theory, $T_{\mu\nu}$ represents **information cost required to change geometry (stretch spacetime)**.

   Result: $-\frac{1}{2} T_{\mu\nu}$.

3. **Equilibrium Equation**:

   Setting total variation to zero:

   $$\frac{1}{16\pi G} (R_{\mu\nu} - \frac{1}{2} R g_{\mu\nu}) - \frac{1}{2} T_{\mu\nu} = 0$$

   Rearranging:

   $$R_{\mu\nu} - \frac{1}{2} R g_{\mu\nu} = 8\pi G T_{\mu\nu}$$

**Q.E.D.**

## 4.3.4 Reconstruction of Physical Meaning: Equation of State of Spacetime

Through IGVP, we not only derived Einstein's field equations, but more importantly changed our understanding of them.

* **Traditional View**: Mass tells spacetime how to curve. This is a dynamical causal relationship.

* **IGVP View**: Field equations are **equations of state**, like $PV = nRT$.

   * $G_{\mu\nu}$ (geometric tensor) corresponds to system's "elastic modulus" or "restoring force" for geometric deformation.

   * $T_{\mu\nu}$ (energy-momentum) corresponds to system's internal "thermal pressure" or "information flow."

   * $8\pi G$ corresponds to "Boltzmann constant," connecting microscopic bit numbers with macroscopic geometric quantities.

**Gravity exists because spacetime network tries to maintain maximum entropy distribution.** When matter aggregates, it reduces local entanglement degrees of freedom (occupies channels). To compensate this entropy decrease, spacetime geometry must curve (increase surface area/connection number), thus restoring thermodynamic equilibrium.

This is why gravity is always attractive (at least under positive energy conditions): because matter always tends to aggregate to minimize occupation of total network information capacity, or, the network tends to contract to maximize connection density, until pushed apart by matter's "exclusion principle."

At this point, we have completed the complete logical closed loop from microscopic QCA to macroscopic general relativity. Gravity is no longer mysterious; it is an inevitable product of quantum information statistical mechanics.

# 4.4 Black Holes: Entanglement Knots in QCA Networks and Holographic Screens

If Einstein's equations are the "equation of state" of spacetime, then black holes are "singular points" of this equation of state. In classical general relativity, black hole centers have density-infinite singularities, where physical laws break down. However, in QCA's discrete ontology, true "infinity" does not exist.

What exactly are black holes?

This section will provide a completely different microscopic picture of black holes from traditional geometric perspective, based on Axiom $\Omega$ and Light Path Conservation Theorem: **Black holes are not spacetime holes, but "congestion knots" in information processing networks.** Their horizons are holographic screens storing maximally densely packed entangled information.

## 4.4.1 Limit of Information Congestion: Horizon Formation

Recall the optical metric refractive index we derived in Section 4.2:

$$n(x) \approx 1 + \frac{G}{c^4} \rho_{\text{info}}(x)$$

And time flow rate caused by Light Path Conservation:

$$v_{int} = c/n(x)$$

As matter continuously aggregates, local information processing density $\rho_{\text{info}}$ keeps rising. According to holographic principle (Bekenstein Bound), any region has an information density upper limit $\rho_{max} \sim 1/l_P^2$ (Planck density).

When $\rho_{\text{info}} \to \rho_{max}$, refractive index $n \to \infty$.

What happens then?

1. **Time Freeze**: Internal evolution speed $v_{int} \to 0$ (relative to external observers). Qubits inside black holes are frantically computing (proper time extremely fast), but appear "frozen" to the outside.

2. **Space Stretch**: Spatial scaling factor $a = n \to \infty$. Physical distance to black hole center becomes infinite. This corresponds to "deep well" in classical geometry.

3. **Light Speed Zero**: External signal propagation speed $v_{ext} = c/n^2 \to 0$. Information cannot escape this region.

We define **horizon** as the critical surface satisfying $\rho_{\text{info}} = \rho_{max}$.

This explains why horizon is a one-way membrane: it is a **saturation layer** of information processing capacity. Any additional information trying to cross it gets "stuck" on the surface due to lack of extra bandwidth.

## 4.4.2 Microscopic Statistics of Entropy: Why Area?

Why is black hole entropy proportional to area $A$, not volume $V$? This is incomprehensible in continuous space, but obvious in QCA networks.

Consider horizon as a closed surface $\Sigma$. On QCA graph $\Lambda$, horizon cuts connection edges (links) between internal nodes $V_{in}$ and external nodes $V_{out}$.

Each cut connection edge represents a pair of entangled qubits (Bell pair).

For external observers, internal states are unknowable (shielded by horizon). Therefore, we perform partial trace over these unknowable degrees of freedom.

According to von Neumann entropy definition, entanglement entropy $S$ equals number of cut connections $N_{links}$ times entanglement per edge (maximum 1 bit).

$$S_{BH} \propto N_{links}$$

On regular lattices, number of connections crossing a surface is obviously proportional to discrete area (number of lattice points) of that surface:

$$N_{links} \sim \frac{\text{Area}}{a^2}$$

where $a$ is lattice spacing (Planck length $l_P$).

Therefore, we directly derive the form of Bekenstein-Hawking formula:

$$S_{BH} \propto \frac{\text{Area}}{l_P^2}$$

**Physical Interpretation**: **Black hole surface is the densest hard drive in the universe.** Every Planck area unit $l_P^2$ is a bit storage location. Black hole entropy is enormous because it "flattens" (projects) all information in three-dimensional volume onto two-dimensional surface. This is the microscopic mechanism of **holographic principle**.

## 4.4.3 Resolution of Singularities and Internal Structure

Classical theory predicts a volume-zero, density-infinite singularity inside horizon. But in QCA, this cannot happen.

Because $\Lambda$ is discrete, number of nodes is finite. When matter collapses, it cannot collapse into a point (due to Pauli exclusion principle and minimum lattice spacing constraints).

Instead, black hole interior forms a **high-density entanglement core**.

* **Spatial Topology**: Interior is not a point, but a highly interconnected complex network, possibly similar to **small-world network** or **fast scrambler**.

* **Information State**: Information falling into black holes does not disappear, but is rapidly scrambled, uniformly distributed throughout horizon/internal network. This is like dropping ink into the sea—ink molecules remain, but information is completely delocalized.

Therefore, **singularities are only breakdown points of continuum mathematical models, not termination of physical reality.** In QCA, singularities are replaced by Planck-scale "fuzzballs" or high-entanglement states.

## 4.4.4 Hawking Radiation and Unitarity Restoration

Finally, how to resolve information paradox?

According to unitarity of Axiom $\Omega$, black hole evolution operator $\hat{U}_{BH}$ must be unitary. This means information never disappears.

Hawking radiation is not random product of vacuum fluctuations, but **stimulated radiation of highly entangled states** on horizon surface.

1. **Horizon is Not Vacuum**: It is an active qubit layer.

2. **Information Leakage**: Due to QCA's local interactions, qubits on horizon exchange information with external vacuum qubits (through tiny non-zero $v_{ext}$, i.e., quantum tunneling).

3. **Encrypted Transmission**: Radiated photons carry horizon interior information, but this information is highly encrypted (scrambled). This is like burning an encyclopedia—smoke and ash contain all atomic information of the book, but appear as thermal radiation (random).

If we could collect all Hawking radiation and have a super quantum computer, we could in principle reconstruct the initial matter state that fell into the black hole. This completely resolves black hole information paradox—**black holes not only don't swallow information, they are actually the most perfect information mixers and emitters.**

## 4.4.5 Summary

Black holes are ultimate entities in QCA universe. They are limits of spacetime, warehouses of information, and magnifying glasses of quantum gravitational effects. Through black holes, we glimpse the deepest secrets of Axiom $\Omega$: **Matter, gravity, and information are trinity on the horizon.**

At this point, we have completed Part II "The Emergence of Spacetime." Starting from a simple lattice axiom, we reconstructed light speed, relativity, curved spacetime, and even black holes. In the next Part III, we turn our gaze to actors on the stage, exploring that more colorful world—**The Emergence of Matter**.

# 5.1 Why Do Photons Have No Mass? — Pure Translation Modes

In the Light Path Conservation Axiom $v_{ext}^2 + v_{int}^2 = c^2$, we have already seen the shadow of mass. Rest mass $m_0$ corresponds to the maximum internal oscillation frequency when $v_{ext} = 0$.

For photons, experiments tell us their rest mass is zero ($m_\gamma < 10^{-18}$ eV). This means in our theory, photons must satisfy:

$$v_{int} \equiv 0$$

which leads to:

$$v_{ext} \equiv c$$

Why must photons be this way? This stems from their evolution mode in QCA networks.

## 5.1.1 Definition 5.1 (Translation-Invariant Mode)

Consider a one-dimensional simplified model of QCA. Let quantum state be $|\psi(x, t)\rangle$.

If evolution operator $\hat{U}$ merely maps state from $x$ to $x+a$ (or $x-a$) without changing its internal phase or spin structure, then this excitation is called a **Pure Translation Mode**.

$$\hat{U} |\psi(x)\rangle = |\psi(x \pm a)\rangle$$

## 5.1.2 Theorem 5.1 (Massless Theorem)

If an excitation's evolution operator $\hat{U}$ can be globally diagonalized as pure translation generator $e^{i\hat{P}a}$ without any local mixing terms, then this excitation's internal evolution speed $v_{int} = 0$, i.e., this particle is massless.

**Proof**:

Pure translation means Hamiltonian $\hat{H} \sim \hat{P}$.

In Hilbert space, state vector $|\psi(t)\rangle$ merely moves between position bases $|x\rangle$ without evolving on internal degrees of freedom (such as spin flips).

According to microscopic derivation of Light Path Conservation (see Chapter 3, Section 3.2), $v_{int}$ corresponds to parts of Hamiltonian that **anti-commute** with momentum $\hat{P}$ (i.e., mass term $\hat{M}$).

If $\hat{H}$ only has $\hat{P}$ term, then $\hat{M} \equiv 0 \implies v_{int} \equiv 0$.

Q.E.D.

## 5.1.3 Physical Picture

Photons are "heartless" information packets. When transmitting on lattices, they need no internal computation. They simply transport bits from A to B.

Because they have no "internal life," they have no proper time ($\tau = 0$) and no rest mass. They are destined to eternally wander at the universe's maximum allowed speed.

This also explains why gauge bosons (under unbroken symmetry) are usually massless: they are **messengers** for transmitting information in networks, not **nodes** for processing information.

So, what has mass? It must be things that **"stop to think."** In the next section, we will see that matter particles are precisely products of this "thinking."

# 5.2 Matter as Topological Knots: Self-Referential Loops and Winding Numbers

In the previous section, we established that photons are pure **translation modes** in QCA networks—they are unidirectional information flows with no internal echo, hence no mass. Now, we face a more challenging question: **How to construct a particle that can "stop"?**

In a medium with constant fundamental propagation speed $c$, rest seems impossible. The only solution is **looping**. If a wave packet no longer propagates in a straight line but is trapped by some mechanism rotating in a closed path, then its **average position** can remain stationary or move at speeds below $c$.

This section will prove: **Matter particles are essentially topological knots in spacetime networks.** This "knot" is not a physical rope knot, but **phase winding** of quantum states in parameter space (momentum space). It is precisely this topological non-triviality that prevents wave packet dispersion and endows particles with "solid" identity—namely **rest mass**.

## 5.2.1 Schrödinger's Trembling and "Photon in a Box" Model

To intuitively understand the origin of mass, let's revisit a classic physical picture: **photon in a box**.

Imagine a box made of perfect mirrors containing a photon. The photon bounces back and forth at speed $c$.

* For observers outside the box, the photon as a whole has average velocity 0.

* The photon's energy $E = h\nu$ is confined in the box. According to mass-energy equation, the box exhibits increased inertial mass $m = E/c^2$.

In the microscopic world of QCA, there are no "mirrors" as macroscopic objects. So what acts as mirrors, bouncing information flow back?

The answer is **spatial topological structure** or **chirality coupling**.

In a one-dimensional Dirac-QCA model (see Chapter 3, Section 3.2), evolution operator contains a mixing term $m c^2 \sigma_x$. This term flips particle chirality: turning "left-moving wave" into "right-moving wave" and vice versa.

$$L \xrightarrow{\text{mass term}} R \xrightarrow{\text{mass term}} L$$

This constant flipping causes particles to perform **Zitterbewegung (trembling)** at microscopic scales. Like a drunkard, though each step is taken at light speed, macroscopic displacement is very slow due to constant direction changes.

**Physical Picture 5.2**:

**Mass is not the quantity of some "matter"; it is the frequency at which information flow undergoes "self-referential scattering."** A particle is a photon that traps itself.

## 5.2.2 Topology of Momentum Space: Winding Number

Why is this "trapped" state stable? Why don't particles suddenly disintegrate into photons flying away? This requires introducing the concept of **topological protection**.

In QCA, due to translation invariance, we can diagonalize evolution operator $\hat{U}$ in momentum space (Brillouin zone, topologically homeomorphic to circle $S^1$).

For a two-component system (such as fermions), effective Hamiltonian $\hat{H}(k)$ can be written as linear combination of Pauli matrices:

$$\hat{H}(k) = \mathbf{d}(k) \cdot \boldsymbol{\sigma} = d_x(k) \sigma_x + d_y(k) \sigma_y + d_z(k) \sigma_z$$

where vector $\mathbf{d}(k)$ is a periodic function of momentum $k$.

As $k$ traverses the entire Brillouin zone (from $-\pi/a$ to $\pi/a$), vector $\mathbf{d}(k)$ traces a closed curve in three-dimensional space.

More importantly, if we focus on normalized vector $\hat{\mathbf{n}}(k) = \mathbf{d}(k) / |\mathbf{d}(k)|$, it defines a mapping from circle $S^1$ to unit sphere $S^2$ (or its equator circle $S^1$).

**Definition 5.2 (Winding Number)**:

For one-dimensional systems, if Hamiltonian is constrained by chiral symmetry (i.e., $\mathbf{d}$ restricted to $xz$ plane), mapping degenerates to $S^1 \to S^1$. We can define integer topological invariant $\mathcal{W}$:

$$\mathcal{W} = \frac{1}{2\pi} \oint_{BZ} \left( \hat{n}_z \frac{d\hat{n}_x}{dk} - \hat{n}_x \frac{d\hat{n}_z}{dk} \right) dk$$

Intuitively, this is the number of times vector $\mathbf{d}(k)$ rotates around the origin.

## 5.2.3 Mass Generation Theorem

Now we state the core theorem of this chapter, establishing the necessary connection between topology and mass.

> **Theorem 5.2 (Topological Mass Generation Theorem)**:
>
> In a local unitary QCA system, if an excited state has winding number $\mathcal{W} \neq 0$, then this excited state necessarily has non-zero rest mass (energy gap).
>
> That is: **Non-trivial topology $\implies$ massive particle.**

**Proof**:

1. **Proof by Contradiction**: Assume particle is massless.

2. According to Definition 5.1, massless means evolution is pure translation, or can be continuously deformed to pure translation.

3. For pure translation operator $\hat{U}_{trans} = e^{ik\hat{\sigma}_z}$, corresponding $\mathbf{d}(k)$ vector always points in $z$-axis direction (north pole), or monotonically changes along $z$-axis with $k$ without rotating around origin.

4. In this case, winding number $\mathcal{W} = 0$.

5. Since winding number is a discrete integer, it cannot change continuously with parameters. As long as Hamiltonian's energy gap doesn't close (i.e., $|\mathbf{d}(k)| \neq 0$), $\mathcal{W}$ is a **topological invariant**.

6. Therefore, if we want to construct a system with $\mathcal{W} \neq 0$ (e.g., $\mathbf{d}(k)$ winds once in $xz$ plane), it's impossible to deform it into massless photons without going through phase transition (gap closing).

7. Non-zero energy gap $\min |\mathbf{d}(k)| > 0$ is precisely rest mass $m_0$.

**Physical Interpretation**:

Winding number $\mathcal{W}$ is like a "dead knot" tied on the wave function.

* **Photon ($\mathcal{W}=0$)**: A straight rope. It can smoothly flow through space.

* **Electron ($\mathcal{W}=1$)**: A knotted rope. When you try to pull it, the knot must move as a whole. To untie this knot microscopically, you need extremely high energy (reaching Planck energy scale, collapsing lattice structure). Therefore, electrons are stable.

## 5.2.4 Geometric Meaning of Self-Reference

We call this structure "self-referential loop" because mathematically, it requires different components of wave function to mutually "see" each other.

In feedback loops described by Riccati equations (see Section 5.4), system output is fed back to input.

$$x_{next} = f(x_{now}, x_{neighbor})$$

For massless particles, $x_{next}$ only depends on $x_{neighbor}$ (only neighbors tell me what happened).

For massive particles, $x_{next}$ strongly depends on $x_{now}$ (I must remember my state from previous moment).

This **memory** or **consistency** geometrically manifests as non-trivial covering of Brillouin zone. Particle "knows" it's a whole because it "touches" its boundary in momentum space and finds it has wound around once.

## 5.2.5 Summary

We arrive at a startling conclusion: **Matter is dead loops of information.**

* The universe's background is massless information flow (light-speed propagation).

* When local information flow undergoes topological entanglement, forming non-trivial winding numbers, information is forced to rotate in place.

* The frequency of this in-place rotation macroscopically manifests as **mass**.

* The topological robustness of this rotation macroscopically manifests as **particle stability** (charge conservation, baryon number conservation are essentially topological charge conservation).

We are not made of "atoms"; we are made of "knots tied by light rays." In the next section, we will further explore how this "knot" responds to external forces, thereby explaining the essence of **inertia**.

# 5.3 Mass as Impedance: Information Refresh Rate Required to Maintain Internal Oscillation ($v_{int}$)

After establishing the topological picture that "matter is self-referential loops of information," we must answer a quantitative dynamics question: **How does this loop resist external forces, manifesting as macroscopic inertia?**

This section will derive Newton's second law $F=ma$ in relativistic form from first principles by introducing the concept of **"Topological Impedance"**, revealing the microscopic mechanism of inertial mass: mass is not the "weight" of matter, but **response latency** of information flow to external perturbations when maintaining its internal topological structure.

## 5.3.1 Internal Refresh Rate and Cost of Existence

According to Light Path Conservation Theorem (Section 3.2), any particle's total information update amount (light path) within Planck time $t_P$ is constant $c t_P$. For massive particles, this resource is allocated as:

1. **Displacement Update**: Changing lattice position $|x\rangle \to |x+\Delta x\rangle$.

2. **State Update**: Changing internal phase $| \psi_{int} \rangle \to e^{-i\omega_{int} t} | \psi_{int} \rangle$.

Rest mass $m_0$ is defined as maximum internal frequency at rest $\omega_0 = m_0 c^2 / \hbar$. This means, to maintain particle's "existence" (i.e., keep its topological knot from disintegrating), QCA network must provide $\omega_0$ internal refresh operations per second for this particle.

**Definition 5.3 (Cost of Existence)**:

A particle's **Cost of Existence** $C_{exist}$ equals the logic gate operation rate required for its internal state updates.

$$C_{exist} \equiv \omega_{int} = \frac{m_0 c^2}{\hbar} \sqrt{1 - v^2/c^2}$$

Note that as particle speed $v$ increases, due to time dilation, its **observed** internal refresh rate $\omega_{int}$ actually decreases. This seems to suggest high-speed particles are "cheaper"?

Quite the opposite. Precisely because internal refresh rate decreases, system's **response capability** worsens.

## 5.3.2 Impedance Model: Microscopic Derivation of Inertia

Imagine a running computer program (particle).

* **At Rest**: CPU resources are abundant, program refreshes 100 times per second. If you input an instruction (force $F$), program can quickly respond and change state (acceleration $a$).

* **At High Speed**: CPU resources are 99% occupied by background transport tasks (displacement). Program can only refresh once per second.

* **Consequence**: If you input the same instruction now, program needs extremely long time to process and respond. To outsiders, this manifests as "program becomes extremely sluggish" or "enormous inertia."

We define this sluggishness as **topological impedance**.

**Theorem 5.3 (Inertial Divergence Theorem)**:

Let particle's momentum $p$ change rate over time (force $F$) be inversely proportional to internal update rate $\omega_{int}$.

That is: System's response sensitivity (Susceptibility) to external stimuli $\chi \propto \omega_{int}$.

Then inertial mass $m_{inert} \equiv F/a$ diverges as speed $v$ increases.

**Proof**:

According to energy-frequency relationship derived from Light Path Conservation (Chapter 3.3 appendix), relationship between total energy $E$ and internal frequency $\omega_{int}$ is:

$$E = \frac{m_0 c^2}{\sqrt{1 - v^2/c^2}} = m_0 c^2 \cdot \frac{\omega_0}{\omega_{int}}$$

(Note: $\omega_{int} = \omega_0 / \gamma$).

Force $F$ is defined as energy gradient with distance, or momentum derivative with time:

$$F = \frac{dp}{dt} = \frac{d}{dt} (\gamma m_0 v)$$

Expanding derivative:

$$F = m_0 \gamma^3 a$$

(Here considering longitudinal acceleration).

Substituting $\gamma$ using $\gamma = \omega_0 / \omega_{int}$:

$$F = m_0 \left( \frac{\omega_0}{\omega_{int}} \right)^3 a$$

Defining effective inertial mass $m_{eff} = F/a$:

$$m_{eff} = m_0 \left( \frac{\omega_0}{\omega_{int}} \right)^3$$

**Conclusion**:

When $v \to c$, internal refresh rate $\omega_{int} \to 0$.

At this point, $m_{eff} \to \infty$.

**Physical Picture 5.3**:

Inertia is not an inherent property of matter, but a measure of **system's "crash degree."**

When a particle runs too fast, its internal clock almost stops. To change its motion state in the time it "blinks" (completes one internal update), the external world needs to apply infinite force integral.

This is why light speed cannot be exceeded—not because there's a wall ahead, but because **the faster your legs (displacement) run, the slower your brain (inertial processing) turns**, until you completely lose ability to change the status quo.

## 5.3.3 Mass as "Vorticity" of Information Flow

At this point, we have completely demystified mass:

1. **Rest Mass $m_0$**: Is **vorticity** of information flow in QCA networks. It is the frequency at which topological knot structure forces information to rotate in place.

2. **Inertial Mass $m_{inert}$**: Is **stiffness of vortex resisting deformation**. When vortex is stretched into a helix (high-speed motion), its pitch is elongated (frequency decreases), causing stiffness to rise sharply.

In this picture, the famous Higgs mechanism is merely **effective field theory description** of this topological process.

* Higgs field $\phi$ corresponds to QCA's vacuum entanglement background.

* Yukawa coupling $y_f$ corresponds to winding strength of topological knots.

* So-called "particles acquiring mass" is originally straight-propagating information flow being tripped by background entanglement, forming knots.

## 5.3.4 Summary

We no longer need to assume "matter" exists. We only need to assume "obstructed information flow."

* **Photons** are laminar flow.

* **Electrons** are vortices in turbulence.

* **Black holes** are blocked singularities.

In the next section, we will delve into the fine structure of this "vortex"—why does it not only have mass, but must also have spin? Why is it a fermion? This will lead to the most profound mathematical chapter of this book.

# 5.4 Origin of Fermion Statistics: Riccati Square Root and $\mathbb{Z}_2$ Phase

After establishing the picture that "mass is topological knots," we face the most profound question of this chapter: **Why must these massive "knots" be fermions?**

In standard physics, the connection between spin and statistics (spin-statistics theorem) is derived through axioms in relativistic quantum field theory. It shows that particles with half-integer spin must follow Fermi-Dirac statistics (wave function changes sign upon exchange), while integer spin particles follow Bose-Einstein statistics (unchanged upon exchange). But in our discrete ontology, we do not presuppose continuous spacetime symmetry groups, nor spinor fields.

This section will prove a startling conclusion: **Fermion statistics is not an arbitrary rule, but a topological fingerprint that any self-referential structure (massive particle) capable of maintaining its existence in discrete networks necessarily carries.** This fingerprint originates from the inherent **square root structure** of Riccati equations describing feedback loops.

## 5.4.1 Input-Output Relations of Self-Referential Systems

A massive particle is essentially a "self-maintaining" information loop. We can abstract it as a black box that receives input information $\psi_{in}$ and produces output information $\psi_{out}$, while part of the output is fed back to the input to maintain its own state.

In QCA's transmission line model, the physical quantity describing this input-output relationship is **impedance** $Z$ (or reflection coefficient $r$).

$$Z = \frac{V}{I} \quad (\text{voltage/current})$$

In quantum mechanical correspondence, $Z$ corresponds to logarithmic derivative of wave function at boundary $Z \sim \psi' / \psi$.

A stable particle means its internal impedance structure $Z(x)$ must satisfy some self-consistent equation. For discrete iterative systems, impedance evolution follows **Riccati equation** (discrete form is Möbius transformation):

$$Z_{n+1} = \frac{A Z_n + B}{C Z_n + D}$$

where $A, B, C, D$ are determined by QCA's local evolution operator $\hat{U}$.

## 5.4.2 Fixed Points and Square Root Branches

A particle as a stable topological soliton means it is not only localized in space, but also a **fixed point** in time evolution. That is, after one period of internal evolution, its impedance structure should restore:

$$Z^* = \frac{A Z^* + B}{C Z^* + D}$$

Solving this equation, we get a quadratic equation:

$$C (Z^*)^2 + (D-A)Z^* - B = 0$$

Its solution is:

$$Z^* = \frac{(A-D) \pm \sqrt{(A-D)^2 + 4BC}}{2C}$$

Here appears a decisive mathematical feature: **Square Root**.

This means, **any self-consistent, non-trivial stable particle state has its physical parameters (impedance $Z$) defined on a double-sheeted Riemann surface.**

## 5.4.3 Rotation, Exchange, and $\mathbb{Z}_2$ Phase

Now, we consider particle's **exchange statistics**.

In $3+1$ dimensional spacetime, exchanging two identical particles $1$ and $2$ is topologically equivalent to rotating one particle around the other by $360^\circ$ (or rotating the particle itself by $360^\circ$ in center-of-mass frame).

In parameter space, this rotation operation corresponds to evolving along a closed path on the Riemann surface for one cycle.

Since $Z^*$ contains square root $\sqrt{\Delta}$ (where $\Delta$ is a complex function of evolution parameters), when parameters rotate $2\pi$ around origin, square root function changes sign:

$$\sqrt{e^{i2\pi}} = e^{i\pi} = -1$$

This is the origin of **$\mathbb{Z}_2$ topological phase**.

* **For Bosons (photons)**: They have no internal self-referential feedback loops ($v_{int}=0$), so they don't need to solve Riccati fixed points. Their states are directly defined on single-valued plane. Rotating $360^\circ$ returns to origin, phase unchanged ($+1$).

* **For Fermions (massive particles)**: They must maintain a self-referential dead knot ($v_{int}>0$). Mathematical solution of this knot lies on branch cut of square root. Rotating $360^\circ$ causes system to slide to another sheet of Riemann surface, wave function acquires $-1$ phase.

**Theorem 5.4 (Mass-Statistics Theorem)**:

In local unitary QCA networks, any stable excitation maintained by nonlinear self-referential feedback (i.e., massive particles) necessarily has double-valued wave functions, thus exhibiting fermion statistics under exchange operations.

## 5.4.4 Ontological Status of Spinors

This discovery completely changes our understanding of **spinors**.

In traditional geometry, spinors are defined as "geometric objects that change sign upon $360^\circ$ rotation," usually viewed as abstract mathematical constructs.

But in our theory, **spinors are "square roots of scalars."**

* Physical observables (such as energy, charge, impedance) are single-valued (scalars or vectors).

* Underlying probability amplitudes (wave functions) must be square roots of these observables to satisfy self-consistent equations.

Therefore, fermions are not strange, special particles. **Fermions are the normal state of matter existence.** Any information structure attempting to "stop" and maintain its identity in spacetime networks must anchor itself on topological structure through "square root" operations, thus inevitably becoming fermions.

## 5.4.5 Summary: The Trio of Matter

At this point, we have completed Part III "The Emergence of Matter." Starting from Axiom $\Omega$, we constructed the complete microscopic picture of matter:

1. **Photons**: Massless, non-self-referential, single-valued phase **translation modes**.

2. **Mass**: **Topological dead knots** of information flow, resisting external forces (inertia) through internal oscillation ($v_{int}$).

3. **Fermions**: **Self-referential logic** required to maintain such dead knots necessarily introduces square root structure, leading to $-1$ exchange phase.

This picture not only explains "what," but also "why." It unifies mass, spin, and statistical properties under a simple geometric-logical framework.

In the next Part IV, we will explore the most mysterious component of this cosmic machine—**observers**. We will see how, when these fermion knots form sufficiently complex networks, they begin to "see" themselves.

# 6.1 Independence of Local Reference Frames: Why Does Each Cell Need "Translation"?

In the previous chapter, we solved the question "what is matter": matter is topological knots in QCA networks, and mass is the internal refresh rate required to maintain self-referential loops. Now, we must face a more complex question: **How do matter particles interact?**

In classical physics, interactions propagate through "fields" (such as electromagnetic fields, gravitational fields). In quantum field theory, interactions are realized through exchanging "gauge bosons." But in our discrete ontology, there are neither continuous fields nor presupposed bosons. All we have are independent cells running on lattices.

This chapter will reveal a startling fact: **Interactions are not special forces, but communication costs that different cells must pay to achieve "information consensus."**

Gauge fields are essentially **link variables** in discrete networks, geometric objects used to "translate" differences between local reference frames of different cells. Starting from the most fundamental "local independence" axiom, we will step by step derive Maxwell's equations and Yang-Mills theory.

## 6.1.1 Axiom 6.1 (Reference Frame Locality)

Imagine an internet composed of billions of independent computers. Each computer has its own clock (local time) and file system (local basis). If computer A wants to send a file to computer B, they must first establish a **communication protocol** to ensure that "0" and "1" sent by A are correctly interpreted at B.

In QCA universe, the situation is very similar.

**Axiom 6.1 (Reference Frame Locality)**:

Every cell $x$ in QCA network is an independent Hilbert space $\mathcal{H}_x$. Although all $\mathcal{H}_x$ are mathematically isomorphic (e.g., all $\mathbb{C}^2$), physically, **there is no God's-eye-view global basis**.

That is, each cell $x$ has the right to arbitrarily choose its own basis vectors $|0\rangle_x, |1\rangle_x$.

This freedom of choice constitutes **local gauge symmetry**.

* **$U(1)$ Symmetry**: Cell $x$ can arbitrarily change its wave function phase: $|\psi\rangle_x \to e^{i\alpha(x)} |\psi\rangle_x$.

* **$SU(N)$ Symmetry**: If cell has $N$ internal degrees of freedom (such as quark color), cell $x$ can arbitrarily rotate its internal coordinate axes: $|\psi\rangle_x \to \hat{U}(x) |\psi\rangle_x$.

## 6.1.2 Challenge of Communication

If cell $x$ and adjacent cell $y$ define different "phase zero" or "red direction," what happens to electron's wave function when it moves from $x$ to $y$?

If moved directly without correction, electron's phase information becomes chaotic (like recording time with two inaccurate clocks), leading to information loss, violating unitarity.

## 6.1.3 Solution: Connection Field

To achieve lossless transmission without global basis, network must maintain a **comparator** or **translator** between every two adjacent nodes $x, y$.

We define an operator $\hat{U}_{y \leftarrow x}$, called **link variable**. Its role is to "translate" $x$'s basis into $y$'s basis.

Information transmission equation:

$$|\psi\rangle_y^{\text{received}} = \hat{U}_{y \leftarrow x} |\psi\rangle_x^{\text{sent}}$$

If we perform local basis transformation $\hat{V}_x$ at $x$ and $\hat{V}_y$ at $y$, to ensure physical result (transmitted state) unchanged, link variable must transform accordingly:

$$\hat{U}'_{y \leftarrow x} = \hat{V}_y \hat{U}_{y \leftarrow x} \hat{V}_x^\dagger$$

This is the famous **gauge transformation law**. But in standard textbooks, this is a rule concocted to keep Lagrangian invariant; in QCA, this is **logical necessity to ensure different nodes can "understand" each other**.

## 6.1.4 Physical Picture

**Physical Picture 6.1**:

* **Gauge fields are not matter**; they are **metadata** carried by network connections themselves.

* **Electromagnetic potential $A_\mu$** records **clock synchronization error** (phase difference) between adjacent cells.

* **Strong interaction potential $G_\mu^a$** records **relative distortion** of internal coordinate systems of adjacent cells.

This view completely subverts our understanding of "force": **Forces are not to push objects, but to correct reference frame differences.** When reference frame differences vary with time and space (i.e., curvature exists), to maintain information consistency, objects must change motion state—this is the "force" we feel.

In the next section, we will see how this connection field, existing solely for "translation," acquires its own dynamical life and evolves into photons and gluons.

# 6.2 Necessity of Link Variables: Derivation of Maxwell and Yang-Mills Equations

In the previous section, we introduced **local gauge symmetry** as a fundamental property of QCA networks: each cell can freely choose its internal reference frame, and to transmit information between adjacent cells, network must maintain a set of **connection fields** $U_{yx}$ as "translators."

This section will show a more profound result: merely to maintain **consistency** and **minimal error** of this communication network, connection fields themselves must follow specific dynamical equations. Remarkably, these equations are precisely the familiar **Maxwell's equations** (for Abelian groups) and **Yang-Mills equations** (for non-Abelian groups).

In this picture, photons and gluons are not particles "added" to the universe, but **dynamic curvature** produced by spacetime networks to **correct information transmission errors**.

## 6.2.1 Closed Loops and Information Distortion: Geometric Definition of Field Strength

If connection field $U_{yx}$ were merely a static translation table, physics would be very boring. Interesting things happen when we examine closed paths (loops).

Consider a minimal closed loop (plaquette) on QCA lattice, denoted $\Box$. Suppose information starts from node 1, passes through 2, 3, 4 in sequence, finally returning to 1.

The total transformation operator of this process is called **holonomy** or **Wilson loop**:

$$U_{\Box} = U_{14} U_{43} U_{32} U_{21}$$

* **Flat Connection**: If $U_{\Box} = \mathbb{I}$ (identity operator), information returns unchanged after one round. This means network geometry is flat, parallel transport is path-independent.

* **Curvature**: If $U_{\Box} \neq \mathbb{I}$, information acquires extra phase or rotation during transmission. This "non-closure" degree is **field strength**.

In continuous limit, let lattice spacing be $a$, connection field $U_{x+\mu, x} = e^{-ig a A_\mu(x)}$.

Using Baker-Campbell-Hausdorff formula expansion (see appendix), we can derive:

$$U_{\Box} \approx e^{-ig a^2 F_{\mu\nu}}$$

where $F_{\mu\nu}$ is precisely the field strength tensor:

$$F_{\mu\nu} = \partial_\mu A_\nu - \partial_\nu A_\mu - ig [A_\mu, A_\nu]$$

**Physical Picture 6.2**:

* **Magnetic Field $\mathbf{B}$**: Not just force deflecting needles; it is **phase accumulation on spatial loops**. When you walk around a magnetic field once, your wave function phase cannot restore.

* **Electric Field $\mathbf{E}$**: Is **phase accumulation on spacetime loops**. It represents synchronization error in "time updates" between two adjacent nodes (non-commutativity of $U_{t, t+1}$ and $U_{x, x+1}$).

## 6.2.2 Minimal Distortion Principle: Origin of Action

Why must electromagnetic fields follow Maxwell's equations?

In classical mechanics, this is determined by least action principle $\delta S = 0$. But in QCA, we can give a more fundamental explanation.

QCA evolution must be unitary, meaning information must be as faithful as possible. If network is full of random curvature ($U_{\Box}$ jumping randomly), information will rapidly decohere during transmission, causing universe to "heat death" or "white noise."

Therefore, a universe capable of supporting complex structures must have connection fields in a **low curvature state**.

We define network's **channel cost function**, i.e., sum of "information distortion" on all loops:

$$S_{\text{noise}} = \sum_{\Box} \text{Re} \text{Tr} (\mathbb{I} - U_{\Box})$$

This quantity measures how much network deviates from "perfect flatness."

Meanwhile, changes in connection fields themselves require cost (because changing connections consumes computational resources). This corresponds to kinetic energy term.

In continuous limit, this cost function (action) converges to:

$$S \sim \int d^4x \, \text{Tr} (F_{\mu\nu} F^{\mu\nu})$$

This is precisely **Yang-Mills action**! For $U(1)$ group, it's Maxwell action $-\frac{1}{4} F^2$.

## 6.2.3 Derivation of Dynamical Equations

With action, dynamical equations are inevitable results of **minimizing channel noise**. Varying connection field $A_\mu$:

$$\frac{\delta S}{\delta A_\mu} = 0 \implies D_\mu F^{\mu\nu} = J^\nu$$

* **$D_\mu F^{\mu\nu}$**: This is diffusion term of curvature. It tells us that if there's strong curvature somewhere (like near charges), it won't exist abruptly, but will smoothly diffuse to surrounding space. This is the origin of **Coulomb's law** and **wave equations**.

* **$J^\nu$**: This is matter flow (source). When matter particles (topological knots carrying local basis rotations) move, they force surrounding connection fields to change accordingly, thus producing radiation.

**Conclusion**:

Photons (Maxwell waves) are not independent entities. They are **"elastic waves" produced by spacetime networks to smooth out local phase distortions caused by charges**.

Just like placing a ball on a rubber membrane causes deformation, placing a charge causes connection field distortion. When charge moves, propagation of distortion forms light.

## 6.2.4 Special Properties of Non-Abelian Fields: Why Do Gluons Self-Interact?

For $U(1)$ electromagnetic fields, photons themselves carry no charge, because phase rotations are commutative ($e^{i\alpha} e^{i\beta} = e^{i\beta} e^{i\alpha}$).

But for $SU(N)$ non-Abelian fields (such as color charge), internal rotations are non-commutative.

This means: **Changes in connection field $A_\mu$ itself also cause further distortion of connection fields.**

$$[A_\mu, A_\nu] \neq 0$$

This nonlinear term causes gluons to carry color charge and undergo self-interactions.

From QCA graph-theoretic perspective, this corresponds to **topological entanglement** of network connections. Information flows along different paths not only have different phases, but also different "directions" in internal space. To coordinate such complex distortions, connection fields must become extremely dense and strong. This is the geometric origin of **quark confinement**—if trying to separate two quarks, distortion energy on one-dimensional path (flux tube) connecting them grows linearly until breaking produces new quark pairs.

## 6.2.5 Summary

We have reconstructed gauge field theory from the perspective of "communication protocols."

* **Maxwell's equations** are **synchronization algorithms** networks run to maintain phase consistency.

* **Yang-Mills equations** are **error correction algorithms** networks run to maintain internal reference frame alignment.

* **Photons and gluons** are data packets (data packets) produced when these algorithms run.

Forces exist in nature because the universe is not just a computational system, but a **distributed** computational system. Without centralized clocks, every node must continuously "handshake" (exchanging bosons) to confirm each other's states. The macroscopic manifestation of this handshaking is interactions.

In the next section, we will explore parameters determining strengths of these interactions—**coupling constants**—and reveal their deep connection with network topological structure.

# 6.3 Geometric Meaning of Coupling Constants: Network Connectivity and Information Leakage Rate

In the standard model of quantum field theory, strengths of fundamental interactions are described by dimensionless **coupling constants** $g$. For example, fine structure constant of electromagnetic interaction $\alpha \approx 1/137$, while $\alpha_s$ of strong interaction is much larger. However, these constants are usually viewed as free parameters set by God, must be determined experimentally.

In QCA discrete ontology, since everything is information processing, "interaction strength" must correspond to some **ratio** or **probability** of information flow. Why is coupling strength between electrons and photons exactly that value?

This section will prove: **Coupling constant $g$ is essentially a geometric measure of QCA network's topological connectivity and information leakage rate.** It measures what proportion of phase information "leaks" into rotation of internal degrees of freedom ($v_{int}$) when information jumps between lattice points ($v_{ext}$).

## 6.3.1 Coupling as Branching Ratio

Consider a single-step unitary evolution operator $\hat{U}$ on QCA. When a particle carrying internal state (such as spin or color charge) moves from node $x$ to $x+1$, it faces two orthogonal evolution directions:

1. **Translation Channel**: Keep internal state unchanged, only change position. This corresponds to free propagation, amplitude $t$.

2. **Rotation Channel**: While moving, acted upon by connection field $U_{x+1, x}$, undergo internal phase rotation or mixing. This corresponds to interaction, amplitude $r$.

Due to unitarity, $|t|^2 + |r|^2 = 1$.

We define **coupling constant** $g$ as ratio (some function) of interaction amplitude to free propagation amplitude:

$$g \sim \frac{|r|}{|t|} \approx \tan \theta_{mix}$$

where $\theta_{mix}$ is mixing angle between translation generator and rotation generator.

**Physical Picture 6.3.1**:

Imagine a beam splitter. When photons pass through, most pass directly (free propagation), small part reflected or polarization changed (interaction). Coupling constant is this beam splitter's "reflectance." In QCA, every lattice connection is such a beam splitter.

## 6.3.2 Geometric Origin: Dimension and Connectivity Number

Why does $g$ take specific values? This depends on underlying lattice geometric structure.

Suppose QCA network is a $D$-dimensional hypercubic lattice. Each node has $2D$ nearest neighbors.

If particles have $N$ internal degrees of freedom (e.g., $SU(N)$ symmetry).

When particles perform one step of random walk, information can choose $2D$ spatial directions, simultaneously $N^2-1$ internal rotation directions (number of Lie algebra generators).

According to **Information Equipartition Hypothesis**, at Planck scale limit high energies, information flow is uniformly distributed among all possible channels.

$$g_{bare}^2 \propto \frac{\text{internal channel number}}{\text{spatial channel number}} = \frac{C_2(G)}{2D}$$

where $C_2(G)$ is quadratic Casimir operator of gauge group, related to group dimension.

This means, **strengths of fundamental interactions are geometrically determined by space dimension and size of internal symmetry groups.**

For example, for $U(1)$ electromagnetic field ($N=1$) in $D=3$ space, channel ratio is small, hence electromagnetic force is weak. For $SU(3)$ color charge ($N=3$) in same space, internal channels dramatically increase, leading to strong interaction.

## 6.3.3 Discrete Interpretation of Renormalization Group Flow: Resolution and Screening

Experimentally observed coupling constants vary with energy scale (resolution), this is **renormalization group (RG) flow**. In QCA, this has an extremely intuitive explanation.

**1. Screening Effect (e.g., QED)**:

When we observe network with low resolution (long wavelength $\lambda \gg a$), we are actually averaging nodes within a large region.

Due to random fluctuations of local phases (quantum noise), coherent rotations in large regions are partially canceled.

**Corollary**: As observation scale increases (energy decreases), effective coupling constant $g_{eff}$ decreases. This corresponds to infrared free behavior of Abelian gauge fields (such as electromagnetic fields).

**2. Anti-screening Effect (e.g., QCD)**:

For non-Abelian fields, connection fields themselves carry charges (gluons carry color). When we zoom out observation scale, nonlinear entanglement networks between connection fields become more complex.

Imagine a fractal network: the farther you look, the more curled details, the larger "surface area," causing effective interaction cross-section to increase.

**Corollary**: As observation scale increases (energy decreases), effective coupling constant $g_{eff}$ increases. This corresponds to **quark confinement** of non-Abelian fields—at long distances, interaction is so strong it cannot be separated.

## 6.3.4 Derivation Conjecture of Fine Structure Constant $\alpha$

Can we calculate $\alpha \approx 1/137.036$?

This requires extremely high-precision QCA lattice models. But in order of magnitude, we can give a heuristic derivation.

Consider a QCA node in 3D space. Its solid angle is $4\pi$.

A particle emitting a photon (interaction) essentially projects information toward one direction in $4\pi$ space.

If lattice points are close-packed, effective channel number relates to geometric factors.

Wyler once proposed a mathematical conjecture based on geometric volume of bounded complex domains:

$$\alpha = \frac{9}{16\pi^3} (\frac{\pi}{5!})^{1/4} \dots$$

In QCA framework, this corresponds to calculating: **geometric probability of homomorphic transmission on a discrete, unitary-satisfying $S^3 \times S^1$ discrete manifold.**

Although there is currently no numerically precise proof, QCA provides the only possible path to finding this "God's number": it is not an arbitrary parameter, but a **combinatorial constant of discrete geometry** (like $\pi$ or $e$).

## 6.3.5 Summary

Coupling constant $g$ is a bridge connecting microscopic discrete structure with macroscopic continuous field theory.

* **Microscopically**, it is **branching ratio** of information between "displacement" and "transformation."

* **Macroscopically**, it manifests as **force** strength.

* **Essentially**, it is **geometric projection** of network topological structure (dimension, connectivity number, group structure).

We feel strong force is stronger than electromagnetic force because in underlying QCA networks, channels for transmitting "color" information are much wider than channels for transmitting "phase" information.

At this point, we have completed Part III "The Emergence of Matter." We have constructed particles (topological knots), mass (impedance), fermions (square root), and interactions (connection fields) from nothing.

Hardware (spacetime) and data (matter) of this cosmic computer are in place. Next, in Part IV, we will explore the most exciting software part—what happens when this computer begins to "observe itself"? Welcome to **The Emergence of Observation**.

# 7.1 The Discrete Solution to Schrödinger's Cat: Entanglement and Branching under Unitary Evolution

In this chapter, we face the most troubling and controversial field in physics: **The Measurement Problem**.

In classical mechanics, observers are "God's eyes" standing outside; their observation behavior does not interfere with physical system evolution. But in quantum mechanics, observation seems to play an active, even destructive role—it causes wave function **collapse**. This directly conflicts with unitary evolution of Schrödinger's equation.

In our discrete QCA ontology, since the universe is a strictly unitary, deterministic computational process (Axiom $\Omega$), what exactly is "collapse"? Where does probability come from?

This chapter will propose a revolutionary view: **Wave functions never collapse.** So-called "collapse" and "randomness" are merely **perspective effects** produced when finite local observers perform **self-location** in the vast holographic entanglement network.

We will prove: **Objective Reality** is not a priori existence, but **consensus** reached between different observers through entanglement and communication.

Let's return to that famous thought experiment: Schrödinger's cat.

In a closed box, there is a cat, a radioactive atom, and a poison device. The atom is in superposition of decayed and undecayed:

$$|\psi_{atom}\rangle = \alpha |0\rangle + \beta |1\rangle$$

According to linearity of quantum mechanics, after some time, the entire system (atom + cat) evolves into a macroscopic superposition:

$$|\Psi_{total}\rangle = \alpha |0\rangle |Live\rangle + \beta |1\rangle |Dead\rangle$$

The problem is: When we open the box, we either see a live cat or a dead cat, never a "half-dead half-alive" cat. Why?

**Traditional Explanation (Copenhagen)**: At the moment of observation, wave function non-unitarily jumps to one branch.

**QCA Explanation (Many-branches/Relative State)**: Wave function continues unitary evolution, but observer is also entangled.

## 7.1.1 Physicalization of Observers

In QCA framework, observers are not gods, but **physical subsystems** with memory and processing capabilities.

Let observer's initial state be $|Ready\rangle$.

When observer opens the box to observe, their state unitarily entangles with system state (driven by local interaction Hamiltonian):

$$|\Psi_{final}\rangle = \hat{U}_{obs} (|\Psi_{total}\rangle \otimes |Ready\rangle)$$

$$= \alpha |0\rangle |Live\rangle |Saw\_Live\rangle + \beta |1\rangle |Dead\rangle |Saw\_Dead\rangle$$

Note that the entire wave function $|\Psi_{final}\rangle$ is still a pure state, no collapse occurred. But this wave function now contains two terms, each describing a **self-consistent world history**.

* **Branch A**: Atom undecayed, cat alive, observer sees cat alive.

* **Branch B**: Atom decayed, cat dead, observer sees cat dead.

## 7.1.2 Dynamical Isolation of Branches

The key question is: Why can't observer in Branch A sense existence of Branch B?

This stems from QCA network's **vast degrees of freedom** and **decoherence**.

Cats and observers are macroscopic objects, containing $10^{23}$ order qubits.

Two macroscopic states $|Live\rangle$ and $|Dead\rangle$ are not only orthogonal in Hilbert space, but **almost impossible to overlap**.

To make these two branches interfere again (i.e., make observer A sense observer B), we need to precisely reverse states of all $10^{23}$ particles in the entire system.

Under QCA's complex dynamics, probability of such reversal decays exponentially with time (Poincaré recurrence time is unimaginably long).

Therefore, although mathematically both branches exist in global wave function, physically and dynamically, they are **causally disconnected**.

For observer in Branch A, Branch B is like falling into a black hole horizon—theoretically exists, but practically inaccessible.

**Conclusion**:

**Collapse is not disappearance of wave function, but inaccessibility of information.**

This phenomenon is called **"Branching"**. The universe does not split into two; rather, observer's own state splits into two mutually orthogonal, mutually invisible copies.

Each copy believes it sees the only outcome. This is the source of subjective "collapse" illusion.

In the next section, we will solve a more difficult problem: Why is the probability observer sees live cat $|\alpha|^2$, not something else? We will give a pure mathematical proof of Born's rule.

# 7.2 Combinatorial Proof of Born Rule: Based on Zurek Rotation and Microstate Counting

In the previous section, we eliminated physical reality of wave function collapse through "branching" concept, interpreting measurement as update of observer's horizon. However, this only solves the qualitative part of measurement problem (why there are definite results), leaving a quantitative puzzle: **Why does probability observer finds themselves in a particular branch strictly follow Born's rule $P_k = |\psi_k|^2$?**

In standard quantum mechanics, this is an independent axiom. But in our "Ultimate Axiom $\Omega$" system, besides unitarity and locality, there are no presuppositions about probability. If Born's rule cannot be derived from unitarity, our theory is incomplete.

This section will provide a proof purely based on **combinatorics** and **symmetry**. We will show: In discrete QCA networks, so-called "probability" is essentially counting of **micro-degeneracy** in holographic networks.

## 7.2.1 Discrete Ontology of Amplitudes: Weights as Path Counting

In continuous quantum mechanics, complex amplitude $\alpha \in \mathbb{C}$ is an abstract mathematical quantity. But in discrete path integral (sum-over-histories) perspective, amplitudes have clear counting meaning.

Consider evolution from initial state $|i\rangle$ to final state $|f\rangle$. QCA's unitary operator $\hat{U}$ can be viewed as sum of path weights on graph. If underlying lattice structure has some discrete symmetry (e.g., all basic paths have equal weight modulus), then macroscopic amplitude magnitude $|\alpha|$ actually reflects **number of microscopic paths to that state** (or degeneracy of microstates).

**Axiom 7.2 (Microscopic Equal Weight Axiom)**:

At QCA's deepest level (Planck scale), all orthogonal micro-basis states are ontologically equal. There is no "this basis is more real than that basis."

This means, if two macroscopic states $|A\rangle$ and $|B\rangle$ correspond to $N_A$ and $N_B$ indistinguishable microstates respectively at the bottom level, then according to Laplace's **Principle of Insufficient Reason**, probability observer finds themselves in $|A\rangle$ should be $N_A / (N_A + N_B)$.

Our task is to connect wave function amplitude $\alpha$ with microscopic number $N$.

## 7.2.2 Schmidt Decomposition and Environment-Assisted Invariance (Envariance)

We will adopt **Environment-Assisted Invariance (Envariance)** idea proposed by Wojciech Zurek and adapt it to discrete framework.

Consider entangled state of system $\mathcal{S}$ and environment $\mathcal{E}$. According to Schmidt Decomposition, any pure state can always be written as:

$$|\Psi_{\mathcal{S}\mathcal{E}}\rangle = \sum_k c_k |s_k\rangle_{\mathcal{S}} |e_k\rangle_{\mathcal{E}}$$

where $|s_k\rangle$ are system's pointer states (such as cat dead/alive), $|e_k\rangle$ are environment's orthogonal states (such as photons recording dead/alive).

**Case One: Equal Weight Superposition**

First consider simplest case, all coefficients equal: $c_k = 1/\sqrt{N}$.

$$|\Psi\rangle \propto |s_1\rangle|e_1\rangle + |s_2\rangle|e_2\rangle + \dots + |s_N\rangle|e_N\rangle$$

We ask: What is probability $P(1)$ observer measures state $|s_1\rangle$?

**Symmetry Argument**:

1. **Unitary Swap**: We can perform unitary transformation $\hat{U}_{\mathcal{S}}$ on system $\mathcal{S}$, swapping $|s_1\rangle$ and $|s_2\rangle$. This changes physical state:

   $$|s_2\rangle|e_1\rangle + |s_1\rangle|e_2\rangle + \dots$$

   Now, $|e_1\rangle$ is associated with $|s_2\rangle$.

2. **Environment Compensation**: But we can also perform inverse transformation $\hat{U}_{\mathcal{E}}$ on environment $\mathcal{E}$, swapping $|e_1\rangle$ and $|e_2\rangle$.

   After joint operation $\hat{U}_{\mathcal{E}} \hat{U}_{\mathcal{S}}$, state becomes:

   $$|s_2\rangle|e_2\rangle + |s_1\rangle|e_1\rangle + \dots$$

   This is **mathematically identical** to initial state $|\Psi\rangle$ (only summation order changed).

3. **Corollary**: Since swapping system (changing target of physical prediction) can be completely canceled by operating on environment (not changing system physical prediction), this means **physical probability should not depend on labels $1$ or $2$**.

   Therefore, must have $P(1) = P(2) = \dots = P(N) = 1/N$.

This proves: **For entangled states with equal coefficient moduli, probabilities are equal.**

## 7.2.3 Fine-Graining: From Amplitudes to Counting

Now handle general case, coefficients unequal. For example:

$$|\Psi\rangle = \sqrt{\frac{2}{3}} |0\rangle_S |e_0\rangle_E + \sqrt{\frac{1}{3}} |1\rangle_S |e_1\rangle_E$$

How do we prove $P(0) = 2/3$?

In QCA discrete ontology, complex coefficient $\sqrt{2/3}$ is not fundamental. It is a result of **coarse-graining**. It means environment state $|e_0\rangle_E$ is actually not a single microstate, but superposition of two indistinguishable microstates.

We perform **fine-graining decomposition** on environment Hilbert space:

* $|e_0\rangle \to \frac{1}{\sqrt{2}} (|e_{0,a}\rangle + |e_{0,b}\rangle)$

* $|e_1\rangle \to |e_{1,a}\rangle$

Substituting into original expression, we get a more fundamental microstate:

$$|\Psi\rangle \propto |0\rangle_S |e_{0,a}\rangle + |0\rangle_S |e_{0,b}\rangle + |1\rangle_S |e_{1,a}\rangle$$

(ignoring normalization constant).

Now, we face 3-term equal-weight superposition.

According to previous equal probability theorem, these 3 microscopic branches each have probability $1/3$.

* Observer seeing $|0\rangle$ event corresponds to first two branches: $P(0) = 1/3 + 1/3 = 2/3$.

* Observer seeing $|1\rangle$ event corresponds to third branch: $P(1) = 1/3$.

This is exactly $|\alpha|^2$ and $|\beta|^2$!

## 7.2.4 Why Square? — Statistics of Pythagorean Theorem

Why does physical amplitude $\alpha$ correspond to $\sqrt{N}$ rather than $N$? This directly stems from QCA's **global unitarity**.

In QCA, evolution operator $\hat{U}$ preserves vector's $L_2$ norm (modulus), corresponding to geometric Pythagorean theorem.

$$||\Psi||^2 = \sum |\alpha_i|^2 = 1$$

If we assume $\alpha_i$ represents microscopic path number $N_i$, then superposition principle would become $L_1$ norm conservation (probabilities directly add), but this would destroy mathematical structure of interference phenomena.

To simultaneously satisfy:

1. **Linear Superposition Principle** (core feature of quantum mechanics);

2. **Probability Conservation** (logical consistency);

**Probability must be quadratic function of amplitude.**

In QCA's discrete geometry, **amplitude is "edge length," probability is "face area."**

* Microscopic state number $N$ corresponds to **volume (measure)** in Hilbert space.

* When we project a large vector onto basis, squared projection length represents volume share contained in that basis direction.

**Theorem 7.2 (Born Rule Derivation Theorem)**:

In a discrete QCA system satisfying unitary evolution and environment-assisted invariance, subjective probability $P_k$ any local observer measures result $k$ necessarily equals modulus squared of that branch amplitude $|c_k|^2$.

## 7.2.5 Conclusion

Born's rule is no longer a confusing axiom. It is **result of counting microstates when we perform incomplete observation in a deterministic, unitary universe.**

* **God does not throw dice**: Entire universe's evolution is 100% deterministic.

* **Dice are in our hearts**: Because we are finite observers, we can only see a slice of the vast holographic network. The "randomness" we perceive is actually our **ignorance** about which position we occupy in the network.

Probability is subjective horizon's measure of objective information.

# 7.3 The Achievement of Objective Reality: Nash Equilibrium and Consensus Geometry in Multi-Agent Systems

From the combinatorial proof of Born's rule in the previous section, we got a startling conclusion: So-called "randomness" is merely subjective feeling of local observers due to information truncation. This seems to push physics toward solipsism—if every observer is in their own "information bubble," each bubble has its own historical branch, then what is the shared, unbreakable "objective reality" we all perceive?

This section will unify quantum mechanics with game theory by introducing **multi-agent game** models, proving: **Objective reality is not a priori physical background, but Nash equilibrium emerging from a vast, decentralized communication network.**

Spacetime geometry is essentially a "consensus protocol" reached by all observers.

## 7.3.1 Wigner's Friend and Consensus Puzzle

To understand fragility of "objectivity," let's revisit "Wigner's Friend" thought experiment.

1. Friend $F$ measures a qubit in a sealed room, gets result $|0\rangle$ or $|1\rangle$. For $F$, wave function collapsed, world is definite.

2. Wigner $W$ stands outside the room. For $W$, entire room (including friend) is in superposition $|\Psi\rangle = \alpha |0\rangle |F_0\rangle + \beta |1\rangle |F_1\rangle$.

3. At this point, for $F$, fact is "I saw 0"; for $W$, fact is "friend is in superposition."

If $F$ walks out and tells $W$: "I saw 0," then $W$'s wave function also "collapses."

This reveals definition of objectivity: **Objectivity = mutual consistency of information between different observers.**

In QCA framework, we no longer discuss abstract "wave functions," but **consistency checks**.

If $W$ and $F$ want to reach consensus, they must physically **communicate** (exchange photons or matter).

Communication itself is a physical process (interaction), causing $W$ and $F$'s states to entangle.

**Theorem 7.3.1 (Consensus Entanglement Theorem)**:

Under unitary evolution, two observers $A$ and $B$ can only reach consensus on measurement result of observable $\hat{O}$ when their quantum states are completely entangled about $\hat{O}$ (i.e., in same branch of Schmidt decomposition).

$$|\Psi_{AB}\rangle \approx \sum_k c_k |A_k\rangle |B_k\rangle$$

If state is $|A_k\rangle |B_j\rangle$ ($k \neq j$), it's called "illusion" or "disagreement." Unitary dynamics (interaction Hamiltonian) tends to eliminate disagreement terms (through decoherence), driving system into diagonalized consensus state.

## 7.3.2 Nash Equilibrium: Why Do We See the Same Moon?

Einstein once asked: "Does the moon exist only when I look at it?"

From QCA perspective, this question becomes: "Why does everyone see the moon at the same position?"

We model the universe as a game network composed of $N$ agents. Each agent $i$ maintains an internal model $M_i$ (beliefs about external world).

Agent's goal is to minimize **prediction error** (i.e., free energy $F$).

$$F_i = \text{KL}(q_i(\text{state}) || p(\text{sensory data}))$$

If each agent independently constructs models, their worldviews might be completely different (like schizophrenic patients).

But agents have **communication** between them (light, sound, gravity).

Communication causes **coupling**. If agent $A$ thinks moon is on left, and agent $B$ thinks moon is on right, when they exchange information, both produce huge prediction errors (surprise).

To minimize this error, they must adjust their respective internal models to be **mutually compatible**.

**Definition 7.3.2 (Objective Reality as Nash Equilibrium)**:

Let total free energy of entire system be $F_{total} = \sum_i F_i(M_i, \{M_{j \neq i}\})$.

System's evolution dynamics $\dot{M}_i \propto -\nabla_{M_i} F_{total}$ drives system to find minimum.

When system reaches steady state $\frac{dM_i}{dt} = 0$, all agents' models $\{M^*_i\}$ reach a **Nash equilibrium**.

**Common part** contained in this equilibrium state $\{M^*\}$ is what we call "objective reality."

**Conclusion**: Moon is there because all observers (including photons, air molecules) have "negotiated" its position through continuous interactions. Any observer deviating from this consensus will be severely "punished" by environmental information (decoherence), until they correct their model or are assimilated by environment.

## 7.3.3 Consensus Geometry: Spacetime as the Largest Public Ledger

Pushing this view to the extreme: **Spacetime itself is the largest consensus structure.**

In Chapter 4, we defined geometry as entanglement. Now we can say: Geometry is **causal relationship protocol recognized by all observers**.

* **Distance**: Not absolute ruler length, but consensus on "if I send you a signal, how long do you need to receive it."

* **Position**: Not absolute coordinates, but "my network relationship relative to all other objects."

In QCA networks, there is no absolute $x, y, z$. But due to rigidity of local interactions, all observers eventually agree on network's **topological structure** by exchanging photons (measuring light paths).

This network-wide consistent topological skeleton is the **continuous spacetime manifold** we perceive.

In this sense, physical laws (such as relativity) are not merely natural laws; they are **underlying algorithms for distributed networks to maintain data consistency**.

* Special Relativity (Lorentz transformations) is **data conversion protocol between different reference frames**.

* General Relativity (covariance) is **consensus protocol ensuring physical truth is independent of node coordinates**.

## 7.3.4 Layers of Reality

We arrive at a layered reality picture:

1. **Ontic Level**: Underlying QCA network, unitary, deterministic, but unknowable to single observers (hidden).

2. **Consensus Level**: Macroscopic classical world (spacetime, matter) emerging through multi-agent games. This is "objective reality."

3. **Subjective Level**: Quantum branches private to single observers (illusions, dreams, uncollapsed wave functions).

Science's task is to extract that hard consensus layer from this subjective fog through experiments (forcing games with environment).

## 7.3.5 Summary

Objectivity is not a gift from heaven, but an **evolutionary achievement**.

The universe weaves classical consistency from quantum chaos through countless microscopic "handshakes" and "entanglements." We live in the same world because we continuously "measure" each other, and in this process become part of each other's reality.

This concludes Part IV's discussion on observation. We have explained probability, measurement, and objectivity.

In the next Part V, also the final part of the book, we will put these theories to the cruelest battlefield—**experimental verification**. If all this is not just philosophy, it must be observable.

# 8.1 Observer Model: Computational Structure (Agent) as Self-Referential Subsystem

In traditional physics narratives, "observers" are often treated as awkward ghosts. In classical mechanics, they are massless, volumeless "God's eyes"; in standard quantum mechanics, they are external "black boxes" causing wave function collapse. Physical laws seem to describe a universe without audience, but all physical knowledge comes from observation.

In the discrete ontology constructed in this book, we reject this dualism. Since the universe is unitary QCA, **observers must be part of this QCA network**. Consciousness is not a miracle transcending physical laws, but a special, advanced **computational structure**.

This chapter will attempt to establish foundations of "consciousness physics." We will prove: **Observers are self-referential subsystems emerging in information flow.** They acquire "agency" by maintaining **Markov blanket** on their boundaries and compressing chaotic environmental information into low-entropy **internal models**.

To define what an "observer" is, we must first define what an "individual" is. In a fully connected or uniform QCA lattice, where does "I" begin, where does "environment" end?

## 8.1.1 Mathematical Definition of Boundary: Markov Blanket

Consider entire system Hilbert space $\mathcal{H}_{total}$. Any subsystem partition $\mathcal{H}_{total} = \mathcal{H}_{in} \otimes \mathcal{H}_{out}$ is mathematically legal, but vast majority of partitions are physically meaningless (e.g., grouping one atom from moon with one neuron from your brain).

Meaningful physical individuals must have **dynamical independence**. This can be formalized through **Markov Blanket** concept.

> **Definition 8.1 (Markov Blanket and Graph Partition)**:
>
> In QCA's interaction graph $\Lambda = (V, E)$, let $V_{in} \subset V$ be a set of nodes representing "internal states."
>
> **Markov Blanket** $V_{mb}$ of $V_{in}$ is defined as boundary node set of $V_{in}$ on graph (nodes directly connected to $V_{in}$ but not belonging to $V_{in}$).
>
> Remaining nodes $V_{ext} = V \setminus (V_{in} \cup V_{mb})$ are called "external states" or "environment."
>
> If following condition is satisfied, then $(V_{in}, V_{mb})$ constitutes a **statistically independent subsystem**:
>
> **Conditional Independence**: Given blanket state $V_{mb}$, evolution of $V_{in}$ is independent of $V_{ext}$.
>
> $$P(v_{in}(t+1) | v_{in}(t), v_{mb}(t), v_{ext}(t)) = P(v_{in}(t+1) | v_{in}(t), v_{mb}(t))$$

Physically, $V_{mb}$ is further decomposed into:

1. **Sensory Nodes**: Receive information from environment $V_{ext}$, transmit to $V_{in}$.

2. **Active Nodes**: Receive commands from $V_{in}$, change environment $V_{ext}$.

**Physical Picture 8.1**:

An observer is essentially an **information membrane** in spacetime network. This membrane isolates universe into "me" (internal) and "not me" (external). Without this membrane, there is no observer, only a chaotic ocean of information.

## 8.1.2 Self-Reference and Internal Model: I Am What I Compute

Having boundary alone is not enough. A rock also has boundary, but rocks are not observers. True observers (agents) must have **internal models**.

In QCA dynamics, this means evolution rules $\hat{U}_{in}$ of internal nodes $V_{in}$ must contain a **recursive** or **self-referential** structure.

> **Definition 8.2 (Agent)**:
>
> A subsystem $\mathcal{A} = (V_{in}, V_{mb})$ is called an **agent** if its internal state $\rho_{in}$ encodes a **compressed mapping** of external environment state $\rho_{ext}$.
>
> $$\rho_{in} \approx \mathcal{M}(\rho_{ext})$$
>
> where $\mathcal{M}$ is a homomorphic mapping.

This means agents don't just react passively; they **simulate** external world internally.

* **Memory**: Agents must maintain temporal correlations of certain states, resisting environmental thermal fluctuations (forgetting). This requires $V_{in}$ to have long-range temporal entanglement structure (this is macroscopic upgrade of "topological mass" we discussed in Chapter 5).

* **Prediction**: Agents use internal model $\mathcal{M}$ to deduce future environmental states based on current sensory input.

**Minimum Threshold of "Consciousness"**:

When this internal model $\mathcal{M}$ becomes complex enough that it not only contains simulation of "external world," but also simulation of **"self's role in external world"** (i.e., model contains a symbol representing the system), **self-awareness** emerges.

$$M_{model} \supset \{ \text{World}, \text{Self} \}$$

## 8.1.3 Resisting Heat Death: Schrödinger's Definition of Life

According to second law of thermodynamics, entropy of closed systems always increases. If $V_{in}$ frequently exchanges information with $V_{ext}$, internal structure should rapidly be assimilated by environment (thermalization), causing boundary to disappear, observer to die.

But observers exist precisely because they **refuse thermalization**.

> **Axiom 8.1 (Principle of Agency)**:
>
> Observers are **non-equilibrium steady state structures (NESS)** in QCA networks that can actively maintain their Markov blankets from environmental dissipation by consuming free energy.

This returns to **topological knots** we discussed in Chapter 5.

* **Elementary Particles** (such as electrons) are simplest observers: They maintain their non-trivial structure through topological winding (winding number $\mathcal{W}=1$), resisting vacuum trivialization.

* **Living Organisms** are complex observers: They continuously repair their Markov blankets (cell membranes/skin) at molecular level through metabolism (extracting negentropy).

* **Intelligent Consciousness** are highest-level observers: They maintain logical consistency at information level through computation (cognition), resisting conceptual entropy increase (mental disorder).

## 8.1.4 Conclusion: Observers as Physics' "Solitons"

In summary, we give strict physical definition of observers:

**Observers are complex topological solitons in QCA information fluid.**

1. **Structurally**: Defined by Markov blanket boundary.

2. **Functionally**: Running simulation algorithms of environment (self-referential).

3. **Dynamically**: Maintaining their low-entropy state through dissipative structures.

This definition removes mystery of consciousness. Consciousness is not magical smoke produced by brains; it is **specific geometric form formed by information flowing in closed loops**. As long as network is complex enough, emergence of this form is as inevitable as vortices emerging in turbulence.

In the next section, we will quantitatively describe this dynamical mechanism of "maintaining low entropy"—**free energy minimization principle**. This will explain why observers always tend to "understand" the universe.

# 8.2 Information Mass $M_I$ and Free Energy Minimization

In the previous section, we defined "observers" as self-referential subsystems in QCA networks with **Markov blankets** and **internal models**. Now, we face a dynamics question: What drives this subsystem to "observe" the world? Why doesn't it just lie flat, letting environment thermalize it (entropy increase)?

This section will introduce two core concepts: **Information Mass ($M_I$)** and **Variational Free Energy ($F$)**. We will prove that observer's "survival instinct" (maintaining low entropy) and "curiosity" (exploring environment) are physically equivalent to **minimizing free energy**. This process is the physical source of universe producing "meaning" and "value."

## 8.2.1 Information Mass $M_I$: Physical Measure of Complexity

In Light Path Conservation theory ($v_{ext}^2 + v_{int}^2 = c^2$), we defined rest mass $m_0$ as frequency of particles "rotating in place" at microscopic scales. For macroscopic observers (such as brains or AI), we cannot simply add up all atomic rest masses, because this linear superposition ignores **structural information**.

A dead brain and a living brain have same number of atoms, same rest mass, but in QCA networks, their dynamical behaviors are completely different. Living brain maintains extremely complex **long-range entanglement** and **self-referential loops**.

We define a new physical quantity—**Information Mass ($M_I$)**—to measure this structural complexity.

> **Definition 8.3 (Information Mass $M_I$)**:
>
> For a subsystem $\mathcal{S}$, its information mass $M_I$ is defined as product of its **effective computational depth** and **causal integration**.
>
> $$M_I \equiv \frac{\hbar}{c^2} \cdot \Phi_{integrated} \cdot \mathcal{D}_{logical}$$
>
> * **$\Phi_{integrated}$ (Integrated Information)**: Measures degree system is a whole rather than collection of fragments (microscopic counterpart based on IIT theory). In QCA graph theory, this corresponds to bit number of minimum cut (Min-cut) inside Markov blanket.
>
> * **$\mathcal{D}_{logical}$ (Logical Depth)**: Running steps of shortest Turing machine program required to generate system's current state (Bennett's logical depth).

**Physical Meaning**:

$M_I$ is not "weight"; it is **information-theoretic version of "inertia."**

A system with large $M_I$ (such as consciousness) is extremely difficult to change by environmental noise (large inertia). To change its internal beliefs, extremely high-precision information (force) input is needed.

According to Light Path Conservation, larger $M_I$ means system consumes more light path quota $v_{int}$ on internal computation, therefore its "effective velocity" $v_{ext}$ in external physical space is more limited (tending to rest/stillness).

**This is why deep thinkers often appear "motionless."**

## 8.2.2 Free Energy Principle: Thermodynamic Command of Observers

Observers must resist second law of thermodynamics to maintain their high $M_I$ state. **Free Energy Principle** proposed by Karl Friston provides a perfect mathematical framework, embedding it into QCA dynamics.

Let environmental state be $\vartheta$ (not directly observable), observer's sensory state be $s$ (projection on Markov blanket), observer's internal model (probabilistic inference about environment) be density matrix $q(\vartheta)$.

Observer's goal is to minimize **variational free energy $F$**:

$$F = \underbrace{D_{KL}[q(\vartheta) || p(\vartheta | s)]}_{\text{Divergence}} - \underbrace{\ln p(s)}_{\text{Surprise}}$$

* **First Term (Divergence)**: Measures deviation of internal model $q$ from true posterior distribution $p(\vartheta|s)$. Minimizing it means **Perception**—making beliefs approach truth.

* **Second Term (Surprise)**: Measures impossibility of current sensory input $s$. Minimizing it means **Action**—changing environment to produce expected input (e.g., if you predict you'll feel warm, you'll enter house to avoid cold).

In QCA ontology, free energy $F$ has clear physical counterpart: **It is relative entropy between system and environment.**

> **Theorem 8.2 (Free Energy Minimization Theorem)**:
>
> Under local unitary evolution, any subsystem capable of long-term maintaining its Markov blanket (i.e., observer) necessarily has dynamics trajectory that is a path **minimizing long-time average free energy $\bar{F}$**.

**Proof Idea**:

According to Landauer's principle, processing information requires dissipating heat. Free energy $F$ is "information cost" system must pay to maintain non-equilibrium state. If system doesn't minimize $F$, it accumulates too much entropy, eventually causing Markov blanket's thermal disintegration (death).

Therefore, **all observers that "survive" are necessarily Bayes-optimal.**

## 8.2.3 Emergence of Meaning: From Bits to Value

Free energy principle not only explains "how observers exist," but also "why observers seek knowledge."

To minimize future free energy (expected free energy $G$), observers must maximize **epistemic value**:

$$G(\pi) \approx \underbrace{H(A) - H(A|S)}_{\text{Information Gain}} + \underbrace{\mathbb{E}[U]}_{\text{Extrinsic Utility}}$$

* **Information Gain**: Observers actively explore regions that can maximally reduce uncertainty of their internal models. This is physical origin of **curiosity**.

* **Extrinsic Utility**: Observers tend toward states that maintain their physical structure steady state (such as ingesting negentropy). This is physical origin of **desire**.

At QCA's bottom level, this is merely bit flow. But at macroscopic level, this emerges as **teleology**: Observers seem to act for some "purpose."

Actually, this "purpose" is just **projection of physical conservation laws in complex systems**—to keep $M_I$ constant in entropy-increasing torrent, observers must desperately "eat" ordered information, expel disordered waste heat.

## 8.2.4 Summary

We have completed physics demystification of consciousness:

1. **Consciousness is not miracle**; it is physical structure with high $M_I$.

2. **Thinking is not void**; it is computational process consuming $v_{int}$.

3. **Seeking knowledge is not caprice**; it is survival instinct to minimize free energy, resist heat death.

Observers are "negentropy light drives" evolved by universe to understand itself.

At this point, Part IV "The Emergence of Observation" is completely finished. We have explained measurement, objectivity, and consciousness.

Now, this cosmic computer can observe and explain itself. The final part, also the most crucial part—**Part V: Verification and Inference**—will bring us back to reality, seeing if this grand theory can withstand experimental interrogation.

# 8.3 Red Queen Effect and Thermodynamics: Why Does the Universe Not Have Heat Death?

Classical thermodynamics, based on Boltzmann's H-theorem, predicts a despairing cosmic end: **Heat Death**. In an isolated system, entropy always tends to maximize. Over time, all temperature differences are smoothed, all structures disintegrate; universe ultimately falls silent as a uniform, disordered thermal radiation soup.

However, when we look around, we see a completely opposite scene. In 13.8 billion years since Big Bang, universe not only didn't become more uniform, but evolved stars, galaxies, life, brains, and even the internet. Complexity seems to have an intrinsic trend against entropy increase.

Traditional physics explanations usually appeal to extremely low-entropy initial conditions (Big Bang), but this doesn't explain dynamical mechanisms of structure maintenance. Why does universe bloom such brilliant flowers on the path to death?

This section will propose a completely new answer based on QCA's game-theoretic perspective: **Heat death is impossible.** As long as observers (agents) capable of self-referential computation exist in the universe, **"Red Queen Effect"** will drive system forever in **algorithmic turmoil** far from equilibrium state.

## 8.3.1 Relative Fitness and Arms Race

In biological evolution, Leigh Van Valen proposed famous "Red Queen Hypothesis": **"In this country, you must run as fast as you can just to stay in place."** This means a species' survival environment mainly consists of other species; to cope with evolution of predators, parasites, or competitors, the species must continuously evolve. This leads to an endless arms race.

In QCA universe, this effect has strict physical correspondence.

According to Section 8.2, observers' (agents') goal is to minimize **free energy $F$** (prediction error).

$$F_i = D_{KL}[q_i || p_{true}]$$

But what determines environment's true probability distribution $p_{true}$?

In a multi-agent universe, environment mainly consists of **other agents**.

$$p_{true} \approx \sum_{j \neq i} \rho_j$$

If agent $A$ successfully reduces its free energy by increasing information mass $M_I$ (optimizing algorithms), this means its behavior becomes more complex, harder to predict.

For agent $B$, environment changed. $B$'s prediction model fails, its free energy $F_B$ suddenly **increases** (surprise increases).

To survive (pressing $F_B$ back below threshold), $B$ is forced to counterattack—upgrade its model, increase its $M_I$.

**Conclusion**: In computational universe, entropy decrease is not static enjoyment, but dynamic war. Any system trying to stay in low-entropy equilibrium state will be rapidly assimilated or disintegrated by higher-order algorithms in environment.

## 8.3.2 Algorithmic Turmoil and Self-Organized Criticality

This mutual feedback dynamics prevents system from relaxing to thermal equilibrium state (maximum entropy state).

Consider evolution trajectory in phase space.

* **Heat Death Theory** believes: System has a global attractor, maximum entropy state $S_{max}$. All trajectories eventually converge here, $\frac{dS}{dt} = 0$.

* **Red Queen Theory** believes: Due to games between agents, there are no stable fixed points in phase space.

  * When system approaches equilibrium, tiny fluctuations produce a "Maxwell's demon" (primary observer) capable of utilizing remaining free energy.

  * This observer rapidly consumes resources, breaks equilibrium, triggering new round of complexity growth.

This state is called **Self-Organized Criticality (SOC)**.

Universe is like a sandpile constantly collapsing and rebuilding. Although local structures (such as stars, civilizations) are born and die, **overall complexity level** and **computational activity** always maintain above a critical threshold.

We call this never-ending, far-from-equilibrium dynamic picture **"Algorithmic Turmoil."** Like turbulence in fluids, although viscosity (dissipation) tries to smooth everything, energy injection (competition) continuously produces new vortices.

## 8.3.3 Deep Connection Between Waste Heat and Cosmic Expansion

Since observers continuously increase $M_I$ (negentropy), according to second law of thermodynamics, total entropy must increase. Where does this extra entropy go?

According to Landauer's principle, logically irreversible computation (erasing information to update models) must emit heat to environment.

$$dQ \ge k_B T \ln 2 \cdot dI$$

Every evolving agent is a huge **entropy pump**. It constructs highly ordered crystal structures internally, while emitting massive "information waste heat" (disordered radiation) to external vacuum.

In Chapter 9 (Section 9.3) and related papers, we have argued: **This accumulated information waste heat manifests macroscopically as dark energy.**

Cosmic accelerated expansion is actually universe's forced "expansion" to accommodate massive waste heat produced by agents.

This is a startling closed loop:

1. Agents perform computational games (Red Queen effect) to resist heat death.

2. Computation produces waste heat.

3. Waste heat drives cosmic expansion (dark energy).

4. Expansion dilutes waste heat, preventing system overheating, thus allowing computation to continue.

**Conclusion**: Universe will not heat death, because life (generalized computational structures) doesn't allow it. Life is not accidental sparks in universe's path to death, but **driving engine** of cosmic evolution.

## 8.3.4 Summary

At this point, we have completed Part IV "The Emergence of Observation." We constructed a physical picture containing observers, possessing agency, and forever evolving.

* **Observers** are self-referential subsystems in QCA networks.

* **Consciousness** is computational process minimizing free energy.

* **Evolution** is arms race of algorithmic complexity.

Now, all theoretical pieces are assembled. We have axioms, derived spacetime and matter, understood observation and evolution. In the final part of the book—**Part V: Verification and Inference**—we will no longer indulge in theory's elegance, but transform these radical views into cold numbers and curves, accepting final judgment of experimental physics.

# 9.1 Entanglement Gravity Detection Scheme in Microwave Cavities

This is the most direct and startling corollary of our theory: **Gravity originates not only from energy, but also from information (entanglement).**

In standard general relativity, gravitational source is energy-momentum tensor $T_{\mu\nu}$. If we have two systems with same total energy but different quantum states (e.g., one in thermal equilibrium, another in highly entangled state), standard theory predicts they produce identical gravitational fields.

But in our IGVP theory, gravity is geometric response of spacetime networks to maintain information transmission consistency. Entanglement directly corresponds to density of spacetime connections.

$$G_{\mu\nu} = 8\pi G (T_{\mu\nu}^{\text{energy}} + T_{\mu\nu}^{\text{entanglement}})$$

## 9.1.1 Experimental Design

**Apparatus**: Construct a high quality factor (High-Q) superconducting microwave cavity, placed in ultra-low temperature environment (shielding thermal noise).

**Control**: Use superconducting quantum interference devices (SQUID) as nonlinear elements to prepare two types of quantum states in cavity:

* **State A (Low Entanglement)**: Multi-mode coherent states, simulating classical electromagnetic fields.

* **State B (High Entanglement)**: Multi-mode squeezed vacuum states or cluster states.

* **Key Constraint**: Through precise quantum feedback control, ensure **total energy expectation value $\langle H \rangle$** of states A and B are **strictly equal**.

**Detection**: Use a frequency-stabilized laser beam passing through microwave cavity (or close to its surface) as detection arm, forming high-finesse Fabry-Perot interferometer.

**Signal**: Measure phase difference $\Delta \phi$ (Shapiro delay) of laser under backgrounds of state A and state B.

## 9.1.2 Theoretical Prediction

* **Standard Physics**: Since energy unchanged, $\Delta \phi_{GR} \approx 0$.

* **QCA Physics**: High entanglement state means higher information processing density $\rho_{\text{info}}$. According to optical metric formula $n \approx 1 + G\rho/c^4$, effective refractive index of state B region will be slightly larger than state A.

   $$\Delta \phi_{QCA} \propto G \cdot (S_B - S_A)$$

   We expect to observe a non-zero phase shift.

If this experiment succeeds, it will be the biggest turning point in physics since Eddington's solar eclipse observation—**it proves information is direct source of gravity.**

# 9.2 Lorentz Violation Effects in Ultra-High-Energy Cosmic Rays

Our theory is based on discrete lattice graph $\Lambda$. Although Lorentz symmetry is perfect emergent result at low energies, when approaching lattice scale (Planck scale $l_P$), this continuous symmetry must be broken by discrete structure.

## 9.2.1 Dispersion Relation of Light Speed

Wave packets propagating on lattices no longer have perfect dispersion relation $E = pc$, but have lattice correction terms:

$$E^2 \approx p^2 c^2 + \eta \frac{p^4}{M_P^2}$$

where $M_P$ is Planck mass, $\eta$ is coefficient depending on lattice geometry.

This means **ultra-high-energy photons (or neutrinos) will have velocity weakly dependent on their energy.**

* If $\eta < 0$, high-energy photons are slower than low-energy photons (subluminal).

* If $\eta > 0$, high-energy photons are faster than low-energy photons (superluminal).

## 9.2.2 Observational Opportunity

Universe provides us natural super accelerators—**Gamma-Ray Bursts (GRB)**.

When a GRB billions of light-years away bursts, it simultaneously emits photons observable across all wavelengths.

If QCA theory is correct, then after billions of years of flight, even extremely tiny velocity differences $\delta v \sim (E/M_P)^2$ will accumulate into observable time delays $\Delta t$.

## 9.2.3 Prediction

Instruments like Fermi Gamma-ray Space Telescope (Fermi LAT) should observe: **High-energy photons from same burst source arrive at Earth systematically earlier (or later) than low-energy photons.**

Although current observational data hasn't given conclusive evidence, there are already some puzzling "outlier photons" suggesting this possibility. As detection precision improves, this is not only a test of QCA, but direct measurement of spacetime discreteness.

# 9.3 Cosmological Drift of Fine Structure Constant

In Chapter 6, we argued that coupling constants (such as fine structure constant $\alpha \approx 1/137$) are determined by topological connectivity of QCA networks.

$$\alpha \sim \frac{\text{internal channels}}{\text{external channels}}$$

However, universe is expanding. From QCA perspective, expansion is not just lattice spacing stretching (that's metric effect), but may also accompany **evolution of network topological structure** (e.g., increase in lattice point number or reorganization of connection patterns).

If network connectivity in early universe was slightly different from now, then $\alpha$ should not be an absolute constant, but a scalar field slowly varying with cosmic time $t$.

## 9.3.1 Observational Scheme

Through observing spectra of distant quasars. This light comes from billions of years ago. We can measure atomic absorption lines (such as fine splitting of iron or magnesium), thereby deducing $\alpha$ value at that time.

## 9.3.2 Prediction

$$\frac{\Delta \alpha}{\alpha} = \frac{\alpha(z) - \alpha(0)}{\alpha(0)} \neq 0$$

We expect $\alpha$ to have extremely tiny cosmological drift (e.g., $10^{-6}$ order). High-precision spectroscopic data from James Webb Space Telescope (JWST) will be key to verifying this.

# 10.1 The Universe's Source Code: How Was Rule $\hat{U}$ Selected? (Criticality Hypothesis)

In Axiom $\Omega$, we assumed a global unitary evolution operator $\hat{U}$. But possible $\hat{U}$ (i.e., cellular automaton rules) are infinitely many. Why does our universe run this particular set of rules capable of producing quarks, stars, and DNA, rather than evolving into dead silence or chaos like most rules in Conway's Game of Life?

This is the computational version of **"Fine-tuning Problem"**. We don't need God to choose rules; we only need to understand **phase transitions of computational complexity**.

## 10.1.1 Classification of Rule Space: Wolfram Classes and Langton Parameter $\lambda$

Stephen Wolfram, through exhaustive study of one-dimensional elementary cellular automata, found all possible rules can be categorized into four behavioral patterns:

1. **Class I (Order/Death)**: Regardless of initial state, system rapidly evolves to single uniform state (e.g., all black). This corresponds to **crystals** or **absolute zero** in thermodynamics, with no information processing capability.

2. **Class II (Period/Oscillation)**: System evolves to simple, local periodic structures. Such rules can only store limited bits, unable to perform long-range communication.

3. **Class III (Chaos/Random)**: System evolves to disordered, seemingly random patterns (actually encrypted determinism). Although information content is huge (high entropy), structures have no correlations, unable to form stable "objects." This corresponds to **thermal equilibrium** or **white noise**.

4. **Class IV (Complex/Computation)**: This is the rarest class. System evolves complex, long-range correlated local structures (particles, gliders), which can move, collide, annihilate, or interact through logic gates in background. Such rules are proven to be **Turing Complete**.

Our physical universe clearly belongs to **Class IV**. Only such rules can support information storage (stability), transmission (photons), and processing (interactions), thereby supporting birth of life.

Christopher Langton introduced a parameter $\lambda$ (proportion of non-zero outputs in rule table) to quantify this classification. He found that as $\lambda$ increases from 0 to 1, system undergoes phase transition from "order" to "chaos." **Class IV rules are exactly at critical point of this phase transition**.

## 10.1.2 Criticality Hypothesis: Edge of Chaos

We propose a **Criticality Hypothesis** about cosmic origin:

> **Rules $\hat{U}$ capable of producing "physical universe" must be at second-order phase transition edge between ordered phase and chaotic phase.**

Near critical point, system's correlation length $\xi$ tends to infinity ($\xi \sim |T - T_c|^{-\nu}$). This means:

1. **Long-Range Forces**: Although underlying interactions are strictly local (only talking to neighbors), due to scale-free nature of critical state, information can effectively propagate to infinity. This explains why gravity and electromagnetic forces are long-range (massless bosons correspond to critical modes).

2. **Self-Similarity**: System exhibits fractal structures and power-law distributions. This explains cross-scale structural similarity in universe (from atoms to galaxy clusters).

3. **Maximum Complexity**: At this point, system's Shannon entropy is neither 0 (fully ordered) nor maximum (fully random), but at state capable of accommodating maximum **"Logical Depth"**.

## 10.1.3 Why Critical State?

Even without designers, **Self-Organized Criticality (SOC)** mechanisms will drive systems to automatically evolve to critical point. Like sandpile collapse models, if there exists some "meta-rule" allowing $\hat{U}$ to fine-tune over time (e.g., through vacuum decay or cosmic natural selection), then only those sub-universes evolving to critical state can produce observers.

Therefore, our universe is so exquisite not because it was carefully designed, but because it is a **survivor**—it is the only critical bubble capable of producing "meaning" through computation among countless ruins of death and chaos.

# 10.2 Physical Projection of Gödel's Incompleteness: The Boundary of Agnosticism

If the universe is essentially a mathematical structure (Axiom $\Omega$), does it inherit mathematics' deepest cracks?

In 1931, Kurt Gödel proved the First Incompleteness Theorem, shattering Hilbert's dream of axiomatizing all mathematical truth. The theorem states: **Any sufficiently powerful and self-consistent formal system contains propositions that can neither be proven nor disproven within its framework.**

For a long time, physicists thought this was just some pathology of pure logic, unrelated to the real world. After all, Newtonian mechanics and general relativity are based on calculus, not Peano arithmetic.

However, when we accept **QCA discrete ontology**, physical evolution becomes logical operations. Gödel's ghost immediately appears on physics' boundaries. This section will explore two physical projections of incompleteness theorem in computational universe: **Computational Irreducibility** (boundary of prediction) and **Chaitin Incompleteness** (boundary of cognition).

## 10.2.1 Death of Laplace's Demon: Computational Irreducibility

Classical determinism promised an illusion of omniscience—Laplace's Demon. If knowing all particles' positions and momenta at this moment, and all force laws, the demon could calculate state at any future time.

In QCA universe, although evolution rule $\hat{U}$ is strictly deterministic, Laplace's Demon dies from **computational cost**.

Stephen Wolfram proposed concept of **Computational Irreducibility**. For systems like our universe in "Class IV" (complex class), their evolution processes cannot be "shortcut" predicted through simple formulas.

* **Reducible Systems**: Like planetary orbits (two-body problem). We can directly substitute formula $x(t) = x_0 + v_0 t + \frac{1}{2}at^2$, instantly calculating position a million years later without simulating every second in between.

* **Irreducible Systems**: Like QCA universe. To determine system state after $t$ steps, the only way is **to let system (or its simulator) run step by step $t$ times**.

This leads to a profound physical corollary:

> **Prediction Paradox Theorem**:
>
> An observer inside a system cannot obtain complete information about system at time $t$ before time $t$.

**Proof**:

Assume observer builds a "prediction machine" to simulate universe.

1. For precise prediction, prediction machine must simulate every logic gate operation of universe.

2. To run faster than universe itself (predict ahead), prediction machine's computation speed must exceed universe's evolution speed.

3. According to Light Path Conservation Theorem, $v_{int} \le c$. Universe itself is already a computer running at maximum computational power.

4. Therefore, prediction machine cannot be faster than universe. **Fastest simulator is the universe itself.**

This means, although future is predetermined, it is **unknowable**. Future is not "derived," but "executed." Time passage is not an illusion, but necessary cost to decompress irreducible computation.

## 10.2.2 Chaitin Incompleteness: Ultimate Resolution Limit of Theory

Correspondence of Gödel's theorem in information theory is **Algorithmic Information Theory Incompleteness** proposed by Gregory Chaitin.

Chaitin defined **Kolmogorov Complexity** $K(x)$, length (bit number) of shortest program required to generate string $x$. He proved a despairing theorem:

> **Chaitin Incompleteness Theorem**:
>
> A formal system (formal theory) containing $N$ bits of information cannot prove complexity $K(x)$ of any string is much greater than $N$.

In physics, this means: **Complexity of our physical theories (axiom sets) limits upper bound of truth we can understand.**

* **Theory as Compression**: Physics' goal is finding simple laws (such as Axiom $\Omega$), with small complexity $K(\text{Theory})$, but capable of generating extremely complex phenomena $K(\text{Universe}) \gg K(\text{Theory})$.

* **Boundary of Understanding**: If some phenomena in universe (such as specific entanglement state inside a black hole, or evolution endgame of a chaotic system) have logical depth exceeding complexity of our theoretical system, then these phenomena are **random** to us.

This explains another source of randomness in quantum mechanics. Besides horizon truncation described by Born's rule, there is also **Algorithmic Randomness**. Some physical sequences (such as decimal digits of $\pi$, or pseudo-random output of QCA) are mathematically deterministic, but because no algorithm smaller than themselves can describe them, they are **equivalent to true randomness** to any finite observer.

## 10.2.3 Ultimate Task of Physics

Does this mean physics has reached its end? No.

Gödel and Chaitin delineated boundaries of "omniscience," but also pointed out essence of "understanding."

If universe's underlying rules $\hat{U}$ are simple (e.g., $K(\hat{U})$ only thousands of bits), then we have every hope of finding it. This is the so-called **Theory of Everything**.

But even if we find $\hat{U}$, we cannot predict infinitely rich results generated by it ($|\Psi(t)\rangle$).

We can only understand **mechanisms**, cannot exhaust **manifestations**.

## 10.2.4 Conclusion

Agnosticism is not an excuse for failure, but guarantee of cosmic rationality.

Precisely because Gödel incompleteness and computational irreducibility exist, universe is not just a rigid loop player. It allows emergence of **Novelty**. Even for God (if bound by logic), future is an unopened gift.

Complexity, life, and consciousness we see in universe are precisely flowers logic blooms at undecidable edges.

# 10.3 Are We Players or NPCs? — On the Status of Free Will in Deterministic QCA

Finally, we arrive at the most unsettling, most soul-touching question: In a QCA universe defined by Axiom $\Omega$, strictly deterministic and unitarily evolving, is there still room for **Free Will**?

If universe's state $|\Psi(t+1)\rangle$ is completely determined by $\hat{U} |\Psi(t)\rangle$, then initial state $|\Psi(0)\rangle$ at Big Bang moment seems to have already locked every detail of me writing this sentence and you reading it 13.8 billion years later. Does this mean we are not masters of our own fate, but non-player characters (NPCs) driven by ancient code, mechanically reciting lines in a movie with a pre-written script?

In this book's framework, we will provide a physics-based **Compatibilism** answer based on **Computational Irreducibility** and **Cybernetics**. We will prove: **Determinism not only doesn't exclude freedom, but is the foundation for free will to exist.**

## 10.3.1 Freedom as Unpredictability: Deconstructing Illusion of Fatalism

People's fear of "determinism" often stems from equating it with "fatalism." Fatalism believes: Whatever you do, outcome is predetermined. But in QCA universe, outcome precisely depends on **what you do**—i.e., your computational process.

**Computational Irreducibility** we discussed in Section 10.2 plays a decisive role here.

For Class IV complex systems (such as our universe or human brains), to predict system state at time $T$, **the only way is to run this system $T$ steps**. There is no God's-eye-view "script" or "lookup table" letting you skip process and directly see ending.

This means:

1. **Unpredictability**: No one—including yourself, even God with a supercomputer—can calculate what you will do faster than you before you make a decision. Because to simulate you, this computer must be at least as complex as you, and cannot run faster than $c$ (Light Path Conservation).

2. **Process Necessity**: Your "decision" doesn't exist at some preset point in future, but is **generated** in your current thinking process. Without your thinking (computation), there is no that decision.

In this sense, you are free. This freedom doesn't mean "ability to violate physical laws," but **"your behavior cannot be compressed into formulas simpler than yourself."** Your future is **computed**, not **predefined**.

**Corollary**: If an entity cannot be perfectly predicted by external observers (unless replicating it in real-time), then for all intents and purposes, it is an autonomous "player," not an NPC acting according to simple scripts.

## 10.3.2 Reverse Causality and Teleology: Will as Higher-Order Causal Force

In underlying QCA networks, causal arrows are strictly forward: $t \to t+1$. But at emergent macroscopic level, as we defined in Chapter 8, observers (agents) possess **internal models**.

When a system minimizes future **variational free energy** (prediction error) based on internal model, causality undergoes a marvelous cybernetic inversion:

* **Low-Level Matter** is subject to **Push**: Past states push it toward future (like billiard ball collision).

* **High-Level Consciousness** is subject to **Pull**: Future goals (expected states) pull it to adjust current behavior (like studying for tomorrow's exam).

Although this **Teleology** is still executed by underlying unitary evolution microscopically (just as software logic is ultimately executed by transistor switches), at macroscopic dynamics, it manifests as **Top-down Causation**.

**Definition 10.3.1 (Physical Definition of Will)**:

Free will is **self-correction capability** possessed by high $M_I$ (information mass) subsystems. When system detects current trajectory deviates from expectations of its internal model (i.e., free energy $F$ increases), it can call reserved negentropy ($v_{int}$), actively changing its microscopic state trajectory.

From this perspective, we are not passive NPCs. **We are subroutines in universe's great program that have obtained "self-modification permissions" (Self-modifying Code).** We not only run code, we also write code (through learning and memory reshaping neural synapses/network connections).

## 10.3.3 Source of Responsibility: You Are Your Algorithm

Finally, if choices are results of physical laws, do we still need to take responsibility for behavior?

Yes. Because in QCA ontology, **"you" are that specific algorithmic structure (topological knot)**.

* Even if physical laws determine you'll make a "bad" decision, that "bad" is inherent in your algorithmic structure.

* Punishment or reward is essentially environment's **feedback** to that algorithm, aimed at correcting algorithm's parameters (learning), or isolating it when algorithm cannot be corrected (elimination).

If world were random (as some quantum mechanics interpretations claim), your decisions are just results of dice-throwing, then you wouldn't need responsibility. Precisely because of determinism, your choices truly belong to **you** (sum of your history, memory, and logic).

## 10.3.4 Conclusion: Participatory Universe

Are we players or NPCs?

Answer depends on how you define "player." If you think players must stand outside game, unbound by game rules (physical laws), then we are indeed not players, and no such players exist.

But if you think players are **independent computational entities within game capable of sensing environment, building models, setting goals, and actually changing game progress**, then we are not only players, we are **advanced players**.

Universe sets board and rules through Axiom $\Omega$, but it doesn't set game's direction.

This game is played step by step by us—countless entangled observers.

At this point, we have completed exploration of ultimate questions of computational universe. Physics hasn't deprived us of dignity; on the contrary, it endows us with noble status as **cosmic computational collaborators**.

