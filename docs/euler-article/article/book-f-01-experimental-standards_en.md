# Volume F-1 | Experimental Standards (Foundation): Five Quality Inspection Tools

> This is an English translation of [中文原文](book-f-01-experimental-standards.md)

## Opening: Why Do We Need "Quality Inspection"?

You made a decision, executed a plan, launched a product.

How do you know it's "reliable"?

Most people's approach is "feeling": feels good, then continue; feels wrong, then adjust.

But "feeling" is unreliable. You need "quality inspection standards": a set of reusable inspection methods.

This volume gives you five foundational quality inspection tools.

---

## Tool One | Frequency Threshold Manual: Find Appropriate Checking Frequency

### Problem: Why Are Some Teams "Over-Checking," Others "Losing Control"?

Have you seen teams like this:
- Three meetings daily, everyone "reporting progress," but no one "doing things."
- Check only once a month, when problems discovered, already too late to fix.

Why? Because checking frequency is wrong.

### Tool's Core: Checking Frequency = 2× Change Frequency

A simple principle: **Your checking frequency must at least reach "twice the change frequency"**.

Why twice?
- If you check too slowly, you can't "see clearly" the real situation, "aliasing" appears (mixing outdated information with new situation).
- Twice is a safety boundary: can discover changes promptly, without over-checking.

### How to Use

**Step One: Estimate Change Frequency**
- How often does this matter have obvious changes?
- For example, product development phase might have progress daily; operation phase might have obvious changes weekly; strategic planning might need adjustment monthly.

**Step Two: Set Checking Frequency**
- If change frequency is "daily," checking frequency should be at least "twice daily" (like morning and evening stand-ups).
- If change frequency is "weekly," checking frequency should be at least "twice weekly" (like Monday set goals, Friday review).

**Step Three: Set Tolerance**
- If checking discovers "folding energy" (information chaos, outdated information mixed with new situation) exceeds a certain threshold, checking frequency is too low, must increase.

### Example: A Software Development Project

- **Development sprint period** (changes very fast): daily stand-up, sync progress and obstacles.
- **Testing optimization period** (medium-speed changes): sync twice weekly, check bug fix progress.
- **Maintenance period** (changes very slow): monthly meeting, discuss whether major version update needed.

### Key Indicators

- Team members feel "sync frequency just right" (not too frequent wasting time, not too sparse losing control).
- Information chaos indicator drops (outdated information and new situation can be clearly distinguished).

---

## Tool Two | Spectral Gap Scale: Identify "Authority"

### Problem: How to Judge If a Principle "Is Worth Upholding"?

You have many "principles": integrity, efficiency, innovation, stability...

But when facing conflicts of interest, you'll find: hard to "have it all." At this time, which principles are "non-negotiable"? Which can be "flexibly adjusted"?

### Tool's Core: What's Been Time-Tested Is True Authority

True "authority" principles have three characteristics:
1. **Repeatedly used**: In various situations, you've used it.
2. **Resists disturbance**: Facing temptation and pressure, you still upheld it.
3. **Retrospective verification**: Looking back, upholding it proved right.

This is the meaning of "spectral gap": truly stable principles and "occasionally popping up ideas" have a clear dividing line.

### How to Use

**Step One: List Your Principles**
Write down five to ten principles you consider important.

**Step Two: Test Stability**
Over the past year, which principles have you consistently upheld? Which have you often compromised?

**Step Three: Calculate "Sacredness Degree"**
For those principles you've consistently upheld, calculate "sacredness degree":
- Sacredness degree = Upholding stability ÷ Environmental disturbance intensity
- If under strong disturbance (like huge temptation, extreme pressure), you still uphold, sacredness degree is high, this is a core principle.
- If under small disturbance you compromise, sacredness degree is low, this might just be a "slogan," not a true principle.

### Example: Evaluating "User First" Principle

- **Repeatedly used**: Over the past year, for all major decisions, I asked "is this beneficial to users."
- **Resists disturbance**: Once, investor demanded adding ads, would affect user experience, I refused.
- **Retrospective verification**: That decision short-term lost revenue, but long-term retention improved, proved right.

Conclusion: This is a high-sacredness core principle, should continue upholding.

### Key Indicators

- Number of core principles (usually 3-5, not too many).
- After upholding core principles, team's long-term stability improves.

---

## Tool Three | KL Error Correction Process: Gently Correct Deviations

### Problem: Why Does "Harsh Punishment" Often Backfire?

Some managers, once discovering employee mistakes, "harshly punish": dock pay, demote, even fire.

Short-term, seems to "deter" others. But long-term, team becomes:
- Afraid to take risks (fear making mistakes).
- Hide problems (fear punishment).
- Lack innovation (only do safe things).

### Tool's Core: Use Minimal Cost to Get People Back on Track

Truly effective error correction is not "harsh punishment," but "small-step correction":
- Give mistake-makers opportunity to improve.
- Use minimal adjustment to get them back on track.
- Establish "make mistake—correct—grow" positive cycle.

### How to Use

**Step One: Diagnose Error Type**
This error is:
- Capability problem (can't do)? → Need training.
- Attitude problem (unwilling to do)? → Need communication about motivation.
- System problem (process has loopholes)? → Need to fix process.

**Step Two: Design Minimal Adjustment**
- If capability problem, arrange training or mentor guidance.
- If attitude problem, one-on-one deep conversation, understand their confusion and concerns.
- If system problem, fix process, avoid others making same mistake.

**Step Three: Observe Improvement**
Give an observation period (like one month), see if there's improvement:
- If improvement, continue supporting.
- If no improvement, reassess whether stronger intervention needed (like changing position).

### Example: Employee Frequently Late

- **Diagnosis**: Not attitude problem (they work very seriously), but system problem (their home is far from company, morning rush hour traffic serious).
- **Minimal Adjustment**: Allow remote work part-time, or adjust work hours (arrive one hour later, leave one hour later).
- **Observation**: After adjustment, they're no longer late, work efficiency also improved.

### Key Indicators

- Improvement rate after mistakes (not "zero mistake rate").
- Team's psychological safety (dare to admit mistakes, raise problems).
- Long-term innovation capability (not conservative due to "fear of mistakes").

---

## Tool Four | NPE Error Log: Structured Error Management

### Problem: Why Do Same Errors Repeatedly Appear?

Have you felt this:
- Each retrospective can find problems.
- But next time, same problems still appear.

Why? Because you only "found problems," didn't "record and classify."

### Tool's Core: Classify Errors into Three Types, Find Patterns

Most errors can be classified into three types:
1. **Aliasing**: Mixing information from different sources, times, natures together.
2. **Insufficient correction**: Method too crude, missing key corrections.
3. **Tail neglect**: Only considering average cases, ignoring extreme case impact.

If you can record which category each error belongs to, you can find "high-frequency error sources," then improve specifically.

### How to Use

**Step One: Establish Error Ledger**
After each project ends, record three types of errors:
- Where did we confuse different information?
- Where were our methods too simple?
- Which extreme cases did we not consider?

**Step Two: Quarterly Summary**
Every three months, compile these three types of errors, find "high-frequency items":
- If "aliasing" most, information management has problems, need to establish "information classification" mechanism.
- If "insufficient correction" most, method too crude, need to introduce more refined tools.
- If "tail neglect" most, risk awareness insufficient, need to establish "extreme case drills."

**Step Three: Targeted Improvement**
In next quarter's processes, add preventive measures for "high-frequency errors."

### Example: A Project Management Team

- **First Quarter**: Discover "aliasing" most (conflating client's "wants" with "needs").
- **Improvement Measure**: Establish "requirement classification table," distinguish "what client clearly said," "what client hinted," "what we speculated."
- **Second Quarter**: Aliasing errors drop 50%, but "insufficient correction" starts increasing (time estimation too optimistic).
- **Improvement Measure**: Introduce "three-point estimation method" (optimistic, pessimistic, most likely), and add collaboration cost.

### Key Indicators

- Repeated error rate drops.
- Team has clear cognition of "error-prone links."
- Process improvement has data support (not "snap judgment").

---

## Tool Five | Reproduction and Pre-Registration Checklist: Ensure Conclusions Reliable

### Problem: Why Can't Some "Success Experiences" Be Replicated?

Have you seen situations like this:
- A certain team did a project, very successful.
- Other teams learned their method, but failed.

Why? Because "success" might just be luck, not method.

### Tool's Core: What's Reproducible Is Reliable

Truly reliable methods must satisfy:
1. **Pre-registration**: Before starting, clarify "what method to use," "how to measure success."
2. **Same Window Same Scale**: When repeating experiment, use same data sources, same measurement standards.
3. **Multiple Verification**: Not "one success counts," but "multiple repetitions, all stable success."

### How to Use

**Step One: Pre-Registration**
Before starting, write down:
- Data sources (window): What data will we use?
- Measurement standard (scale): What's called success? What's called failure?
- Analysis method: What method will we use to analyze?

This pre-registration, make public to all team members, can't modify afterwards.

**Step Two: Execute and Record**
Strictly execute according to pre-registration, record all key data.

**Step Three: Reproduction Verification**
At different times, different teams, different scenarios, use same method, see if can get similar results.

### Example: Verify Whether "Daily Stand-Up" Is Effective

- **Pre-Registration**: We will implement "daily stand-up" in Team A, lasting one month. Measurement standard is "project delay frequency" and "team satisfaction."
- **Execute**: Team A holds 15-minute stand-up every morning at 9am, records daily progress and obstacles.
- **Verification**: After one month, project delay frequency dropped 30%, team satisfaction improved.
- **Reproduction**: Also implement in Teams B and C, two months later, also saw similar effects.

Conclusion: This method is reproducible, can be promoted.

### Key Indicators

- Reproduction success rate (in different scenarios, method still effective).
- Pre-registration compliance (team strictly executes according to pre-registration, not "Monday morning quarterback").

---

## Conclusion: From Quality Inspection to Improvement

These five quality inspection tools help you ensure "decision and execution" quality:

1. **Frequency Threshold Manual**: Find appropriate checking frequency, not excessive, not out of control.
2. **Spectral Gap Scale**: Identify true core principles, distinguish "authority" from "slogans."
3. **KL Error Correction Process**: Use minimal cost to correct deviations, establish positive cycle.
4. **NPE Error Log**: Structured error management, find high-frequency error sources, improve specifically.
5. **Reproduction and Pre-Registration Checklist**: Ensure method reproducible, avoid "accidental success."

These tools are not for "nitpicking," but for "continuous improvement."

With quality inspection standards, you don't have to rely on "feeling," but can use data and logic to make more reliable judgments.

And this is the key step from "snap judgment" to "professionalization."
