\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{geometry}
\usepackage{hyperref}
\usepackage{braket}
\usepackage{graphicx}

\geometry{left=2.5cm,right=2.5cm,top=2.5cm,bottom=2.5cm}

\title{The Red Queen Universe: The Cosmological Constant from Agent Games and the Avoidance of Heat Death}
\author{Haobo Ma$^1$ \and Wenlin Zhang$^2$\\
\small $^1$Independent Researcher\\
\small $^2$National University of Singapore}
\date{}

\begin{document}

\maketitle

\begin{abstract}
The classical picture of the heat death of the universe describes the ultimate fate of the cosmos as a state where, over long time scales, all available free energy is exhausted, the system tends toward thermal equilibrium and maximum entropy, and all macroscopic structures and computational activities cease. However, the observational fact that the universe has continuously generated and maintained multi-level low-entropy structures---especially life and intelligent civilizations---over approximately 13.8 billion years of evolution remains opaque under the standard explanation relying solely on "low-entropy initial conditions." On the other hand, the cosmological constant and dark energy problems suggest that the value of the vacuum energy density and its coincidence with matter density still lack a microscopic information-theoretic explanation.

In this paper, within the framework of a Quantum Cellular Automaton (QCA) universe and the conservation of information rate, we formalize the cosmic system as a multi-agent game field composed of "agents." Each agent is characterized as maintaining a low-entropy structure far from equilibrium in a local Hilbert subspace and minimizing variational free energy through internal computation, thereby continuously performing information erasure and prediction error correction. Based on Landauer's principle and established results in the thermodynamics of computation, we prove that under appropriate coarse-graining and isotropy assumptions, the Landauer waste heat generated by the irreversible computations of all agents in the universe is equivalent to a homogeneous, isotropic energy component with an approximate equation of state $w \simeq -1$. Its gravitational effect in Friedmann dynamics is equivalent to a dynamical cosmological "constant" $\Lambda_{\mathrm{eff}}(t)$.

Furthermore, we introduce a multi-agent dynamic model based on the Red Queen effect, expressing the fitness competition and complexity arms race among agents as a system coupling replicator equations with complexity variables and Lotka--Volterra dynamics. Linearization and spectral analysis of such systems yield a general theorem: within a broad parameter range satisfying "Red Queen conditions," internal equilibrium points are not asymptotically stable. Instead, the system tends toward limit cycles or chaotic attractors in phase space, causing complexity and Landauer information erasure rates to remain positive on cosmic time scales. Thus, the classical "static limit equilibrium state" is dynamically excluded.

Building on this, the paper presents three main conclusions. First, under the constraints of a QCA universe and information rate conservation $v_{\mathrm{ext}}^2+v_{\mathrm{int}}^2=c^2$, the information erasure flow corresponding to internal agent computation constitutes a natural class of "information vacuum energy density" $\rho_{\mathrm{info}}(t)$, whose integral determines the effective cosmological constant $\Lambda_{\mathrm{eff}}(t)$, providing an information-theoretic origin for dark energy. Second, the Red Queen agent game establishes a feedback loop between "complexity and dark energy": richer computational activities generate larger $\rho_{\mathrm{info}}$, which in turn alters large-scale spacetime dynamics and conversely affects the survival environment of agents, forming a "Red Queen Universe" evolutionary mode. Third, as long as a non-zero density agent network exists in the universe and satisfies Red Queen conditions, in the continuous limit of General Relativity, the state required for heat death---"global absence of available free energy, complete entropization of structure and computation"---is no longer a dynamical attractor. The universe can evolve in a perpetually far-from-equilibrium "algorithmic turbulence" state.

In the application section, we discuss the alleviation of the cosmological "coincidence problem" by complexity-driven $\Lambda_{\mathrm{eff}}(t)$, relationships with entropic cosmology and the Causal Entropic Principle, and potential testable predictions in the context of dark energy evolution observations (such as recent DESI indications of time-varying dark energy). Finally, engineering proposals based on QCA quantum simulation and multi-agent simulation are presented to provide pathways for testing the "Red Queen Universe" framework on controllable platforms.
\end{abstract}

\textbf{Keywords:} Quantum Cellular Automata; Cosmological Constant; Dark Energy; Heat Death; Landauer's Principle; Variational Free Energy; Red Queen Effect; Multi-Agent Games; Thermodynamics of Computation; Information Cosmology

\section{Introduction \& Historical Context}

\subsection{Cosmic Heat Death and the Dark Energy Problem}

Since the nineteenth century, the "Heat Death of the Universe" picture, based on the Second Law of Thermodynamics, has held that if the universe is an effectively closed system, its total entropy increases monotonically with time, eventually tending toward a maximum entropy equilibrium state with no available free energy, known as the "Big Freeze" or "Heat Death." In the framework of modern cosmology, if the universe is flat or open on large scales and contains a positive cosmological constant, the standard inference is that the universe will asymptotically approach an approximate de Sitter equilibrium state after infinite time, where energy differences in all local structures and processes will be smoothed out.

Observations based on Type Ia supernovae, the Cosmic Microwave Background, and large-scale structure at the end of the twentieth century indicated that the universe is currently in a phase of accelerated expansion. This phenomenon is usually described by a dark energy component with an approximate equation of state $w \simeq -1$, typically realized in Einstein's equations as a cosmological constant $\Lambda$. Dark energy density accounts for about 70\% of the current cosmic energy budget, while matter accounts for about 30\%, forming the so-called $\Lambda$CDM standard model.

However, the cosmological constant introduces two classic puzzles. The first is the "Cosmological Constant Problem": zero-point energy estimates from quantum field theory are tens of orders of magnitude larger than the observed value, and there is no consensus on how to screen or reconcile this huge vacuum energy contribution. The second is the "Coincidence Problem": why the dark energy density and matter density happen to be of the same order of magnitude in the current cosmic epoch, whereas they were not for most of cosmic history.

Recent observations of large samples of galaxies and quasars by projects like DESI have even given indications that dark energy may evolve with time, further stimulating theoretical exploration into dynamical dark energy and the origins of a non-trivial cosmological constant.

\subsection{Information, Computation, and Thermodynamics: Landauer's Principle and Computational Engines}

The deep connection between information theory and thermodynamics was first systematically elucidated by Landauer. Landauer's Principle states that the minimum energy cost required to erase one bit of classical information in an environment at temperature $T$ is $E_{\min}=k_{\mathrm{B}}T\ln 2$. Any logically irreversible operation (such as erasure or merging computational paths) is necessarily accompanied by entropy production and heat dissipation of at least this magnitude. Bennett subsequently developed the thermodynamics of computation, viewing computers as thermal engines that convert free energy into waste heat and "mathematical work," and proved that reversible computation can operate arbitrarily close to the Landauer limit. Subsequent work extended Landauer's Principle to quantum and non-equilibrium systems, confirming its universality as a physical lower bound for information processing.

These results suggest that any system performing irreversible computation---including artificial computers, biological organisms, and even broader "natural computation" processes---must necessarily emit a minimal amount of heat to the environment and produce a corresponding entropy increase. This concept provides a foundation for understanding macroscopic cosmic thermodynamics from an information-theoretic perspective.

\subsection{Variational Free Energy and Agents: Inspiration from the Free Energy Principle}

In cognitive science and neuroscience, the Free Energy Principle proposed by Friston states that all biological systems maintaining their boundaries and homeostasis can be viewed as inference machines that minimize variational free energy in some sense. The core is to introduce a joint probability model $P(s,\vartheta)$ over sensory inputs $s$ and hidden variables $\vartheta$, as well as an internal approximate posterior $q(\vartheta)$. The system minimizes a variational functional by changing its internal state:
\begin{equation}
F(q,s)=\mathrm{KL}\bigl(q(\vartheta) \mid P(\vartheta\mid s)\bigr)-\ln P(s),
\end{equation}
which can equivalently be written in forms containing model evidence and entropy terms.

In this framework, organisms reduce prediction error and "surprise" by updating internal models and taking actions, thereby maintaining ordered structures far from equilibrium. Although initially applied to neural systems, the form of the Free Energy Principle does not depend on specific material substrates, so it can be abstracted to the more general concept of an "agent."

\subsection{The Red Queen Effect and Evolutionary Arms Races}

The Red Queen hypothesis was proposed by Van Valen to explain the "Van Valen Law" in the paleontological record, where species extinction rates are approximately independent of species age. The hypothesis emphasizes that the "effective environment" of a species is mainly composed of other coexisting species, so fitness improvement is relative: when one species gains an advantage, it deteriorates the ecological niche of other species, forcing the latter to evolve to maintain their chances of survival.

The Red Queen effect can be generalized to sexual selection and host-parasite interactions at the individual level, and can also be viewed as a zero-sum game: all species need to "run as fast as they can" just to stay in place. Such game dynamics are ubiquitous in multi-agent systems, and research in Agent-based models shows that local interactions can often produce complex emergent phenomena on macroscopic scales.

This paper elevates this idea to the cosmological scale: viewing the universe as a game network composed of multi-level agents (from molecular machines to life and technological civilizations), where the Red Queen effect maintains a non-equilibrium state over cosmic time scales through a "complexity arms race."

\subsection{QCA Universe and Conservation of Information Rate}

Quantum Cellular Automata provide a natural framework for strict quantum dynamics on discrete spacetime. A QCA can be viewed as an array of finite-dimensional quantum systems defined on a lattice, whose evolution is given by translation-invariant, causal unitary operators iterated over discrete time steps. A large body of work has shown that field equations such as Dirac, Weyl, and Maxwell can emerge from QCA models in appropriate continuous limits, providing a rigorous path for "spacetime and field theory originating from quantum computation."

In previous work, the external group velocity $v_{\mathrm{ext}}$ of local excitations and the internal phase rotation or "intrinsic evolution velocity" $v_{\mathrm{int}}$ were combined into an information rate vector $\mathbf{u}=(v_{\mathrm{ext}},v_{\mathrm{int}})$, proposing the information rate conservation relation:
\begin{equation}
v_{\mathrm{ext}}^{2}+v_{\mathrm{int}}^{2}=c^{2},
\end{equation}
where $c$ is the maximum propagation speed of the QCA, corresponding to the speed of light in the continuous limit. This relation unifies the normalization of four-velocity in special relativity, proper time, and the mass-frequency relation $mc^{2}=\hbar\omega_{\mathrm{int}}$ into a geometric constraint of information rate budget, providing the basis for constructing the link between "internal computation" and "external geometry" in this paper.

\subsection{Objectives and Main Contributions}

Synthesizing the above background, this paper addresses three interconnected questions:

1. If the universe is a QCA universe containing a vast number of agents maintaining their structures through computation, how does the thermodynamic necessity of these computations feed back into large-scale spacetime geometry and the cosmological constant?

2. Can Red Queen-style multi-agent games dynamically prevent the universe from entering a final state of complete equilibrium heat death?

3. Is this "Red Queen Universe" mechanism intrinsically linked to the value of dark energy and its possible time evolution, and is it compatible with existing observations?

Around these questions, the main work of this paper is organized as follows:

\begin{itemize}
  \item In "Model \& Assumptions," we formalize the QCA universe and agents, introduce the information vacuum energy density $\rho_{\mathrm{info}}(t)$ based on Landauer's principle, and give its relation to the cosmological constant $\Lambda_{\mathrm{eff}}(t)$.
  \item In "Main Results," we present two core theorems: first, the Landauer-$\Lambda$ relation theorem, proving that under isotropy and Red Queen continuous computation assumptions, the irreversible computation of agents is equivalent to a dynamical cosmological constant; second, the Red Queen non-equilibrium theorem, which, under a multi-agent replicator-complexity dynamic model, excludes internal stable equilibrium points and guarantees that complexity and information erasure rates remain positive over the long term.
  \item In "Proofs" and the appendices, we provide rigorous derivations of the above theorems, including the continuity equation with source terms derived from Einstein field equations and energy-momentum conservation, and linear stability analysis of the Lotka--Volterra--replicator system.
  \item In "Model Apply," we construct a parameterization of complexity-driven $\Lambda_{\mathrm{eff}}(t)$, compare it with entropic cosmology and the Causal Entropic Principle, and discuss potential connections with observations of time-varying dark energy.
  \item In "Engineering Proposals," we propose specific schemes for testing this framework on quantum simulation platforms and in multi-agent simulations.
\end{itemize}

\section{Model \& Assumptions}

This section presents the basic structure and assumptions of the "Red Queen Universe" model.

\subsection{QCA Universe and Macroscopic Geometry}

Assume that at the fundamental level, the universe is described by a tensor product space $\mathcal{H}=\bigotimes_{x\in\Lambda}\mathcal{H}_{x}$ defined by a countable lattice set $\Lambda$ and local Hilbert spaces $\mathcal{H}_{x}\simeq\mathbb{C}^{d}$. Global evolution is given by a translation-invariant, local, and causal unitary operator $U:\mathcal{H}\to\mathcal{H}$ over discrete time steps $n\in\mathbb{Z}$, which is the standard definition of a QCA.

In the long-wavelength and low-energy limit, where lattice spacing and time step tend to zero, QCA evolution approximates continuous relativistic field equations, macroscopically describable by a metric with Friedmann--Lema√Ætre--Robertson--Walker (FLRW) symmetry:
\begin{equation}
\mathrm{d}s^{2}=-c^{2}\mathrm{d}t^{2}+a^{2}(t)\gamma_{ij}\mathrm{d}x^{i}\mathrm{d}x^{j},
\end{equation}
where $a(t)$ is the scale factor, and $\gamma_{ij}$ is the constant curvature metric of three-dimensional space. The cosmological constant or dark energy component manifests in this continuous limit as a term in the energy-momentum tensor $T_{\mu\nu}^{(\Lambda)}=-\rho_{\Lambda}g_{\mu\nu}$, corresponding to an equation of state $p_{\Lambda}=-\rho_{\Lambda}$.

\subsection{Information Rate Conservation and Definition of Agents}

For every excitation or structure localized in the QCA universe, consider its effective worldline $\gamma$ and the external group velocity $v_{\mathrm{ext}}$ and internal evolution velocity $v_{\mathrm{int}}$ along that worldline, satisfying the information rate conservation relation:
\begin{equation}
v_{\mathrm{ext}}^{2}+v_{\mathrm{int}}^{2}=c^{2}.
\end{equation}
Here, $v_{\mathrm{ext}}$ reflects the propagation speed of the excitation on the lattice, while $v_{\mathrm{int}}$ characterizes the phase evolution and internal computation rate of its internal state in the local Hilbert subspace.

\textbf{Definition:}
\begin{itemize}
  \item An \textbf{Agent} $\mathcal{A}$ is a finite connected lattice subset $\Lambda_{\mathcal{A}}\subset\Lambda$ on the QCA and a family of density operators ${\rho_{\mathcal{A}}(t)}_{t}$ on $\mathcal{H}_{\mathcal{A}}=\bigotimes_{x\in\Lambda_{\mathcal{A}}}\mathcal{H}_{x}$, satisfying:
  \begin{enumerate}
    \item $\rho_{\mathcal{A}}(t)$ maintains a significant deviation from the environmental equilibrium state $\rho_{\mathrm{eq}}(t)$ over long time scales, i.e., there exists a macroscopic observable $O$ such that $\lvert\mathrm{tr}[(\rho_{\mathcal{A}}-\rho_{\mathrm{eq}})O]\rvert$ has positive measure on a time set larger than some fixed threshold.
    \item This deviation is maintained by internal computation, i.e., the evolution of $\rho_{\mathcal{A}}(t)$ can be decomposed into approximately reversible internal unitary operators and irreversible "information erasure" maps, where the latter thermodynamically satisfies the Landauer bound.
  \end{enumerate}
  \item The \textbf{Information Mass} $M_{I}$ of an agent is defined such that its internal evolution frequency $\omega_{\mathrm{int}}$ and effective energy $E_{I}$ satisfy the relation:
  \begin{equation}
  E_{I}=M_{I}c^{2}=\hbar\omega_{\mathrm{int}},
  \end{equation}
  linking the internal computation rate to the relativistic mass scale.
\end{itemize}

\subsection{Variational Free Energy and Agent Objective Function}

Referencing the Free Energy Principle, treat each agent as an inference machine possessing a generative model $P(s,\vartheta)$ of environmental signals $s$ and internal states $\vartheta$. Define its variational free energy:
\begin{equation}
F_{\mathcal{A}}(t)=\mathrm{KL}\bigl(q_{t}(\vartheta) \mid P(\vartheta\mid s_{t})\bigr)-\ln P(s_{t}),
\end{equation}
where $q_{t}(\vartheta)$ is the approximate posterior distribution of hidden variables held by the agent at time $t$, and $s_{t}$ is its sensory input at that time. The "survival strategy" of an agent can be abstracted as minimizing the path integral or long-time average of $F_{\mathcal{A}}(t)$ under the information rate budget constraint:
\begin{equation}
v_{\mathrm{ext}}^{2}(t)+v_{\mathrm{int}}^{2}(t)=c^{2}.
\end{equation}
This optimization process necessarily involves compressing the internal state space and filtering out unnecessary high-dimensional components, thus physically corresponding to frequent irreversible writing and erasure of internal storage and representations.

\subsection{Landauer Information Waste Heat and Information Vacuum Energy Density}

For a single agent $\mathcal{A}$, assume its information erasure rate in an ambient temperature field $T_{\mathrm{bg}}(x,t)$ is $\dot{I}_{\mathrm{erase}}^{(\mathcal{A})}(t)$ (in bits/s). Landauer's Principle requires its minimum dissipation power to satisfy:
\begin{equation}
P_{\mathrm{L}}^{(\mathcal{A})}(t)\geq k_{\mathrm{B}}\ln 2\,T_{\mathrm{bg}}^{(\mathcal{A})}(t)\,\dot{I}_{\mathrm{erase}}^{(\mathcal{A})}(t),
\end{equation}
where $T_{\mathrm{bg}}^{(\mathcal{A})}$ is the effective environmental temperature of the agent.

On cosmological scales, coarse-graining over a comoving volume $V$ and summing the information erasure contributions of all agents $\mathcal{A}_{i}$ located within that volume yields the average Landauer power density per unit comoving volume:
\begin{equation}
P_{\mathrm{info}}(t)=\frac{1}{V}\sum_{i}k_{\mathrm{B}}\ln 2\,T_{\mathrm{bg}}^{(i)}(t)\,\dot{I}_{\mathrm{erase}}^{(i)}(t).
\end{equation}

One of the core assumptions of this paper is:

\textbf{Assumption 1 (Information Waste Heat--Vacuum Energy Integrability):} In the QCA universe, the Landauer waste heat produced by the irreversible computations of agents is microscopically encoded into a homogeneous, isotropic "information vacuum" excitation in the underlying QCA degrees of freedom. Its average energy-momentum tensor in the continuous limit has the approximate form:
\begin{equation}
T_{\mu\nu}^{(\mathrm{info})}(t)\simeq -\rho_{\mathrm{info}}(t)\,g_{\mu\nu},
\end{equation}
where the information vacuum energy density $\rho_{\mathrm{info}}(t)$ satisfies the sourced continuity equation:
\begin{equation}
\dot{\rho}_{\mathrm{info}}+3H(1+w_{\mathrm{info}})\rho_{\mathrm{info}}=P_{\mathrm{info}}(t),
\end{equation}
and satisfies $w_{\mathrm{info}}(t)\simeq -1$ in the Red Queen era.

This assumption essentially reinterprets Landauer waste heat from "conventional thermal radiation" to an energy reserve stored in the underlying QCA degrees of freedom that macroscopically exhibits negative pressure, making its gravitational effect equivalent to dark energy.

Under this assumption, an effective cosmological "constant" can be defined:
\begin{equation}
\Lambda_{\mathrm{eff}}(t)=\Lambda_{\mathrm{bare}}+8\pi G\,\rho_{\mathrm{info}}(t),
\end{equation}
where $\Lambda_{\mathrm{bare}}$ is a possible underlying constant part (e.g., vacuum energy remaining after renormalization), and $\rho_{\mathrm{info}}(t)$ is the dynamical part emerging from agent computation.

\subsection{Red Queen Conditions and Multi-Agent Games}

To characterize competition and arms races between agents, we introduce the following abstractions:

\begin{itemize}
  \item Assume there are several species of agents in the universe, denoted by $i=1,\dots,n$, with comoving number density $N_{i}(t)$ and average complexity $C_{i}(t)$ (understood as algorithmic complexity, structural information, or average dimension of internal state space).
  \item Define the fitness of species $i$ as:
  \begin{equation}
  W_{i}(t)=f_{i}\bigl(\mathbf{N}(t),\mathbf{C}(t)\bigr),
  \end{equation}
  where $\mathbf{N}=(N_{1},\dots,N_{n})$ and $\mathbf{C}=(C_{1},\dots,C_{n})$.
  \item Complexity $C_{i}$ enhances the ability of that class of agents to capture and utilize free energy on the one hand, but increases Landauer costs on the other.
\end{itemize}

The \textbf{Red Queen Conditions} can be formalized as:

1. For any $i\neq j$, with other variables fixed, $\partial f_{j}/\partial C_{i}<0$: an increase in the complexity of one class of agents deteriorates the fitness of other classes.
2. For each $i$, within the resource-abundant interval, $\partial f_{i}/\partial C_{i}>0$: before costs outweigh benefits, increasing one's own complexity increases fitness.

Multi-agent systems satisfying the above conditions typically exhibit evolutionary arms races, whose dynamics can be described by replicator equations or Lotka--Volterra type equations.

\section{Main Results (Theorems and alignments)}

This section presents the two core theorems of this paper and several corollaries.

\subsection{Theorem 1 (Landauer--$\Lambda$ Relation Theorem)}

In the continuous limit of a QCA universe, assume:

1. The large-scale geometry is an isotropic, homogeneous FLRW spacetime satisfying standard Friedmann equations.
2. There exists a game network composed of agents in the universe, with an average Landauer power density $P_{\mathrm{info}}(t)$ over comoving volume elements.
3. Information waste heat satisfies Assumption 1, i.e., its macroscopic gravitational effect can be described by a fluid with equation of state $p_{\mathrm{info}}=w_{\mathrm{info}}\rho_{\mathrm{info}}$, and $w_{\mathrm{info}}\simeq -1$ in the Red Queen era.

Then, the information vacuum energy density $\rho_{\mathrm{info}}(t)$ and Landauer power density $P_{\mathrm{info}}(t)$ satisfy the integral relation:
\begin{equation}
\rho_{\mathrm{info}}(t)=\rho_{\mathrm{info}}(t_{0})+\int_{t_{0}}^{t}P_{\mathrm{info}}(\tau)\,\mathrm{d}\tau+\mathcal{O}(\epsilon),
\end{equation}
where $\epsilon$ characterizes the deviation of $w_{\mathrm{info}}+1$. Correspondingly, the effective cosmological constant is:
\begin{equation}
\Lambda_{\mathrm{eff}}(t)=\Lambda_{\mathrm{eff}}(t_{0})+8\pi G\int_{t_{0}}^{t}P_{\mathrm{info}}(\tau)\,\mathrm{d}\tau+\mathcal{O}(\epsilon).
\end{equation}
In other words, under the good approximation $w_{\mathrm{info}}\approx -1$, the dynamical part of the cosmological constant is equivalent to the accumulation of Landauer energy flux generated by the irreversible computations of all agents in the universe over cosmic time.

\subsection{Corollary 1 (Complexity--Dark Energy Coupling)}

If we further assume:

1. There exists a macroscopic complexity function:
   \begin{equation}
   C(t)=\sum_{i}N_{i}(t)\,C_{i}(t),
   \end{equation}
   characterizing the total complexity of agents per unit comoving volume.
2. The average information erasure rate per unit complexity and the environmental temperature are approximately constant, i.e., there exists a constant $\alpha>0$ such that:
   \begin{equation}
   P_{\mathrm{info}}(t)\simeq \alpha\,T_{\mathrm{bg}}(t)\,\dot{C}(t),
   \end{equation}
   where $T_{\mathrm{bg}}(t)$ is the macroscopic average of the cosmic background temperature field.

Then we have:
\begin{equation}
\Lambda_{\mathrm{eff}}(t)\simeq \Lambda_{\mathrm{eff}}(t_{0})+8\pi G\alpha\int_{C(t_{0})}^{C(t)}T_{\mathrm{bg}}(C)\,\mathrm{d}C.
\end{equation}
Under the approximation that $T_{\mathrm{bg}}$ varies slowly with time, the above equation gives a monotonic coupling relation between $\Lambda_{\mathrm{eff}}(t)$ and complexity $C(t)$. If $C(t)$ grows rapidly during a certain period of cosmic history (such as the epoch of galaxy formation and the emergence of life), $\Lambda_{\mathrm{eff}}$ will also complete a major jump during this period, thereby providing an information-theoretic explanation for the "coincidence" between dark energy density and the history of structure formation.

\subsection{Theorem 2 (Red Queen Non-Equilibrium Theorem, Simplified Two-Species Case)}

Consider two classes of agent populations satisfying Red Queen conditions, whose dynamics are given by the following ordinary differential equations:
\begin{equation}
\frac{\mathrm{d}N_{i}}{\mathrm{d}t}=N_{i}\bigl(r_{i}C_{i}-d_{i}-\gamma(N_{1}+N_{2})\bigr),\quad i=1,2,
\end{equation}
\begin{equation}
\frac{\mathrm{d}C_{i}}{\mathrm{d}t}=\alpha_{i}N_{i}-\beta_{i}C_{i},
\end{equation}
where $r_{i},d_{i},\gamma,\alpha_{i},\beta_{i}>0$. The above equations can be viewed as a coupling of Lotka--Volterra type population dynamics and complexity evolution equations: $(r_{i}C_{i})$ represents fitness gains from complexity, $(\gamma(N_{1}+N_{2}))$ represents resource competition, and $(\beta_{i}C_{i})$ represents Landauer costs of maintaining complexity.

Assume there exists an internal equilibrium point $(N_{1}^{\ast},N_{2}^{\ast},C_{1}^{\ast},C_{2}^{\ast})$ satisfying $N_{i}^{\ast}>0$, $C_{i}^{\ast}>0$. If the parameters satisfy:
\begin{equation}
r_{1}\alpha_{1}N_{1}^{\ast}+r_{2}\alpha_{2}N_{2}^{\ast}>\beta_{1}^{2}+\beta_{2}^{2}+2\gamma\bigl(r_{1}C_{1}^{\ast}+r_{2}C_{2}^{\ast}\bigr),
\end{equation}
then this equilibrium point is not asymptotically stable; instead, the Jacobian matrix of the linearized system at this point has at least one pair of conjugate complex eigenvalues with positive real parts. For an open dense set of parameters, the system undergoes a Hopf bifurcation near this equilibrium point, evolving to a class of limit cycles or more complex attractors.

In this case, the total complexity
\begin{equation}
C_{\mathrm{tot}}(t)=N_{1}(t)C_{1}(t)+N_{2}(t)C_{2}(t)
\end{equation}
and the Landauer power density $P_{\mathrm{info}}(t)\propto C_{\mathrm{tot}}(t)$ will not converge to a constant over long times, but will oscillate or exhibit quasi-periodic/chaotic behavior within a bounded interval, with a non-zero time average. Therefore, the system will not enter a static equilibrium state in finite time, but is maintained in a perpetual non-equilibrium dynamic driven by the "Red Queen."

\subsection{Corollary 2 (Necessary Condition for Avoiding Heat Death)}

In a QCA--FLRW universe, if the following are satisfied:

1. There exists at least one class of agent populations satisfying Red Queen conditions, whose dynamics satisfy the conditions of Theorem 2, such that the average derivative of $C_{\mathrm{tot}}(t)$ over any finite time interval is non-zero;
2. Information waste heat--vacuum energy integrability (Assumption 1) holds, so that $P_{\mathrm{info}}(t)$ continuously injects energy into $\rho_{\mathrm{info}}$;
3. The overall free energy supply of the universe is not exhausted in finite time, i.e., the "energy budget" of the underlying QCA allows the above process to continue for arbitrarily long times;

Then the state required by the classical heat death picture---"the entire universe reaching a complete equilibrium state with no available free energy, no structure, and no computation after a finite time"---is not a dynamical attractor of the universe. Instead, the universe can evolve in a long-standing "algorithmic turbulence" phase, characterized by accelerated expansion dominated by dark energy on macroscopic scales, complex structures continuously emerging and dissipating on mesoscopic scales, and irreversible computational processes constantly occurring on microscopic scales.

\section{Proofs}

This section outlines the proof ideas for the above theorems, leaving more technical derivations to the appendices.

\subsection{Proof of Theorem 1}

In FLRW spacetime, assume the total cosmic energy-momentum tensor is:
\begin{equation}
T_{\mu\nu}=T_{\mu\nu}^{(m)}+T_{\mu\nu}^{(r)}+T_{\mu\nu}^{(\mathrm{info})},
\end{equation}
where $(m)$ and $(r)$ denote matter and radiation components, respectively, and $(\mathrm{info})$ denotes the information vacuum component. For a component $X$ with perfect fluid form:
\begin{equation}
T_{\mu\nu}^{(X)}=(\rho_{X}+p_{X})u_{\mu}u_{\nu}+p_{X}g_{\mu\nu},
\end{equation}
where $u^{\mu}$ is the comoving four-velocity. Under cosmological symmetry, energy conservation for different components can be written as sourced continuity equations:
\begin{equation}
\dot{\rho}_{X}+3H(\rho_{X}+p_{X})=Q_{X}(t),
\end{equation}
where $Q_{X}$ is the energy source term for that component from other components, satisfying $\sum_{X}Q_{X}=0$.

For the information vacuum component, assume its source term is precisely the Landauer power density:
\begin{equation}
Q_{\mathrm{info}}(t)=P_{\mathrm{info}}(t),
\end{equation}
while the source terms for matter and radiation components are energy losses of opposite sign. Substituting the equation of state $p_{\mathrm{info}}=w_{\mathrm{info}}\rho_{\mathrm{info}}$ yields:
\begin{equation}
\dot{\rho}_{\mathrm{info}}+3H(1+w_{\mathrm{info}})\rho_{\mathrm{info}}=P_{\mathrm{info}}(t).
\end{equation}
In the Red Queen era, we assume $w_{\mathrm{info}}(t)=-1+\delta(t)$, where $\delta(t)$ is a small quantity satisfying $\lvert\delta(t)\rvert\ll 1$. The continuity equation becomes:
\begin{equation}
\dot{\rho}_{\mathrm{info}}+3H\delta(t)\rho_{\mathrm{info}}=P_{\mathrm{info}}(t),
\end{equation}
with formal solution:
\begin{equation}
\rho_{\mathrm{info}}(t)=\rho_{\mathrm{info}}(t_{0})\exp\left(-3\int_{t_{0}}^{t}H(\tau)\delta(\tau)\,\mathrm{d}\tau\right)+\int_{t_{0}}^{t}P_{\mathrm{info}}(s)\exp\left(-3\int_{s}^{t}H(\tau)\delta(\tau)\,\mathrm{d}\tau\right)\mathrm{d}s.
\end{equation}
Under the condition that $\lvert\delta\rvert\ll 1$ and $H$ is finite over the period considered, the deviation of the exponential factor from 1 is $\mathcal{O}(\epsilon)$, where $\epsilon=\sup_{[t_{0},t]}\lvert 3H\delta\rvert\Delta t$, and $\Delta t$ is the characteristic time scale. Thus, it can be written as:
\begin{equation}
\rho_{\mathrm{info}}(t)=\rho_{\mathrm{info}}(t_{0})+\int_{t_{0}}^{t}P_{\mathrm{info}}(\tau)\,\mathrm{d}\tau+\mathcal{O}(\epsilon),
\end{equation}
which is the assertion of Theorem 1.

The effective cosmological constant is given by:
\begin{equation}
\Lambda_{\mathrm{eff}}(t)=\Lambda_{\mathrm{bare}}+8\pi G\rho_{\mathrm{info}}(t),
\end{equation}
naturally yielding the integral form.

\subsection{Proof of Corollary 1}

In the framework of Theorem 1, if there exists a macroscopic complexity function:
\begin{equation}
C(t)=\sum_{i}N_{i}(t)\,C_{i}(t),
\end{equation}
and under coarse-graining, the average information erasure power per unit complexity satisfies:
\begin{equation}
P_{\mathrm{info}}(t)\simeq \alpha T_{\mathrm{bg}}(t)\,\dot{C}(t),
\end{equation}
then we have:
\begin{equation}
\rho_{\mathrm{info}}(t)-\rho_{\mathrm{info}}(t_{0})\simeq \alpha\int_{t_{0}}^{t}T_{\mathrm{bg}}(\tau)\,\dot{C}(\tau)\,\mathrm{d}\tau.
\end{equation}
Changing variables $C=C(\tau)$, we get:
\begin{equation}
\rho_{\mathrm{info}}(t)-\rho_{\mathrm{info}}(t_{0})\simeq \alpha\int_{C(t_{0})}^{C(t)}T_{\mathrm{bg}}(C)\,\mathrm{d}C.
\end{equation}
Multiplying by $8\pi G$ yields the expression for $\Lambda_{\mathrm{eff}}(t)$.

If $T_{\mathrm{bg}}(t)$ varies slowly over the relevant time period, it can be approximated as a constant $\bar{T}$, yielding:
\begin{equation}
\Lambda_{\mathrm{eff}}(t)\approx \Lambda_{\mathrm{eff}}(t_{0})+8\pi G\alpha\bar{T}\bigl(C(t)-C(t_{0})\bigr),
\end{equation}
indicating that the growth rate of the cosmological constant is proportional to the complexity growth rate in the first-order approximation.

\subsection{Proof Idea of Theorem 2}

Consider the four-dimensional state vector:
\begin{equation}
\mathbf{x}=(N_{1},N_{2},C_{1},C_{2})^{\mathsf{T}},
\end{equation}
the system can be written as $\dot{\mathbf{x}}=\mathbf{F}(\mathbf{x})$. The internal equilibrium point $\mathbf{x}^{\ast}$ satisfies:
\begin{equation}
N_{i}^{\ast}\bigl(r_{i}C_{i}^{\ast}-d_{i}-\gamma(N_{1}^{\ast}+N_{2}^{\ast})\bigr)=0,\quad \alpha_{i}N_{i}^{\ast}-\beta_{i}C_{i}^{\ast}=0.
\end{equation}
In the non-trivial case $N_{i}^{\ast}>0, C_{i}^{\ast}>0$, the above equations give:
\begin{equation}
C_{i}^{\ast}=\frac{\alpha_{i}}{\beta_{i}}N_{i}^{\ast},\quad r_{i}\frac{\alpha_{i}}{\beta_{i}}N_{i}^{\ast}-d_{i}-\gamma(N_{1}^{\ast}+N_{2}^{\ast})=0.
\end{equation}
From this, explicit expressions for $(N_{1}^{\ast},N_{2}^{\ast})$ can be solved (see Appendix B for explicit forms).

Linearizing $\mathbf{F}$ at $\mathbf{x}^{\ast}$, we obtain the Jacobian matrix $J=\mathrm{D}\mathbf{F}\rvert_{\mathbf{x}^{\ast}}$. In the current model, $J$ has a block structure, and its eigenvalues can be obtained by solving the characteristic polynomial:
\begin{equation}
\det(\lambda I-J)=0.
\end{equation}
Direct calculation shows that the characteristic polynomial is a quartic polynomial:
\begin{equation}
\lambda^{4}+a_{1}\lambda^{3}+a_{2}\lambda^{2}+a_{3}\lambda+a_{4}=0,
\end{equation}
where coefficients $a_{k}$ are polynomial functions of parameters and equilibrium coordinates.

The Routh--Hurwitz criterion gives necessary and sufficient conditions for all eigenvalues to have negative real parts. Substituting the current coefficients, the stability condition is equivalent to a system of inequalities composed of $(r_{i},\alpha_{i},\beta_{i},\gamma,d_{i})$ and $(N_{i}^{\ast},C_{i}^{\ast})$. Detailed derivation is provided in Appendix B.

The key observation is: when
\begin{equation}
r_{1}\alpha_{1}N_{1}^{\ast}+r_{2}\alpha_{2}N_{2}^{\ast}
\end{equation}
is sufficiently large, certain Hurwitz principal minors become negative, thus at least one pair of conjugate eigenvalues crosses the imaginary axis. More specifically, when
\begin{equation}
r_{1}\alpha_{1}N_{1}^{\ast}+r_{2}\alpha_{2}N_{2}^{\ast}>\beta_{1}^{2}+\beta_{2}^{2}+2\gamma\bigl(r_{1}C_{1}^{\ast}+r_{2}C_{2}^{\ast}\bigr),
\end{equation}
two roots of the quartic characteristic polynomial have positive real parts, and the equilibrium point becomes an unstable focus or saddle-focus.

In parameter space, the condition where the real part of eigenvalues is zero corresponds to a hypersurface of codimension one; crossing this hypersurface produces a Hopf bifurcation. On one side, the equilibrium is stable; on the other, it is unstable and a limit cycle emerges. Since the above inequality defines an open set, and the physical range of parameters typically allows $r_{i}\alpha_{i}$ to take large values, this implies that parameter regions where a "stable internal equilibrium point exists" are not dominant in physically reasonable parameter space.

Therefore, in a broad parameter range, the system avoids convergence to static equilibrium, entering instead continuous oscillations or more complex dynamics, thus supporting Red Queen-style complexity arms races.

\subsection{Proof Idea of Corollary 2}

Under the setting of Theorem 2, the time derivative of total complexity
\begin{equation}
C_{\mathrm{tot}}(t)=N_{1}(t)C_{1}(t)+N_{2}(t)C_{2}(t)
\end{equation}
is
\begin{equation}
\dot{C}_{\mathrm{tot}}=\dot{N}_{1}C_{1}+N_{1}\dot{C}_{1}+\dot{N}_{2}C_{2}+N_{2}\dot{C}_{2}.
\end{equation}
Substituting the dynamical equations yields a polynomial expression in $(N_{i},C_{i})$. Since the equilibrium point is unstable, the trajectory will not converge to a constant vector in finite time, but will evolve in phase space around the unstable equilibrium. The long-time average $\langle\dot{C}_{\mathrm{tot}}\rangle$ on a limit cycle or chaotic attractor is generally non-zero or does not vanish over the entire cosmic time. Therefore, Landauer power
\begin{equation}
P_{\mathrm{info}}(t)\propto C_{\mathrm{tot}}(t)
\end{equation}
remains positive over the long term or at least has a positive time average over any finite time interval. Combining with Theorem 1, it is seen that the information vacuum energy density $\rho_{\mathrm{info}}(t)$ continues to grow or evolves slowly around a certain value over time. The state of "energy injection termination, universe entering complete thermal equilibrium" in the classical sense is no longer a dynamical attractor.

\section{Model Apply}

This section discusses the cosmological applications and several qualitative predictions of the model.

\subsection{Complexity-Driven $\Lambda_{\mathrm{eff}}(t)$ and the "Coincidence Problem"}

In the cosmic history dominated by galaxy formation and stellar evolution, entropy production and complexity growth are mainly contributed by processes such as stellar nucleosynthesis, galaxy formation, planetary geological activity, and the evolution of life and civilization. Existing work shows that the cosmic total entropy production rate peaked about $2\text{--}4\times 10^{9}$ years after the Big Bang, quite close to the peak of star formation rate.

In this model, if the complexity function $C(t)$ is regarded as a comprehensive indicator of these multi-scale processes, and information power density $P_{\mathrm{info}}(t)$ is proportional to $\dot{C}(t)$, then from Corollary 1:
\begin{equation}
\Lambda_{\mathrm{eff}}(t)\approx \Lambda_{\mathrm{eff}}(t_{0})+K\int_{C(t_{0})}^{C(t)}T_{\mathrm{bg}}(C)\,\mathrm{d}C,
\end{equation}
where constant $K=8\pi G\alpha$. If $C(t)$ rises rapidly during the epoch of galaxy and life emergence and then enters a phase of slow evolution, $\Lambda_{\mathrm{eff}}(t)$ will also complete its main growth during this period and evolve slowly thereafter. This naturally explains why the current dark energy density and matter density are of the same order of magnitude: both are closely related to the abundance of available free energy and complex structures in the universe.

This idea conceptually resonates with the Causal Entropic Principle and Entropic Cosmology, works that attempt to explain the magnitude and behavior of dark energy by maximizing the total entropy production rate in the observable region or introducing entropy-associated effective pressure. The difference in this paper is:
\begin{itemize}
  \item It focuses on the Landauer energy flow corresponding to \textbf{computation-induced irreversible information erasure}, rather than just any form of entropy production;
  \item Under the QCA universe framework, it connects this information waste heat with the energy coding of underlying vacuum degrees of freedom, thereby endowing dark energy with the meaning of "information vacuum" at the microscopic level.
\end{itemize}

\subsection{Possible Time Evolution of Dark Energy and Observations}

Recent data from projects like DESI give indications that dark energy may evolve slowly with cosmic time. Fits to dark energy equation of state parametrizations like Chevallier--Polarski--Linder $w(z)=w_{0}+w_{a}z/(1+z)$ show that $w_{a}$ may deviate from zero. In this model, the time evolution of $\Lambda_{\mathrm{eff}}(t)$ is entirely determined by $P_{\mathrm{info}}(t)$, so:

\begin{itemize}
  \item If the complexity of agents in the universe grows rapidly in a certain period and then approaches a plateau, $\Lambda_{\mathrm{eff}}(t)$ shows significant growth in the early stage and tends to a constant in the late stage, corresponding to $w(z)$ deviating from $-1$ at high redshift and approaching $-1$ at low redshift.
  \item If complexity continues to grow slowly over extremely long time scales (e.g., computational activities of advanced civilizations continue to expand in the late universe), $\Lambda_{\mathrm{eff}}(t)$ will evolve slowly for a long time, leading to a small but measurable deviation of $w(z)$ at low redshift.
\end{itemize}

Conversely, if future more precise observations strictly support $w=-1$ and $\Lambda$ is strictly constant throughout cosmic history, then this model must degenerate to the limit where $P_{\mathrm{info}}(t)$ is macroscopically independent of time. The physical meaning would be that the total Landauer power density of agent computational activities is approximately constant over cosmic time. Although this scenario is not strongly supported by current complexity evolution pictures, it is not theoretically impossible.

\subsection{Heat Death, Computation, and "Free Energy" in the Red Queen Universe}

Traditional heat death theory is based on macroscopic thermodynamics, viewing the universe as a closed system that eventually exhausts all available free energy. However, from the perspective of computational thermodynamics and complex systems, the availability of "free energy" depends on whether agents can identify and utilize structures and gradients in the system. The Red Queen Universe model emphasizes:

\begin{itemize}
  \item Agents identify low-entropy structures and free energy sources in the environment (e.g., stellar radiation, chemical potential differences) through internal computation;
  \item Red Queen games between agents mean that progress by any party changes the environment of others, forcing the entire system to constantly reorganize;
  \item In a QCA universe, as long as underlying evolution allows the conversion of part of the information vacuum energy into locally available free energy, the agent network can constantly refresh the spatial distribution of available free energy through complexity arms races.
\end{itemize}

In this framework, "heat death" is no longer simple entropy saturation, but a question of "whether there exist agents that can utilize free energy and encodable structures." As long as Red Queen conditions and information rate conservation allow agents to persist and perform irreversible computation, the universe can macroscopically remain far from simple equilibrium.

\section{Discussion (risks, boundaries, past work)}

This section discusses the risks, applicability boundaries, and relationship with existing work of the "Red Queen Universe" framework.

\subsection{Relationship with Traditional Heat Death Picture}

The traditional heat death picture is built on macroscopic closed systems and classical thermodynamics, viewing the universe as a "giant heat bath" that eventually reaches thermal equilibrium. This paper does not deny the generality of entropy increase, but emphasizes:

\begin{itemize}
  \item Entropy increase can be accompanied by continuous growth of complexity, especially in the presence of agents, where entropy production processes are often realized through maintaining and creating new low-entropy structures;
  \item In a QCA universe, the immense information capacity of underlying degrees of freedom means that even if macroscopic entropy approaches an upper bound, there may still exist unutilized computation and coding space;
  \item What heat death truly concerns is "whether ordered structures and computation end," not whether entropy reaches a mathematical maximum.
\end{itemize}

Therefore, the Red Queen Universe does not negate the thermodynamic basis of heat death, but proposes that in the presence of agents and information vacuum energy, the universe can maintain a non-equilibrium state for a long time through complexity arms races and computation-geometry feedback mechanisms.

\subsection{Comparison with Entropic Cosmology and Causal Entropic Principle}

Entropic cosmology and related models attempt to understand gravity and dark energy from the perspective of entropy and information, for example, by introducing effective forces related to entropy and temperature on cosmological horizons, or predicting the size of the cosmological constant by maximizing entropy production rate in causal regions.

Compared to these works, the differences in this paper are:
\begin{itemize}
  \item Introducing explicit agents and computational thermodynamics, treating dark energy as the accumulation of Landauer waste heat from irreversible computation in the information vacuum, rather than a macroscopic potential directly derived from horizon entropy;
  \item Using QCA universe as the microscopic ontology, emphasizing the role of discrete spacetime and information processing structures in the origin of dark energy;
  \item Introducing Red Queen evolutionary dynamics into cosmology, proposing that complexity arms races are an important dynamical mechanism for avoiding heat death.
\end{itemize}

These differences make the "Red Queen Universe" closer to a unified framework of "Information--Computation--Geometry," rather than a simple entropy maximization principle.

\subsection{Relationship with Anthropic Principle and Intelligent Universe Hypotheses}
The Anthropic Principle states that our observations of the universe are biased by selection effects, i.e., we can only exist in a universe that allows observers to exist. Some work further proposes hypotheses that intelligence or consciousness plays a more fundamental role in cosmic structure.

The stance of this paper is more conservative: defining "agents" as generalized maintainers of low-entropy structures and executors of computation, ranging from molecular machines to organisms to technological civilizations. The Red Queen Universe framework only requires these agents to exist and perform irreversible computation, without relying on whether they possess self-consciousness in the human sense. In a sense, this is a "weak intelligent universe" view: intelligence and computation are natural emergences of cosmic information dynamics, while dark energy and the avoidance of heat death are macroscopic projections of this dynamics on the geometric level.

\subsection{Model Boundaries and Open Problems}

Although this paper is self-consistent in its internal mathematical structure, there are still several important open problems and limitations:

1. \textbf{Physical Basis of Information Waste Heat--Vacuum Energy Integrability}: Assumption 1 maps Landauer heat directly to information vacuum energy with $w\simeq -1$. This requires a more specific mechanism at the microscopic QCA level, such as proving through scattering theory or spectral flow theory that certain classes of irreversible computations inevitably introduce a specific form of background energy density.
2. \textbf{Quantitative Definition of Agent Complexity}: This paper abstracts complexity as $C_{i}$, but how to characterize agent complexity with observables (such as entropy, mutual information, algorithmic complexity) in the real universe remains unclear.
3. \textbf{Cosmic Free Energy Budget and Long-Term Evolution}: The Red Queen Universe assumes that the underlying QCA can provide available free energy for agents over arbitrarily long time scales. The compatibility of this with long-term processes like black hole evaporation and proton decay needs further analysis.
4. \textbf{Integration with Quantum Gravity and Holographic Principle}: If the universe must ultimately be unified under some holographic theory or quantum gravity framework, the role of information vacuum energy in this model and its relationship with that unified framework remain to be clarified.

These problems point to directions for future work.

\section{Conclusion}

At the intersection of Quantum Cellular Automata universe, computational thermodynamics, and multi-agent game theory, this paper proposes the "Red Queen Universe" as a unified picture of cosmic evolution and the origin of dark energy. The core ideas can be summarized as follows:

1. Viewing the universe as a QCA composed of multi-level agents, most agents maintain themselves far from equilibrium low-entropy structures through internal computation and prediction error correction, and their behavior can be characterized within the Free Energy Principle framework.
2. Based on Landauer's principle and energy-momentum conservation, an information vacuum energy density $\rho_{\mathrm{info}}(t)$ is constructed. It is proved that under the condition $w_{\mathrm{info}}\simeq -1$, the dynamical part of the cosmological constant is equivalent to the integral of Landauer power generated by the irreversible computation of all agents in the universe over cosmic time (Landauer--$\Lambda$ relation).
3. By introducing a multi-agent replicator-complexity dynamic model satisfying Red Queen conditions, it is proved that within a broad parameter range, internal equilibrium points are unstable, and the system enters limit cycles or chaotic attractors. This leads to cosmic complexity and information erasure rates remaining positive on long time scales, dynamically excluding simple heat death attractor states.
4. The complexity-driven $\Lambda_{\mathrm{eff}}(t)$ is linked to the coincidence problem of dark energy, entropic cosmology, and the Causal Entropic Principle, and potential connections with observations of possible time evolution of dark energy are discussed.
5. Engineering proposals for testing elements of this framework on QCA quantum simulation platforms and in multi-agent simulations are presented, providing experimental and numerical pathways to elevate the "Red Queen Universe" from a philosophical picture to a testable physical theory.

In this picture, life and intelligence are no longer accidental interludes in cosmic evolution, but the main driving forces pushing the universe away from equilibrium at the information level. The cosmological constant and the mechanism for avoiding heat death are unified into a whole of Information--Computation--Geometry, providing a new perspective for understanding the ultimate fate of the universe.

\section{Acknowledgements}

This work has benefited from a large body of existing literature on Quantum Cellular Automata, computational thermodynamics, the Free Energy Principle, and the cosmological constant problem. All numerical and schematic derivations in the text are based on public data and did not use unpublished observational data. The code required for numerical examples and phase diagrams can be implemented with conventional ODE solvers in general scientific computing environments; specific implementation details can be reproduced according to the equations in the appendices.

\begin{thebibliography}{99}
\bibitem{Landauer1961} R. Landauer, "Irreversibility and Heat Generation in the Computing Process," IBM Journal of Research and Development 5, 183--191 (1961).
\bibitem{Bennett1982} C. H. Bennett, "The Thermodynamics of Computation: A Review," International Journal of Theoretical Physics 21, 905--940 (1982).
\bibitem{Chattopadhyay2025} P. Chattopadhyay et al., "Landauer Principle and Thermodynamics of Computation," arXiv:2506.10876 (2025).
\bibitem{Friston2010} K. Friston, "The Free-energy Principle: A Unified Brain Theory?," Nature Reviews Neuroscience 11, 127--138 (2010).
\bibitem{Peebles2003} P. J. E. Peebles and B. Ratra, "The Cosmological Constant and Dark Energy," Reviews of Modern Physics 75, 559--606 (2003).
\bibitem{Weinberg1989} S. Weinberg, "The Cosmological Constant Problem," Reviews of Modern Physics 61, 1--23 (1989).
\bibitem{VanValen1973} L. Van Valen, "A New Evolutionary Law," Evolutionary Theory 1, 1--30 (1973).
\bibitem{Seselja2023} D. ≈†e≈°elja and C. Stra√üer, "Agent-based Modeling in the Philosophy of Science," Stanford Encyclopedia of Philosophy (2023).
\bibitem{WikiHeatDeath} "Heat Death of the Universe," in Wikipedia, accessed 2025.
\bibitem{Bolotin2023} Y. L. Bolotin, A. L. Tur, V. A. Cherkaskiy, "Cosmology Based on Entropy," arXiv:2310.10144 (2023).
\bibitem{Nojiri2022} S. Nojiri, S. D. Odintsov, V. Faraoni, "Barrow Entropic Dark Energy: A Review," Physics of the Dark Universe 36, 101050 (2022).
\bibitem{Bousso2007} R. Bousso, R. Harnik, G. D. Kribs, G. Perez, "Predicting the Cosmological Constant from the Causal Entropic Principle," Physical Review D 76, 043513 (2007).
\bibitem{DESI2025} DESI Collaboration, reports on time-evolving dark energy presented at APS Global Physics Summit (2025).
\bibitem{Arrighi2019} P. Arrighi, "An Overview of Quantum Cellular Automata," Natural Computing 18, 885--899 (2019).
\bibitem{Farrelly2020} T. Farrelly, "A Review of Quantum Cellular Automata," Quantum 4, 368 (2020).
\bibitem{Bialynicki1994} I. Bialynicki-Birula, "Weyl, Dirac, and Maxwell Equations on a Lattice as Unitary Cellular Automata," Physical Review D 49, 6920--6927 (1994).
\bibitem{Bisio2015} A. Bisio, G. M. D'Ariano, A. Tosini, "Quantum Field as a Quantum Cellular Automaton: The Dirac Free Evolution in One Dimension," Annals of Physics 354, 244--264 (2015).
\bibitem{Gross2012} D. Gross, V. Nesme, H. Vogts, R. F. Werner, "Index Theory of One Dimensional Quantum Walks and Cellular Automata," Communications in Mathematical Physics 310, 419--454 (2012).
\bibitem{Suprano2024} A. Suprano et al., "Photonic Cellular Automaton Simulation of Relativistic Quantum Field," Physical Review Research 6, 033136 (2024).
\bibitem{Bisio2016} A. Bisio, G. M. D'Ariano, P. Perinotti, "Quantum Cellular Automaton Theory of Light," Annals of Physics 368, 177--190 (2016).
\bibitem{Azarian2023} B. Azarian, "Life Need Not Ever End," Noema Magazine (2023).
\bibitem{Ma2025a} H. Ma, "Universal Conservation of Information Celerity: From Quantum Cellular Automata to Relativity, Mass and Gravity" (2025), preprint.
\bibitem{Ma2025b} H. Ma, "Information-volume Conservation and the Emergence of Optical Metrics" (2025), preprint.
\bibitem{Ma2025c} H. Ma, "The Red Queen Universe: Agent Games, Dark Energy and the Avoidance of Heat Death" (this work).
\end{thebibliography}

\appendix

\section{Appendix A: Detailed Derivation from Landauer Power to Effective Cosmological Constant}

This appendix provides a detailed derivation from Landauer power density $P_{\mathrm{info}}(t)$ to the effective cosmological constant $\Lambda_{\mathrm{eff}}(t)$.

\subsection{A.1 Energy-Momentum Conservation and Sourced Continuity Equation}

In the FLRW background, Einstein's equations
\begin{equation}
G_{\mu\nu}+\Lambda_{\mathrm{bare}}g_{\mu\nu}=8\pi GT_{\mu\nu}
\end{equation}
combined with the Bianchi identity $\nabla^{\mu}G_{\mu\nu}=0$ lead to
\begin{equation}
\nabla^{\mu}\bigl(T_{\mu\nu}-\tfrac{\Lambda_{\mathrm{bare}}}{8\pi G}g_{\mu\nu}\bigr)=0.
\end{equation}
If we introduce an effective cosmological constant $\Lambda_{\mathrm{eff}}(t)$ and absorb it into the right-hand side, it can be written as:
\begin{equation}
G_{\mu\nu}=8\pi G\tilde{T}_{\mu\nu},\quad \tilde{T}_{\mu\nu}=T_{\mu\nu}^{(m)}+T_{\mu\nu}^{(r)}+T_{\mu\nu}^{(\mathrm{info})},
\end{equation}
where
\begin{equation}
T_{\mu\nu}^{(\mathrm{info})}=-\frac{\Lambda_{\mathrm{eff}}(t)}{8\pi G}g_{\mu\nu}.
\end{equation}
Applying the energy-momentum conservation equation to each component:
\begin{equation}
\nabla^{\mu}T_{\mu\nu}^{(X)}=Q_{\nu}^{(X)},\quad \sum_{X}Q_{\nu}^{(X)}=0,
\end{equation}
and using the comoving frame $u^{\mu}=(1,0,0,0)$, focusing only on the energy component $\nu=0$, we obtain:
\begin{equation}
\dot{\rho}_{X}+3H(\rho_{X}+p_{X})=Q^{(X)}_{0}(t).
\end{equation}
Assume the source terms for matter and radiation components are:
\begin{equation}
Q_{0}^{(m)}=-\Gamma_{m}(t),\quad Q_{0}^{(r)}=-\Gamma_{r}(t),
\end{equation}
and combine energy losses caused by computation into the source term for the information vacuum component:
\begin{equation}
Q_{0}^{(\mathrm{info})}=\Gamma_{m}(t)+\Gamma_{r}(t)=P_{\mathrm{info}}(t),
\end{equation}
then the information component satisfies:
\begin{equation}
\dot{\rho}_{\mathrm{info}}+3H(\rho_{\mathrm{info}}+p_{\mathrm{info}})=P_{\mathrm{info}}(t).
\end{equation}
If $p_{\mathrm{info}}=w_{\mathrm{info}}\rho_{\mathrm{info}}$, then the above equation is the sourced continuity equation in the main text.

\subsection{A.2 Justification for $w_{\mathrm{info}}\simeq -1$}

For the information vacuum energy to manifest macroscopically as dark energy, its equation of state must be close to $-1$. From a QCA perspective, a possible microscopic picture is:
\begin{itemize}
  \item Irreversible computations of agents permanently "erase" a portion of locally accessible degrees of freedom into inaccessible global entangled structures through scattering and entanglement with the environment;
  \item These entangled degrees of freedom are uniformly distributed on large scales, carry no net momentum flux, and their local perturbations equilibrate rapidly under QCA evolution;
  \item In the continuous limit, the contribution of such degrees of freedom to macroscopic geometry is approximately isotropic and equivalent to a fluid with negative pressure.
\end{itemize}
This picture is formally similar to viewing vacuum energy as zero-point energy of field theory, but its origin is not field mode oscillations but "information fragments" generated during irreversible computation. In specific models, one can prove that a class of irreversible scattering is always accompanied by a spectral shift of fixed sign by constructing a QCA Hamiltonian with a given scattering matrix and spectral flow, thereby producing an equivalent vacuum energy contribution. This is a direction for future work.

Under the approximation required for this paper, it suffices that $w_{\mathrm{info}}$ satisfies
\begin{equation}
\lvert w_{\mathrm{info}}+1\rvert\ll 1
\end{equation}
in the Red Queen era to guarantee the validity of the integral approximation.

\subsection{A.3 Complexity-Driven $P_{\mathrm{info}}(t)$ Model}

Assume the average information erasure rate corresponding to unit complexity is a constant $\kappa$, then:
\begin{equation}
\dot{I}_{\mathrm{erase}}(t)=\kappa\,C(t).
\end{equation}
If the ambient temperature $T_{\mathrm{bg}}(t)$ varies slowly over the period considered, the Landauer power density is:
\begin{equation}
P_{\mathrm{info}}(t)=k_{\mathrm{B}}\ln 2\,T_{\mathrm{bg}}(t)\,\kappa\,C(t)\approx \alpha\,C(t),
\end{equation}
where $\alpha=k_{\mathrm{B}}\ln 2\,\kappa\bar{T}$, and $\bar{T}$ is the characteristic value of $T_{\mathrm{bg}}$.

More generally, if we consider the rate of change of complexity, then
\begin{equation}
P_{\mathrm{info}}(t)\approx \alpha\,T_{\mathrm{bg}}(t)\,\dot{C}(t)
\end{equation}
is a more reasonable approximation, because information erasure is typically more directly related to complexity change than to absolute value. Substituting this into the integral expression of A.1 yields the relation in Corollary 1 of the main text.

\subsection{A.4 Consistency with Friedmann Equations}

Incorporating information vacuum energy into the Friedmann equation:
\begin{equation}
H^{2}(t)=\frac{8\pi G}{3}\bigl(\rho_{m}(t)+\rho_{r}(t)+\rho_{\mathrm{info}}(t)\bigr)-\frac{k}{a^{2}(t)},
\end{equation}
where $k$ is spatial curvature. Since $\rho_{\mathrm{info}}(t)$ is a function obtained by integrating $P_{\mathrm{info}}(t)$, as long as $P_{\mathrm{info}}(t)$ is smooth on large scales and satisfies appropriate growth conditions, it will not introduce rapid oscillations or instabilities that violate observational constraints.

It should be emphasized that this model does not claim that all dark energy is composed of information vacuum energy, but provides an information-theoretic origin for part or all of $\Lambda_{\mathrm{eff}}(t)$. Quantitative fitting requires considering $\rho_{\mathrm{info}}$ together with other possible dynamical dark energy components.

\section{Appendix B: Linear Stability Analysis of Red Queen Dynamics Model}

This appendix provides details of the linear stability analysis for the two-species Red Queen dynamics model in Theorem 2.

\subsection{B.1 Solution of Equilibrium Points}

Consider the system:
\begin{equation}
\frac{\mathrm{d}N_{i}}{\mathrm{d}t}=N_{i}\bigl(r_{i}C_{i}-d_{i}-\gamma(N_{1}+N_{2})\bigr),\quad i=1,2,
\end{equation}
\begin{equation}
\frac{\mathrm{d}C_{i}}{\mathrm{d}t}=\alpha_{i}N_{i}-\beta_{i}C_{i}.
\end{equation}
Equilibrium points satisfy:
\begin{equation}
N_{i}^{\ast}\bigl(r_{i}C_{i}^{\ast}-d_{i}-\gamma(N_{1}^{\ast}+N_{2}^{\ast})\bigr)=0,\quad \alpha_{i}N_{i}^{\ast}-\beta_{i}C_{i}^{\ast}=0.
\end{equation}
For non-trivial equilibrium points, we require $N_{i}^{\ast}>0, C_{i}^{\ast}>0$, so we must have:
\begin{equation}
C_{i}^{\ast}=\frac{\alpha_{i}}{\beta_{i}}N_{i}^{\ast},
\end{equation}
Substituting back, we get:
\begin{equation}
r_{i}\frac{\alpha_{i}}{\beta_{i}}N_{i}^{\ast}-d_{i}-\gamma(N_{1}^{\ast}+N_{2}^{\ast})=0.
\end{equation}
This gives two linear equations:
\begin{equation}
\bigl(r_{1}\tfrac{\alpha_{1}}{\beta_{1}}-\gamma\bigr)N_{1}^{\ast}-\gamma N_{2}^{\ast}=d_{1},
\end{equation}
\begin{equation}
-\gamma N_{1}^{\ast}+\bigl(r_{2}\tfrac{\alpha_{2}}{\beta_{2}}-\gamma\bigr)N_{2}^{\ast}=d_{2}.
\end{equation}
If the coefficient matrix
\begin{equation}
M=\begin{pmatrix}
r_{1}\tfrac{\alpha_{1}}{\beta_{1}}-\gamma & -\gamma\\
-\gamma & r_{2}\tfrac{\alpha_{2}}{\beta_{2}}-\gamma
\end{pmatrix}
\end{equation}
is invertible, then
\begin{equation}
\begin{pmatrix}
N_{1}^{\ast}\\
N_{2}^{\ast}
\end{pmatrix}=M^{-1}\begin{pmatrix}
d_{1}\\
d_{2}
\end{pmatrix},
\end{equation}
and subsequently
\begin{equation}
C_{i}^{\ast}=\frac{\alpha_{i}}{\beta_{i}}N_{i}^{\ast}.
\end{equation}
Requiring $N_{i}^{\ast}>0, C_{i}^{\ast}>0$ restricts the parameter space, but can generally be satisfied within physically reasonable parameter ranges.

\subsection{B.2 Construction of Jacobian Matrix}

Let $\mathbf{x}=(N_{1},N_{2},C_{1},C_{2})^{\mathsf{T}}$. The components of the vector field $\mathbf{F}(\mathbf{x})$ are:
\begin{equation}
F_{1}=N_{1}\bigl(r_{1}C_{1}-d_{1}-\gamma(N_{1}+N_{2})\bigr),
\end{equation}
\begin{equation}
F_{2}=N_{2}\bigl(r_{2}C_{2}-d_{2}-\gamma(N_{1}+N_{2})\bigr),
\end{equation}
\begin{equation}
F_{3}=\alpha_{1}N_{1}-\beta_{1}C_{1},
\end{equation}
\begin{equation}
F_{4}=\alpha_{2}N_{2}-\beta_{2}C_{2}.
\end{equation}
The elements of the Jacobian matrix $J$ are $J_{ij}=\partial F_{i}/\partial x_{j}$. At the equilibrium point $\mathbf{x}^{\ast}$, its non-zero elements are:

\begin{itemize}
  \item For $F_{1}$:
  \begin{equation}
  \frac{\partial F_{1}}{\partial N_{1}}=r_{1}C_{1}^{\ast}-d_{1}-\gamma(2N_{1}^{\ast}+N_{2}^{\ast}),
  \quad
  \frac{\partial F_{1}}{\partial N_{2}}=-\gamma N_{1}^{\ast},
  \quad
  \frac{\partial F_{1}}{\partial C_{1}}=r_{1}N_{1}^{\ast}.
  \end{equation}
  Using the equilibrium condition $r_{1}C_{1}^{\ast}-d_{1}-\gamma(N_{1}^{\ast}+N_{2}^{\ast})=0$, this simplifies to:
  \begin{equation}
  \frac{\partial F_{1}}{\partial N_{1}}=-\gamma N_{1}^{\ast}.
  \end{equation}

  \item For $F_{2}$:
  \begin{equation}
  \frac{\partial F_{2}}{\partial N_{2}}=r_{2}C_{2}^{\ast}-d_{2}-\gamma(N_{1}^{\ast}+2N_{2}^{\ast})=-\gamma N_{2}^{\ast},
  \end{equation}
  \begin{equation}
  \frac{\partial F_{2}}{\partial N_{1}}=-\gamma N_{2}^{\ast},
  \quad
  \frac{\partial F_{2}}{\partial C_{2}}=r_{2}N_{2}^{\ast}.
  \end{equation}

  \item For $F_{3}$:
  \begin{equation}
  \frac{\partial F_{3}}{\partial N_{1}}=\alpha_{1},\quad
  \frac{\partial F_{3}}{\partial C_{1}}=-\beta_{1}.
  \end{equation}

  \item For $F_{4}$:
  \begin{equation}
  \frac{\partial F_{4}}{\partial N_{2}}=\alpha_{2},\quad
  \frac{\partial F_{4}}{\partial C_{2}}=-\beta_{2}.
  \end{equation}
\end{itemize}

Thus, the Jacobian matrix at the equilibrium point is:
\begin{equation}
J=\begin{pmatrix}
-\gamma N_{1}^{\ast} & -\gamma N_{1}^{\ast} & r_{1}N_{1}^{\ast} & 0\\
-\gamma N_{2}^{\ast} & -\gamma N_{2}^{\ast} & 0 & r_{2}N_{2}^{\ast}\\
\alpha_{1} & 0 & -\beta_{1} & 0\\
0 & \alpha_{2} & 0 & -\beta_{2}
\end{pmatrix}.
\end{equation}

\subsection{B.3 Characteristic Polynomial and Routh--Hurwitz Criterion}

The characteristic polynomial is:
\begin{equation}
\det(\lambda I-J)=\lambda^{4}+a_{1}\lambda^{3}+a_{2}\lambda^{2}+a_{3}\lambda+a_{4},
\end{equation}
where coefficients $a_{k}$ can be obtained by direct expansion. To simplify notation, let:
\begin{equation}
A_{i}=\gamma N_{i}^{\ast},\quad
B_{i}=r_{i}N_{i}^{\ast},\quad
C_{i}=\alpha_{i},\quad
D_{i}=\beta_{i}.
\end{equation}
Then:
\begin{equation}
a_{1}=2(A_{1}+A_{2})+D_{1}+D_{2},
\end{equation}
\begin{equation}
a_{2}=(A_{1}+A_{2})^{2}+2(A_{1}+A_{2})(D_{1}+D_{2})+D_{1}D_{2}+B_{1}C_{1}+B_{2}C_{2},
\end{equation}
\begin{equation}
a_{3}=(A_{1}+A_{2})^{2}(D_{1}+D_{2})+(A_{1}+A_{2})D_{1}D_{2}+(A_{1}+A_{2})(B_{1}C_{1}+B_{2}C_{2})+D_{1}B_{2}C_{2}+D_{2}B_{1}C_{1},
\end{equation}
\begin{equation}
a_{4}=(A_{1}+A_{2})^{2}D_{1}D_{2}+(A_{1}+A_{2})D_{1}B_{2}C_{2}+(A_{1}+A_{2})D_{2}B_{1}C_{1}.
\end{equation}

The Routh--Hurwitz criterion states that all eigenvalues have negative real parts if and only if the following conditions hold:
1. $a_{1}>0$,
2. $a_{1}a_{2}-a_{3}>0$,
3. $(a_{1}a_{2}-a_{3})a_{3}-a_{1}^{2}a_{4}>0$,
4. $a_{4}>0$.

In the current model, all $A_{i},D_{i},B_{i},C_{i}>0$, so $a_{1},a_{2},a_{4}>0$ automatically hold. Stability critically depends on the second and third conditions. Through algebraic simplification, the second condition can be written as:
\begin{equation}
a_{1}a_{2}-a_{3}=K_{0}-K_{1},
\end{equation}
where $K_{0}$ is a positive term containing only $A_{i},D_{i}$, and $K_{1}$ is a term related to $B_{i}C_{i}$. Clearly, when $B_{1}C_{1}+B_{2}C_{2}$ is sufficiently large, i.e., when the sum of $r_{i}\alpha_{i}N_{i}^{\ast}$ is sufficiently large, $K_{1}$ can exceed $K_{0}$, causing $a_{1}a_{2}-a_{3}<0$, thus violating the second Hurwitz condition. This indicates the existence of a critical surface; on one side, the equilibrium is stable, and on the other, it is unstable.

Similarly, the third condition can be written as:
\begin{equation}
(a_{1}a_{2}-a_{3})a_{3}-a_{1}^{2}a_{4}=L_{0}-L_{1},
\end{equation}
where the highest order term in $L_{1}$ is proportional to $(B_{1}C_{1}+B_{2}C_{2})^{2}$. When $B_{1}C_{1}+B_{2}C_{2}$ is large, this quantity can also become negative.

Comparing $B_{i}C_{i}=r_{i}\alpha_{i}(N_{i}^{\ast})^{2}$ with $A_{i}^{2}=\gamma^{2}(N_{i}^{\ast})^{2}$ and $D_{i}^{2}=\beta_{i}^{2}$, we can derive a simple sufficient condition:
\begin{equation}
r_{1}\alpha_{1}N_{1}^{\ast}+r_{2}\alpha_{2}N_{2}^{\ast}>\beta_{1}^{2}+\beta_{2}^{2}+2\gamma\bigl(r_{1}C_{1}^{\ast}+r_{2}C_{2}^{\ast}\bigr),
\end{equation}
Under this condition, at least one Hurwitz condition is violated, and the equilibrium point is unstable. This gives the instability condition in Theorem 2 of the main text.

\subsection{B.4 Hopf Bifurcation and Existence of Limit Cycles}

When parameters vary continuously such that a Hurwitz condition crosses zero from positive, corresponding eigenvalues will cross the imaginary axis. If exactly one pair of conjugate complex eigenvalues crosses the imaginary axis while other eigenvalues still have negative real parts, the system undergoes a Hopf bifurcation, generating stable or unstable limit cycles.

In the current model, since the system has a four-dimensional state space and a simple coupling structure, parameter perturbations satisfying general position conditions will typically lead to this typical scenario. Specific parameters can be verified by numerical calculation, which is not repeated here. Importantly, there exists an open dense set of parameters such that the internal equilibrium point is unstable and at least one limit cycle or more complex attractor exists. This supports the conclusion in Theorem 2 regarding "Red Queen dynamics leading to sustained non-equilibrium."

\end{document}

